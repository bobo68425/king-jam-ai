"""
Video Generator Service - å½±ç‰‡ç”Ÿæˆæœå‹™ v3.0
============================================
ä½¿ç”¨ Google Vertex AI Veo æ¨¡å‹ç”Ÿæˆé«˜å“è³ªå½±ç‰‡
æ”¯æ´ï¼šVeo 3ã€Veo 3 Fastã€Imagen åœ–ç‰‡ç”Ÿæˆ
"""

import os
import uuid
import asyncio
import base64
import io
import tempfile
import struct
import math
import time
from pathlib import Path
from typing import Optional, List, Dict, Any, Callable
from pydantic import BaseModel

# é…ç½®
GOOGLE_GEMINI_KEY = os.getenv("GOOGLE_GEMINI_KEY")
GOOGLE_CLOUD_PROJECT = os.getenv("GOOGLE_CLOUD_PROJECT", "veo-saas-backend")
GOOGLE_CLOUD_LOCATION = os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")
GOOGLE_APPLICATION_CREDENTIALS = os.getenv("GOOGLE_APPLICATION_CREDENTIALS", "")

# åˆå§‹åŒ– Google GenAI Client
genai_client = None
vertexai_client = None

# æ–¹æ³• 1: ä½¿ç”¨ Vertex AI SDKï¼ˆæœå‹™å¸³æˆ¶èªè­‰ï¼‰
if GOOGLE_CLOUD_PROJECT and GOOGLE_APPLICATION_CREDENTIALS:
    try:
        from google import genai
        from google.genai import types
        
        # ä½¿ç”¨ Vertex AI æ¨¡å¼
        vertexai_client = genai.Client(
            vertexai=True,
            project=GOOGLE_CLOUD_PROJECT,
            location=GOOGLE_CLOUD_LOCATION,
        )
        print(f"[VideoGenerator] âœ“ Vertex AI Client åˆå§‹åŒ–æˆåŠŸ (å°ˆæ¡ˆ: {GOOGLE_CLOUD_PROJECT})")
    except Exception as e:
        print(f"[VideoGenerator] Vertex AI åˆå§‹åŒ–å¤±æ•—: {e}")

# æ–¹æ³• 2: ä½¿ç”¨ API Keyï¼ˆå‚™é¸ï¼‰
if not vertexai_client and GOOGLE_GEMINI_KEY:
    try:
        from google import genai
        genai_client = genai.Client(api_key=GOOGLE_GEMINI_KEY)
        print("[VideoGenerator] âœ“ GenAI Client (API Key) åˆå§‹åŒ–æˆåŠŸ")
    except ImportError:
        try:
            import google.genai as genai
            genai_client = genai.Client(api_key=GOOGLE_GEMINI_KEY)
            print("[VideoGenerator] âœ“ GenAI Client (API Key) åˆå§‹åŒ–æˆåŠŸ (fallback)")
        except ImportError:
            print("[VideoGenerator] âœ— google-genai SDK æœªå®‰è£")

# é¸æ“‡å¯ç”¨çš„ client
active_client = vertexai_client or genai_client
if active_client:
    print(f"[VideoGenerator] ä½¿ç”¨ {'Vertex AI' if vertexai_client else 'API Key'} æ¨¡å¼")

# å˜—è©¦å°å…¥ PIL
try:
    from PIL import Image, ImageDraw, ImageFont
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

# å˜—è©¦å°å…¥ edge-tts
try:
    import edge_tts
    EDGE_TTS_AVAILABLE = True
except ImportError:
    EDGE_TTS_AVAILABLE = False


# ============================================================
# Veo æ¨¡å‹é…ç½®
# ============================================================

VEO_MODELS = {
    "veo-3-fast": "veo-3.0-fast-generate-preview",  # å¿«é€Ÿç”Ÿæˆ
    "veo-3": "veo-3.0-generate-preview",            # é«˜å“è³ª
    "veo-2": "veo-2.0-generate-001",                # ç©©å®šç‰ˆæœ¬
}

IMAGEN_MODELS = {
    "fast": "models/imagen-4.0-fast-generate-001",
    "standard": "models/imagen-4.0-generate-001",
}


# ============================================================
# è³‡æ–™æ¨¡å‹
# ============================================================

class VideoResult(BaseModel):
    """å½±ç‰‡ç”Ÿæˆçµæœ"""
    video_url: str
    video_base64: Optional[str] = None
    thumbnail_url: Optional[str] = None
    duration: float
    format: str
    file_size: int
    scene_images: Optional[List[str]] = None
    generation_method: str = "unknown"  # veo, imagen+ffmpeg, placeholder


# ============================================================
# Video Generator ä¸»æœå‹™
# ============================================================

class VideoGeneratorService:
    """
    å½±ç‰‡ç”Ÿæˆæœå‹™ v3.0
    
    å„ªå…ˆä½¿ç”¨ Google Veo æ¨¡å‹ç›´æ¥ç”Ÿæˆå½±ç‰‡
    å‚™é¸æ–¹æ¡ˆï¼šImagen åœ–ç‰‡ + FFmpeg åˆæˆ
    """
    
    TTS_VOICES = {
        "female": "zh-TW-HsiaoChenNeural",
        "male": "zh-TW-YunJheNeural",
        "friendly": "zh-TW-HsiaoChenNeural",
        "professional": "zh-CN-XiaoxiaoNeural",
        "energetic": "zh-CN-YunyangNeural",
    }
    
    def __init__(self):
        self.output_dir = Path(tempfile.gettempdir()) / "kingjam_videos"
        self.output_dir.mkdir(exist_ok=True)
    
    async def generate_video(
        self,
        script: Dict[str, Any],
        progress_callback: Optional[Callable] = None,
        quality: str = "standard"
    ) -> VideoResult:
        """
        ç”Ÿæˆå½±ç‰‡
        
        å“è³ªç­‰ç´šï¼š
        - standard: Imagen åœ–ç‰‡ + FFmpeg åˆæˆ
        - premium: Veo 3 Fast
        - ultra: Veo 3 æœ€é«˜å“è³ª
        """
        project_id = script.get("project_id", str(uuid.uuid4()))
        scenes = script.get("scenes", [])
        total_duration = sum(s.get("duration_seconds", 5) for s in scenes)
        format_str = script.get("format", "9:16")
        color_palette = script.get("color_palette", ["#6366F1", "#8B5CF6"])
        
        if not scenes:
            raise ValueError("è…³æœ¬ä¸­æ²’æœ‰å ´æ™¯")
        
        quality_names = {"standard": "æ¨™æº–", "premium": "é«˜ç´š", "ultra": "é ‚ç´š"}
        print(f"[VideoGenerator] ğŸ¬ é–‹å§‹ç”Ÿæˆå½±ç‰‡ (å“è³ª: {quality_names.get(quality, quality)})")
        print(f"[VideoGenerator] ğŸ“‹ å ´æ™¯æ•¸: {len(scenes)}, ç¸½æ™‚é•·: {total_duration}ç§’")
        
        # æ ¹æ“šå“è³ªç­‰ç´šé¸æ“‡ç”Ÿæˆæ–¹æ³•
        if quality in ["premium", "ultra"]:
            # é«˜ç´š/é ‚ç´šï¼šä½¿ç”¨ Veo æ¨¡å‹
            veo_model = "veo-3.0-generate-preview" if quality == "ultra" else "veo-3.0-fast-generate-preview"
            print(f"[VideoGenerator] ğŸ¥ ä½¿ç”¨ Veo æ¨¡å‹: {veo_model}")
            
            video_result = await self._generate_with_veo(script, project_id, preferred_model=veo_model)
            if video_result:
                return video_result
            
            # Veo å¤±æ•—ï¼Œé™ç´šåˆ° Imagen + FFmpeg
            print("[VideoGenerator] âš ï¸ Veo ä¸å¯ç”¨ï¼Œé™ç´šåˆ° Imagen + FFmpeg")
        
        # æ¨™æº–å“è³ª æˆ– Veo å¤±æ•—çš„é™ç´šæ–¹æ¡ˆ
        print("[VideoGenerator] ğŸ“¸ ä½¿ç”¨ Imagen + FFmpeg æ–¹æ¡ˆ")
        return await self._generate_with_imagen_ffmpeg(script, project_id)
    
    async def _generate_with_veo(
        self,
        script: Dict[str, Any],
        project_id: str,
        preferred_model: str = "veo-3.0-fast-generate-preview"
    ) -> Optional[VideoResult]:
        """
        ä½¿ç”¨ Google Veo æ¨¡å‹ç›´æ¥ç”Ÿæˆå½±ç‰‡
        
        æ¨¡å‹é¸é …ï¼š
        - veo-3.0-generate-preview: é ‚ç´šå“è³ª
        - veo-3.0-fast-generate-preview: å¿«é€Ÿç”Ÿæˆ
        """
        client = vertexai_client or genai_client
        if not client:
            print("[VideoGenerator] Veo: æ²’æœ‰å¯ç”¨çš„ Client")
            return None
        
        scenes = script.get("scenes", [])
        format_str = script.get("format", "9:16")
        color_palette = script.get("color_palette", ["#6366F1", "#8B5CF6"])
        total_duration = sum(s.get("duration_seconds", 5) for s in scenes)
        
        # æ§‹å»ºå®Œæ•´çš„å½±ç‰‡æç¤ºè©
        video_prompt = self._build_veo_prompt(script)
        
        # è¨­å®šå½±ç‰‡åƒæ•¸
        aspect_ratio = "9:16" if format_str == "9:16" else "16:9" if format_str == "16:9" else "1:1"
        
        # å„ªå…ˆä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹
        veo_models = [preferred_model]
        # å‚™é¸æ¨¡å‹
        fallback_models = [
            "veo-3.0-fast-generate-preview",
            "veo-3.0-generate-preview", 
            "veo-2.0-generate-001",
        ]
        for m in fallback_models:
            if m not in veo_models:
                veo_models.append(m)
        
        for model_name in veo_models:
            try:
                print(f"[VideoGenerator] ğŸ¥ å˜—è©¦ Veo æ¨¡å‹: {model_name}")
                
                # èª¿ç”¨ Veo API ç”Ÿæˆå½±ç‰‡
                if hasattr(client.models, 'generate_videos'):
                    # Veo åªæ”¯æŒ 4, 6, 8 ç§’çš„å½±ç‰‡
                    veo_duration = 8  # ä½¿ç”¨æœ€é•·çš„ 8 ç§’
                    
                    # ç™¼èµ·ç”Ÿæˆè«‹æ±‚
                    operation = await asyncio.to_thread(
                        client.models.generate_videos,
                        model=model_name,
                        prompt=video_prompt,
                        config={
                            "aspect_ratio": aspect_ratio,
                            "duration_seconds": veo_duration,
                            "number_of_videos": 1,
                            "generate_audio": True,
                        }
                    )
                    
                    print(f"[VideoGenerator] ğŸ“¡ Operation: {operation.name}")
                    
                    # è¼ªè©¢ç­‰å¾…æ“ä½œå®Œæˆ
                    max_wait = 180  # æœ€å¤šç­‰å¾… 3 åˆ†é˜
                    poll_interval = 5
                    waited = 0
                    
                    while waited < max_wait:
                        # ä½¿ç”¨ client.operations.get ç²å–æœ€æ–°ç‹€æ…‹
                        operation = await asyncio.to_thread(
                            client.operations.get,
                            operation=operation
                        )
                        
                        if operation.done:
                            break
                        
                        print(f"[VideoGenerator] â³ ç”Ÿæˆä¸­... ({waited}/{max_wait}s)")
                        await asyncio.sleep(poll_interval)
                        waited += poll_interval
                    
                    # æª¢æŸ¥çµæœ
                    if operation.error:
                        print(f"[VideoGenerator] âŒ Veo éŒ¯èª¤: {operation.error}")
                        continue
                    
                    if not operation.done:
                        print(f"[VideoGenerator] â±ï¸ Veo è¶…æ™‚")
                        continue
                    
                    # ç²å–å½±ç‰‡
                    response = operation.response
                    if response and hasattr(response, 'generated_videos') and response.generated_videos:
                        video_data = response.generated_videos[0]
                        
                        # ç²å–å½±ç‰‡ bytes
                        video_bytes = None
                        if hasattr(video_data, 'video') and hasattr(video_data.video, 'video_bytes'):
                            video_bytes = video_data.video.video_bytes
                        
                        if video_bytes:
                            video_base64 = base64.b64encode(video_bytes).decode()
                            video_url = f"data:video/mp4;base64,{video_base64}"
                            
                            print(f"[VideoGenerator] âœ… Veo å½±ç‰‡ç”ŸæˆæˆåŠŸï¼å¤§å°: {len(video_bytes) / 1024 / 1024:.2f} MB")
                            
                            return VideoResult(
                                video_url=video_url,
                                video_base64=video_base64,
                                thumbnail_url=None,
                                duration=veo_duration,
                                format=format_str,
                                file_size=len(video_bytes),
                                scene_images=None,
                                generation_method="veo"
                            )
                        else:
                            print(f"[VideoGenerator] ç„¡æ³•ç²å–å½±ç‰‡ bytes")
                    else:
                        print(f"[VideoGenerator] ç„¡å½±ç‰‡çµæœ")
                
            except asyncio.TimeoutError:
                print(f"[VideoGenerator] Veo {model_name} è¶…æ™‚")
                continue
            except Exception as e:
                error_msg = str(e)
                # æ‰“å°å®Œæ•´éŒ¯èª¤ä»¥ä¾¿èª¿è©¦
                print(f"[VideoGenerator] Veo {model_name} å®Œæ•´éŒ¯èª¤: {error_msg}")
                if "not found" in error_msg.lower() or "404" in error_msg:
                    print(f"[VideoGenerator] â†’ æ¨¡å‹ä¸å­˜åœ¨æˆ–æœªå•Ÿç”¨")
                elif "permission" in error_msg.lower() or "403" in error_msg:
                    print(f"[VideoGenerator] â†’ ç„¡æ¬Šé™å­˜å–æ­¤æ¨¡å‹")
                elif "quota" in error_msg.lower():
                    print(f"[VideoGenerator] â†’ é…é¡ä¸è¶³")
                continue
        
        print("[VideoGenerator] æ‰€æœ‰ Veo æ¨¡å‹éƒ½ä¸å¯ç”¨")
        return None
    
    def _build_veo_prompt(self, script: Dict[str, Any]) -> str:
        """
        æ§‹å»ºå„ªåŒ–çš„ Veo å½±ç‰‡æç¤ºè©
        æ¡ç”¨ Google å®˜æ–¹æ¨è–¦çš„æç¤ºè©æ ¼å¼
        """
        scenes = script.get("scenes", [])
        title = script.get("title", "")
        description = script.get("description", "")
        style = script.get("overall_style", "modern, professional")
        color_palette = script.get("color_palette", ["#6366F1", "#8B5CF6"])
        brand_name = script.get("brand_name", "")
        goal = script.get("goal", "")
        personality = script.get("personality", "professional")
        
        # äººç‰©é¢¨æ ¼æ˜ å°„
        personality_styles = {
            "professional": "clean, corporate, sophisticated",
            "friendly": "warm, inviting, approachable",
            "luxurious": "elegant, premium, high-end",
            "playful": "fun, colorful, energetic",
            "minimalist": "minimal, sleek, modern",
            "innovative": "futuristic, cutting-edge, tech",
            "trustworthy": "reliable, stable, genuine",
        }
        
        style_modifier = personality_styles.get(personality, "modern")
        
        # æå–ä¸»è¦è¦–è¦ºå…ƒç´ 
        visual_elements = []
        for scene in scenes[:3]:  # å–å‰ 3 å€‹å ´æ™¯çš„é‡é»
            visual = scene.get("visual_prompt", "")
            if visual:
                # æå–é—œéµè©
                visual_elements.append(visual)
        
        # æ§‹å»ºå„ªåŒ–çš„æç¤ºè©ï¼ˆéµå¾ª Google Veo æœ€ä½³å¯¦è¸ï¼‰
        # æ ¼å¼ï¼š[Camera movement] + [Subject] + [Action] + [Scene details] + [Style]
        
        primary_color = color_palette[0] if color_palette else "#6366F1"
        secondary_color = color_palette[1] if len(color_palette) > 1 else primary_color
        
        # ä¸»è¦–è¦ºæè¿°
        main_visual = visual_elements[0] if visual_elements else description
        
        # æ§‹å»ºå°ˆæ¥­ç´šæç¤ºè©
        prompt_parts = []
        
        # 1. é–‹å ´é¡é ­å‹•ä½œ
        camera_movements = [
            "Smooth cinematic dolly shot",
            "Dynamic tracking shot",
            "Elegant crane shot moving down",
            "Slow push-in shot",
            "Aerial drone shot descending",
        ]
        import random
        camera = random.choice(camera_movements)
        
        # 2. ä¸»é«”æè¿°
        subject = main_visual if main_visual else "modern product showcase"
        
        # 3. é¢¨æ ¼é—œéµè©
        style_keywords = f"{style_modifier}, {style}"
        
        # 4. æŠ€è¡“å“è³ªé—œéµè©
        quality_keywords = "8K, cinematic lighting, shallow depth of field, professional color grading, film grain"
        
        # 5. æ°›åœé—œéµè©
        if "luxury" in personality or "luxurious" in personality:
            atmosphere = "golden hour lighting, premium atmosphere, sophisticated"
        elif "playful" in personality:
            atmosphere = "bright, vibrant colors, energetic mood"
        elif "minimalist" in personality:
            atmosphere = "clean white background, soft shadows, negative space"
        else:
            atmosphere = "professional studio lighting, modern aesthetic"
        
        # çµ„åˆæœ€çµ‚æç¤ºè©
        prompt = f"""{camera}, {subject}. {style_keywords}. {atmosphere}. {quality_keywords}.

Visual narrative: {description}

Key visual elements:
- Color palette: {primary_color} and {secondary_color}
- Style: {style}
- Mood: {personality_styles.get(personality, 'professional')}

Technical requirements:
- Vertical 9:16 format for social media
- Smooth, continuous motion
- No text overlays
- Professional quality suitable for advertising
- Clean transitions if multiple scenes"""

        print(f"[VideoGenerator] ğŸ“ Veo æç¤ºè©:\n{prompt[:200]}...")
        
        return prompt.strip()
    
    async def _generate_with_imagen_ffmpeg(
        self,
        script: Dict[str, Any],
        project_id: str
    ) -> VideoResult:
        """
        ä½¿ç”¨ Imagen ç”Ÿæˆåœ–ç‰‡ + FFmpeg åˆæˆå½±ç‰‡
        """
        scenes = script.get("scenes", [])
        format_str = script.get("format", "9:16")
        color_palette = script.get("color_palette", ["#6366F1", "#8B5CF6"])
        total_duration = sum(s.get("duration_seconds", 5) for s in scenes)
        
        # è¨­å®šå°ºå¯¸
        if format_str == "9:16":
            width, height = 1080, 1920
        elif format_str == "16:9":
            width, height = 1920, 1080
        else:
            width, height = 1080, 1080
        
        # 1. ç”Ÿæˆæ‰€æœ‰å ´æ™¯åœ–ç‰‡
        scene_images: List[str] = []
        scene_audios: List[Optional[str]] = []
        
        for i, scene in enumerate(scenes):
            print(f"[VideoGenerator] ğŸ“¸ ç”Ÿæˆå ´æ™¯ {i+1}/{len(scenes)}")
            
            visual_prompt = scene.get("visual_prompt", "")
            text_overlay = scene.get("text_overlay")
            narration = scene.get("narration_text", "")
            
            # ç”Ÿæˆåœ–ç‰‡
            image_base64 = await self._generate_image(
                visual_prompt,
                color_palette,
                width,
                height,
                text_overlay,
                i + 1,
                len(scenes)
            )
            
            if image_base64:
                scene_images.append(image_base64)
            
            # ç”ŸæˆèªéŸ³
            audio_path = None
            if narration and EDGE_TTS_AVAILABLE:
                voice_style = scene.get("voice_emotion", "friendly")
                audio_path = await self._generate_tts(narration, project_id, i, voice_style)
            scene_audios.append(audio_path)
        
        print(f"[VideoGenerator] âœ… åœ–ç‰‡ç”Ÿæˆå®Œæˆï¼Œå…± {len(scene_images)} å¼µ")
        
        # 2. ç”ŸæˆèƒŒæ™¯éŸ³æ¨‚
        music_path = await self._generate_background_music(
            script.get("music_genre", "upbeat"),
            total_duration,
            project_id
        )
        
        # 3. ä½¿ç”¨ FFmpeg åˆæˆå½±ç‰‡
        video_path = await self._create_video_ffmpeg(
            scene_images,
            scenes,
            scene_audios,
            music_path,
            project_id,
            width,
            height
        )
        
        # 4. è®€å–å½±ç‰‡
        video_base64 = None
        video_url = ""
        file_size = 0
        generation_method = "imagen+ffmpeg"
        
        if video_path and os.path.exists(video_path):
            with open(video_path, "rb") as f:
                video_data = f.read()
                file_size = len(video_data)
                video_base64 = base64.b64encode(video_data).decode()
                video_url = f"data:video/mp4;base64,{video_base64}"
            
            print(f"[VideoGenerator] ğŸ‰ å½±ç‰‡åˆæˆæˆåŠŸï¼Œå¤§å°: {file_size / 1024 / 1024:.2f} MB")
            
            # æ¸…ç†
            try:
                os.remove(video_path)
            except:
                pass
        else:
            video_url = scene_images[0] if scene_images else ""
            generation_method = "placeholder"
        
        return VideoResult(
            video_url=video_url,
            video_base64=video_base64,
            thumbnail_url=scene_images[0] if scene_images else None,
            duration=total_duration,
            format=format_str,
            file_size=file_size,
            scene_images=scene_images,
            generation_method=generation_method
        )
    
    async def _generate_image(
        self,
        visual_prompt: str,
        color_palette: List[str],
        width: int,
        height: int,
        text_overlay: Optional[str],
        scene_num: int,
        total_scenes: int
    ) -> Optional[str]:
        """ç”Ÿæˆå ´æ™¯åœ–ç‰‡"""
        
        aspect_ratio = f"{width}:{height}"
        if width == 1080 and height == 1920:
            aspect_ratio = "9:16"
        elif width == 1920 and height == 1080:
            aspect_ratio = "16:9"
        elif width == height:
            aspect_ratio = "1:1"
        
        # 1. å˜—è©¦ä½¿ç”¨ Imagen
        client = vertexai_client or genai_client
        if client and visual_prompt:
            enhanced_prompt = f"""
            {visual_prompt}
            
            Style: Professional video frame, cinematic quality, {aspect_ratio} format.
            Technical: High resolution, sharp details, vibrant colors, modern aesthetic.
            """
            
            imagen_models = [
                "models/imagen-4.0-fast-generate-001",
                "models/gemini-2.0-flash-exp-image-generation",
                "models/imagen-4.0-generate-001",
            ]
            
            for model_name in imagen_models:
                try:
                    if hasattr(client.models, 'generate_images'):
                        response = await asyncio.wait_for(
                            asyncio.to_thread(
                                client.models.generate_images,
                                model=model_name,
                                prompt=enhanced_prompt
                            ),
                            timeout=60.0
                        )
                        
                        if response.generated_images:
                            image_data = response.generated_images[0].image.image_bytes
                            
                            # èª¿æ•´å°ºå¯¸
                            img = Image.open(io.BytesIO(image_data))
                            img = self._resize_image(img, width, height)
                            
                            # æ·»åŠ æ–‡å­—
                            if text_overlay:
                                img = self._add_text_overlay(img, text_overlay, color_palette)
                            
                            buffer = io.BytesIO()
                            img.save(buffer, format='PNG', quality=95)
                            
                            print(f"[VideoGenerator] âœ“ Imagen åœ–ç‰‡ç”ŸæˆæˆåŠŸ")
                            return f"data:image/png;base64,{base64.b64encode(buffer.getvalue()).decode()}"
                        
                except asyncio.TimeoutError:
                    continue
                except Exception as e:
                    if "not found" not in str(e).lower():
                        print(f"[VideoGenerator] Imagen éŒ¯èª¤: {str(e)[:80]}")
                    continue
        
        # 2. ç”Ÿæˆè¨­è¨ˆåœ–
        print(f"[VideoGenerator] ğŸ¨ å ´æ™¯ {scene_num}: ä½¿ç”¨è¨­è¨ˆåœ–")
        return self._generate_designed_image(color_palette, width, height, text_overlay, scene_num, total_scenes)
    
    def _resize_image(self, img: Image.Image, target_width: int, target_height: int) -> Image.Image:
        """èª¿æ•´åœ–ç‰‡å°ºå¯¸"""
        original_ratio = img.width / img.height
        target_ratio = target_width / target_height
        
        if original_ratio > target_ratio:
            new_width = int(img.height * target_ratio)
            left = (img.width - new_width) // 2
            img = img.crop((left, 0, left + new_width, img.height))
        else:
            new_height = int(img.width / target_ratio)
            top = (img.height - new_height) // 2
            img = img.crop((0, top, img.width, top + new_height))
        
        return img.resize((target_width, target_height), Image.Resampling.LANCZOS)
    
    def _add_text_overlay(self, img: Image.Image, text: str, color_palette: List[str]) -> Image.Image:
        """æ·»åŠ æ–‡å­—ç–ŠåŠ """
        draw = ImageDraw.Draw(img)
        width, height = img.size
        
        try:
            font_size = width // 18
            try:
                font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", font_size)
            except:
                font = ImageFont.load_default()
            
            text = text[:40] if len(text) > 40 else text
            bbox = draw.textbbox((0, 0), text, font=font)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]
            
            x = (width - text_width) // 2
            y = height // 2 - text_height // 2
            
            padding = 30
            draw.rounded_rectangle(
                [(x - padding, y - padding), (x + text_width + padding, y + text_height + padding)],
                radius=15,
                fill=(0, 0, 0, 180)
            )
            draw.text((x, y), text, fill=(255, 255, 255), font=font)
            
        except Exception as e:
            print(f"[VideoGenerator] æ–‡å­—ç¹ªè£½éŒ¯èª¤: {e}")
        
        return img
    
    def _generate_designed_image(
        self,
        color_palette: List[str],
        width: int,
        height: int,
        text_overlay: Optional[str],
        scene_num: int,
        total_scenes: int
    ) -> str:
        """ç”Ÿæˆè¨­è¨ˆåœ–"""
        if not PIL_AVAILABLE:
            return ""
        
        try:
            import random
            
            img = Image.new('RGB', (width, height))
            c1 = self._hex_to_rgb(color_palette[0] if color_palette else "#6366F1")
            c2 = self._hex_to_rgb(color_palette[1] if len(color_palette) > 1 else "#8B5CF6")
            
            # ç¹ªè£½æ¼¸å±¤
            for y in range(height):
                ratio = y / height
                r = int(c1[0] * (1 - ratio) + c2[0] * ratio)
                g = int(c1[1] * (1 - ratio) + c2[1] * ratio)
                b = int(c1[2] * (1 - ratio) + c2[2] * ratio)
                for x in range(width):
                    img.putpixel((x, y), (r, g, b))
            
            draw = ImageDraw.Draw(img)
            
            # è£é£¾
            for _ in range(8):
                cx = random.randint(0, width)
                cy = random.randint(0, height)
                radius = random.randint(50, 300)
                draw.ellipse(
                    [(cx - radius, cy - radius), (cx + radius, cy + radius)],
                    outline=(255, 255, 255),
                    width=2
                )
            
            # å ´æ™¯ç·¨è™Ÿ
            try:
                font_size = width // 25
                try:
                    font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", font_size)
                except:
                    font = ImageFont.load_default()
                draw.text((40, 40), f"Scene {scene_num}/{total_scenes}", fill=(255, 255, 255), font=font)
            except:
                pass
            
            # ä¸»è¦æ–‡å­—
            if text_overlay:
                try:
                    font_size = width // 15
                    try:
                        font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", font_size)
                    except:
                        font = ImageFont.load_default()
                    
                    text = text_overlay[:30]
                    bbox = draw.textbbox((0, 0), text, font=font)
                    text_width = bbox[2] - bbox[0]
                    text_height = bbox[3] - bbox[1]
                    
                    x = (width - text_width) // 2
                    y = height // 2 - text_height // 2
                    
                    padding = 40
                    draw.rounded_rectangle(
                        [(x - padding, y - padding), (x + text_width + padding, y + text_height + padding)],
                        radius=20,
                        fill=(0, 0, 0, 200)
                    )
                    draw.text((x, y), text, fill=(255, 255, 255), font=font)
                except:
                    pass
            
            # æµ®æ°´å°
            try:
                font = ImageFont.load_default()
                draw.text((width - 150, height - 50), "KingJam AI", fill=(255, 255, 255), font=font)
            except:
                pass
            
            buffer = io.BytesIO()
            img.save(buffer, format='PNG', quality=95)
            return f"data:image/png;base64,{base64.b64encode(buffer.getvalue()).decode()}"
            
        except Exception as e:
            print(f"[VideoGenerator] è¨­è¨ˆåœ–ç”ŸæˆéŒ¯èª¤: {e}")
            return ""
    
    async def _generate_tts(
        self,
        text: str,
        project_id: str,
        scene_idx: int,
        voice_style: str = "friendly"
    ) -> Optional[str]:
        """ç”Ÿæˆ TTS èªéŸ³"""
        if not EDGE_TTS_AVAILABLE:
            return None
        
        try:
            voice = self.TTS_VOICES.get(voice_style, self.TTS_VOICES["friendly"])
            audio_path = self.output_dir / f"tts_{project_id}_{scene_idx}.mp3"
            
            communicate = edge_tts.Communicate(text, voice)
            await communicate.save(str(audio_path))
            
            print(f"[VideoGenerator] ğŸ¤ TTS: {text[:30]}...")
            return str(audio_path)
            
        except Exception as e:
            print(f"[VideoGenerator] TTS éŒ¯èª¤: {e}")
            return None
    
    async def _generate_background_music(
        self,
        mood: str,
        duration: float,
        project_id: str
    ) -> Optional[str]:
        """ç”ŸæˆèƒŒæ™¯éŸ³æ¨‚"""
        try:
            music_path = self.output_dir / f"bgm_{project_id}.wav"
            
            sample_rate = 44100
            total_samples = int(duration * sample_rate)
            
            mood_freqs = {
                "upbeat": [261.63, 329.63, 392.00],
                "calm": [220.00, 277.18, 329.63],
                "energetic": [293.66, 369.99, 440.00],
                "inspirational": [246.94, 311.13, 369.99],
            }
            freqs = mood_freqs.get(mood, mood_freqs["upbeat"])
            
            audio_data = []
            for i in range(total_samples):
                t = i / sample_rate
                sample = sum(math.sin(2 * math.pi * f * t) * 0.12 for f in freqs)
                beat = math.sin(2 * math.pi * 2 * t) * 0.3 + 0.7
                sample *= beat
                
                fade_samples = int(sample_rate * 0.5)
                if i < fade_samples:
                    sample *= i / fade_samples
                elif i > total_samples - fade_samples:
                    sample *= (total_samples - i) / fade_samples
                
                audio_data.append(int(sample * 32767))
            
            with open(music_path, 'wb') as f:
                f.write(b'RIFF')
                f.write(struct.pack('<I', 36 + len(audio_data) * 2))
                f.write(b'WAVE')
                f.write(b'fmt ')
                f.write(struct.pack('<I', 16))
                f.write(struct.pack('<H', 1))
                f.write(struct.pack('<H', 1))
                f.write(struct.pack('<I', sample_rate))
                f.write(struct.pack('<I', sample_rate * 2))
                f.write(struct.pack('<H', 2))
                f.write(struct.pack('<H', 16))
                f.write(b'data')
                f.write(struct.pack('<I', len(audio_data) * 2))
                for s in audio_data:
                    f.write(struct.pack('<h', s))
            
            print(f"[VideoGenerator] ğŸµ èƒŒæ™¯éŸ³æ¨‚ ({duration:.1f}ç§’)")
            return str(music_path)
            
        except Exception as e:
            print(f"[VideoGenerator] èƒŒæ™¯éŸ³æ¨‚éŒ¯èª¤: {e}")
            return None
    
    async def _create_video_ffmpeg(
        self,
        scene_images: List[str],
        scenes: List[Dict],
        scene_audios: List[Optional[str]],
        music_path: Optional[str],
        project_id: str,
        width: int,
        height: int
    ) -> Optional[str]:
        """ä½¿ç”¨ FFmpeg åˆæˆå½±ç‰‡"""
        
        # æª¢æŸ¥ FFmpeg
        try:
            result = await asyncio.create_subprocess_exec(
                "ffmpeg", "-version",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            await result.communicate()
            if result.returncode != 0:
                return None
        except:
            print("[VideoGenerator] FFmpeg æœªå®‰è£")
            return None
        
        try:
            scale_filter = f"scale={width}:{height}:force_original_aspect_ratio=decrease,pad={width}:{height}:(ow-iw)/2:(oh-ih)/2:black"
            
            # ä¿å­˜åœ–ç‰‡
            image_paths = []
            for i, img_base64 in enumerate(scene_images):
                img_data = img_base64.split(",")[1] if "," in img_base64 else img_base64
                img_bytes = base64.b64decode(img_data)
                
                img_path = self.output_dir / f"scene_{project_id}_{i}.png"
                with open(img_path, "wb") as f:
                    f.write(img_bytes)
                
                duration = scenes[i].get("duration_seconds", 5) if i < len(scenes) else 5
                image_paths.append((str(img_path), duration))
            
            # ç”Ÿæˆæ¯å€‹å ´æ™¯çš„è¦–é »ç‰‡æ®µ
            segment_files = []
            for i, (img_path, duration) in enumerate(image_paths):
                segment_path = self.output_dir / f"segment_{project_id}_{i}.mp4"
                
                cmd = [
                    "ffmpeg", "-y",
                    "-loop", "1",
                    "-i", img_path,
                    "-t", str(duration),
                    "-vf", f"{scale_filter},format=yuv420p",
                    "-c:v", "libx264",
                    "-preset", "medium",
                    "-crf", "18",
                    "-r", "30",
                    "-pix_fmt", "yuv420p",
                    str(segment_path)
                ]
                
                process = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                await process.communicate()
                
                if os.path.exists(segment_path):
                    segment_files.append(str(segment_path))
            
            if not segment_files:
                return None
            
            # åˆä½µç‰‡æ®µ
            concat_file = self.output_dir / f"concat_{project_id}.txt"
            with open(concat_file, "w") as f:
                for seg in segment_files:
                    f.write(f"file '{seg}'\n")
            
            merged_video = self.output_dir / f"merged_{project_id}.mp4"
            
            cmd = [
                "ffmpeg", "-y",
                "-f", "concat",
                "-safe", "0",
                "-i", str(concat_file),
                "-c", "copy",
                str(merged_video)
            ]
            
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            await process.communicate()
            
            # æ·»åŠ èƒŒæ™¯éŸ³æ¨‚
            output_path = self.output_dir / f"video_{project_id}.mp4"
            
            if music_path and os.path.exists(music_path) and os.path.exists(merged_video):
                cmd = [
                    "ffmpeg", "-y",
                    "-i", str(merged_video),
                    "-i", music_path,
                    "-c:v", "copy",
                    "-c:a", "aac",
                    "-b:a", "192k",
                    "-shortest",
                    "-map", "0:v:0",
                    "-map", "1:a:0",
                    str(output_path)
                ]
                
                process = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                await process.communicate()
                
                if process.returncode != 0:
                    output_path = merged_video
            else:
                output_path = merged_video
            
            # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
            try:
                for seg in segment_files:
                    if os.path.exists(seg):
                        os.remove(seg)
                for i in range(len(scene_images)):
                    img_path = self.output_dir / f"scene_{project_id}_{i}.png"
                    if img_path.exists():
                        os.remove(img_path)
                if concat_file.exists():
                    os.remove(concat_file)
                if music_path and os.path.exists(music_path):
                    os.remove(music_path)
                if merged_video.exists() and str(merged_video) != str(output_path):
                    os.remove(merged_video)
            except:
                pass
            
            if os.path.exists(output_path):
                print(f"[VideoGenerator] ğŸ¬ FFmpeg å½±ç‰‡åˆæˆæˆåŠŸ")
                return str(output_path)
            
            return None
            
        except Exception as e:
            print(f"[VideoGenerator] FFmpeg éŒ¯èª¤: {e}")
            return None
    
    def _hex_to_rgb(self, hex_color: str) -> tuple:
        """HEX è½‰ RGB"""
        hex_color = hex_color.lstrip('#')
        return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))


# å–®ä¾‹å¯¦ä¾‹
video_generator = VideoGeneratorService()

"""
Video Generator Service - å½±ç‰‡ç”Ÿæˆæœå‹™ v3.0
============================================
ä½¿ç”¨ Google Vertex AI Veo æ¨¡å‹ç”Ÿæˆé«˜å“è³ªå½±ç‰‡
æ”¯æ´ï¼šVeo 3ã€Veo 3 Fastã€Imagen åœ–ç‰‡ç”Ÿæˆ
"""

import os
import uuid
import asyncio
import base64
import io
import tempfile
import struct
import math
import time
from pathlib import Path
from typing import Optional, List, Dict, Any, Callable
from pydantic import BaseModel

# é…ç½®
GOOGLE_GEMINI_KEY = os.getenv("GOOGLE_GEMINI_KEY")
GOOGLE_CLOUD_PROJECT = os.getenv("GOOGLE_CLOUD_PROJECT", "veo-saas-backend")
GOOGLE_CLOUD_LOCATION = os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")
GOOGLE_APPLICATION_CREDENTIALS = os.getenv("GOOGLE_APPLICATION_CREDENTIALS", "")

# åˆå§‹åŒ– Google GenAI Client
genai_client = None
vertexai_client = None

# æ–¹æ³• 1: ä½¿ç”¨ Vertex AI SDKï¼ˆæœå‹™å¸³æˆ¶èªè­‰ï¼‰
if GOOGLE_CLOUD_PROJECT and GOOGLE_APPLICATION_CREDENTIALS:
    try:
        from google import genai
        from google.genai import types
        
        # ä½¿ç”¨ Vertex AI æ¨¡å¼
        vertexai_client = genai.Client(
            vertexai=True,
            project=GOOGLE_CLOUD_PROJECT,
            location=GOOGLE_CLOUD_LOCATION,
        )
        print(f"[VideoGenerator] âœ“ Vertex AI Client åˆå§‹åŒ–æˆåŠŸ (å°ˆæ¡ˆ: {GOOGLE_CLOUD_PROJECT})")
    except Exception as e:
        print(f"[VideoGenerator] Vertex AI åˆå§‹åŒ–å¤±æ•—: {e}")

# æ–¹æ³• 2: ä½¿ç”¨ API Keyï¼ˆå‚™é¸ï¼‰
if not vertexai_client and GOOGLE_GEMINI_KEY:
    try:
        from google import genai
        genai_client = genai.Client(api_key=GOOGLE_GEMINI_KEY)
        print("[VideoGenerator] âœ“ GenAI Client (API Key) åˆå§‹åŒ–æˆåŠŸ")
    except ImportError:
        try:
            import google.genai as genai
            genai_client = genai.Client(api_key=GOOGLE_GEMINI_KEY)
            print("[VideoGenerator] âœ“ GenAI Client (API Key) åˆå§‹åŒ–æˆåŠŸ (fallback)")
        except ImportError:
            print("[VideoGenerator] âœ— google-genai SDK æœªå®‰è£")

# é¸æ“‡å¯ç”¨çš„ client
active_client = vertexai_client or genai_client
if active_client:
    print(f"[VideoGenerator] ä½¿ç”¨ {'Vertex AI' if vertexai_client else 'API Key'} æ¨¡å¼")

# å˜—è©¦å°å…¥ PIL
try:
    from PIL import Image, ImageDraw, ImageFont
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

# å˜—è©¦å°å…¥ edge-tts
try:
    import edge_tts
    EDGE_TTS_AVAILABLE = True
except ImportError:
    EDGE_TTS_AVAILABLE = False


# ============================================================
# Veo æ¨¡å‹é…ç½®
# ============================================================

VEO_MODELS = {
    "veo-3-fast": "veo-3.0-fast-generate-preview",  # å¿«é€Ÿç”Ÿæˆ
    "veo-3": "veo-3.0-generate-preview",            # é«˜å“è³ª
    "veo-2": "veo-2.0-generate-001",                # ç©©å®šç‰ˆæœ¬
}

IMAGEN_MODELS = {
    "fast": "models/imagen-4.0-fast-generate-001",
    "standard": "models/imagen-4.0-generate-001",
}


# ============================================================
# è³‡æ–™æ¨¡å‹
# ============================================================

class VideoResult(BaseModel):
    """å½±ç‰‡ç”Ÿæˆçµæœ"""
    video_url: str
    video_base64: Optional[str] = None
    thumbnail_url: Optional[str] = None
    duration: float
    format: str
    file_size: int
    scene_images: Optional[List[str]] = None
    generation_method: str = "unknown"  # veo, imagen+ffmpeg, placeholder


# ============================================================
# Video Generator ä¸»æœå‹™
# ============================================================

class VideoGeneratorService:
    """
    å½±ç‰‡ç”Ÿæˆæœå‹™ v3.0
    
    å„ªå…ˆä½¿ç”¨ Google Veo æ¨¡å‹ç›´æ¥ç”Ÿæˆå½±ç‰‡
    å‚™é¸æ–¹æ¡ˆï¼šImagen åœ–ç‰‡ + FFmpeg åˆæˆ
    """
    
    TTS_VOICES = {
        "female": "zh-TW-HsiaoChenNeural",
        "male": "zh-TW-YunJheNeural",
        "friendly": "zh-TW-HsiaoChenNeural",
        "professional": "zh-CN-XiaoxiaoNeural",
        "energetic": "zh-CN-YunyangNeural",
    }
    
    def __init__(self):
        self.output_dir = Path(tempfile.gettempdir()) / "kingjam_videos"
        self.output_dir.mkdir(exist_ok=True)
    
    async def generate_video(
        self,
        script: Dict[str, Any],
        progress_callback: Optional[Callable] = None,
        quality: str = "standard"
    ) -> VideoResult:
        """
        ç”Ÿæˆå½±ç‰‡
        
        å“è³ªç­‰ç´šï¼š
        - standard: Imagen åœ–ç‰‡ + FFmpeg åˆæˆ
        - premium: Veo 3 Fast
        - ultra: Veo 3 æœ€é«˜å“è³ª
        """
        project_id = script.get("project_id", str(uuid.uuid4()))
        scenes = script.get("scenes", [])
        total_duration = sum(s.get("duration_seconds", 5) for s in scenes)
        format_str = script.get("format", "9:16")
        color_palette = script.get("color_palette", ["#6366F1", "#8B5CF6"])
        
        if not scenes:
            raise ValueError("è…³æœ¬ä¸­æ²’æœ‰å ´æ™¯")
        
        quality_names = {"standard": "æ¨™æº–", "premium": "é«˜ç´š", "ultra": "é ‚ç´š"}
        print(f"[VideoGenerator] ğŸ¬ é–‹å§‹ç”Ÿæˆå½±ç‰‡ (å“è³ª: {quality_names.get(quality, quality)})")
        print(f"[VideoGenerator] ğŸ“‹ å ´æ™¯æ•¸: {len(scenes)}, ç¸½æ™‚é•·: {total_duration}ç§’")
        
        # æ ¹æ“šå“è³ªç­‰ç´šé¸æ“‡ç”Ÿæˆæ–¹æ³•
        if quality in ["premium", "ultra"]:
            # é«˜ç´š/é ‚ç´šï¼šä½¿ç”¨ Veo æ¨¡å‹
            veo_model = "veo-3.0-generate-preview" if quality == "ultra" else "veo-3.0-fast-generate-preview"
            print(f"[VideoGenerator] ğŸ¥ ä½¿ç”¨ Veo æ¨¡å‹: {veo_model}")
            
            video_result = await self._generate_with_veo(script, project_id, preferred_model=veo_model)
            if video_result:
                return video_result
            
            # Veo å¤±æ•—ï¼Œé™ç´šåˆ° Imagen + FFmpeg
            print("[VideoGenerator] âš ï¸ Veo ä¸å¯ç”¨ï¼Œé™ç´šåˆ° Imagen + FFmpeg")
        
        # æ¨™æº–å“è³ª æˆ– Veo å¤±æ•—çš„é™ç´šæ–¹æ¡ˆ
        print("[VideoGenerator] ğŸ“¸ ä½¿ç”¨ Imagen + FFmpeg æ–¹æ¡ˆ")
        return await self._generate_with_imagen_ffmpeg(script, project_id)
    
    async def _generate_with_veo(
        self,
        script: Dict[str, Any],
        project_id: str,
        preferred_model: str = "veo-3.0-fast-generate-preview"
    ) -> Optional[VideoResult]:
        """
        ä½¿ç”¨ Google Veo æ¨¡å‹ç›´æ¥ç”Ÿæˆå½±ç‰‡
        
        æ¨¡å‹é¸é …ï¼š
        - veo-3.0-generate-preview: é ‚ç´šå“è³ª
        - veo-3.0-fast-generate-preview: å¿«é€Ÿç”Ÿæˆ
        """
        client = vertexai_client or genai_client
        if not client:
            print("[VideoGenerator] Veo: æ²’æœ‰å¯ç”¨çš„ Client")
            return None
        
        scenes = script.get("scenes", [])
        format_str = script.get("format", "9:16")
        color_palette = script.get("color_palette", ["#6366F1", "#8B5CF6"])
        total_duration = sum(s.get("duration_seconds", 5) for s in scenes)
        
        # æ§‹å»ºå®Œæ•´çš„å½±ç‰‡æç¤ºè©
        video_prompt = self._build_veo_prompt(script)
        
        # è¨­å®šå½±ç‰‡åƒæ•¸
        aspect_ratio = "9:16" if format_str == "9:16" else "16:9" if format_str == "16:9" else "1:1"
        
        # å„ªå…ˆä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹
        veo_models = [preferred_model]
        # å‚™é¸æ¨¡å‹
        fallback_models = [
            "veo-3.0-fast-generate-preview",
            "veo-3.0-generate-preview", 
            "veo-2.0-generate-001",
        ]
        for m in fallback_models:
            if m not in veo_models:
                veo_models.append(m)
        
        for model_name in veo_models:
            try:
                print(f"[VideoGenerator] ğŸ¥ å˜—è©¦ Veo æ¨¡å‹: {model_name}")
                
                # èª¿ç”¨ Veo API ç”Ÿæˆå½±ç‰‡
                if hasattr(client.models, 'generate_videos'):
                    # Veo åªæ”¯æŒ 4, 6, 8 ç§’çš„å½±ç‰‡
                    veo_duration = 8  # ä½¿ç”¨æœ€é•·çš„ 8 ç§’
                    
                    # ç™¼èµ·ç”Ÿæˆè«‹æ±‚
                    operation = await asyncio.to_thread(
                        client.models.generate_videos,
                        model=model_name,
                        prompt=video_prompt,
                        config={
                            "aspect_ratio": aspect_ratio,
                            "duration_seconds": veo_duration,
                            "number_of_videos": 1,
                            "generate_audio": True,
                        }
                    )
                    
                    print(f"[VideoGenerator] ğŸ“¡ Operation: {operation.name}")
                    
                    # è¼ªè©¢ç­‰å¾…æ“ä½œå®Œæˆ
                    max_wait = 180  # æœ€å¤šç­‰å¾… 3 åˆ†é˜
                    poll_interval = 5
                    waited = 0
                    
                    while waited < max_wait:
                        # ä½¿ç”¨ client.operations.get ç²å–æœ€æ–°ç‹€æ…‹
                        operation = await asyncio.to_thread(
                            client.operations.get,
                            operation=operation
                        )
                        
                        if operation.done:
                            break
                        
                        print(f"[VideoGenerator] â³ ç”Ÿæˆä¸­... ({waited}/{max_wait}s)")
                        await asyncio.sleep(poll_interval)
                        waited += poll_interval
                    
                    # æª¢æŸ¥çµæœ
                    if operation.error:
                        print(f"[VideoGenerator] âŒ Veo éŒ¯èª¤: {operation.error}")
                        continue
                    
                    if not operation.done:
                        print(f"[VideoGenerator] â±ï¸ Veo è¶…æ™‚")
                        continue
                    
                    # ç²å–å½±ç‰‡
                    response = operation.response
                    if response and hasattr(response, 'generated_videos') and response.generated_videos:
                        video_data = response.generated_videos[0]
                        
                        # ç²å–å½±ç‰‡ bytes
                        video_bytes = None
                        if hasattr(video_data, 'video') and hasattr(video_data.video, 'video_bytes'):
                            video_bytes = video_data.video.video_bytes
                        
                        if video_bytes:
                            print(f"[VideoGenerator] âœ… Veo å½±ç‰‡ç”ŸæˆæˆåŠŸï¼å¤§å°: {len(video_bytes) / 1024 / 1024:.2f} MB")
                            
                            # ä¿å­˜åˆ°éœæ…‹ç›®éŒ„
                            static_dir = Path("/app/static/videos")
                            static_dir.mkdir(parents=True, exist_ok=True)
                            
                            video_filename = f"veo_{project_id}.mp4"
                            static_path = static_dir / video_filename
                            
                            with open(static_path, "wb") as f:
                                f.write(video_bytes)
                            
                            video_url = f"/video/download/{video_filename}"
                            print(f"[VideoGenerator] ğŸ“ Veo å½±ç‰‡å·²ä¿å­˜: {static_path}")
                            
                            return VideoResult(
                                video_url=video_url,
                                video_base64=None,
                                thumbnail_url=None,
                                duration=veo_duration,
                                format=format_str,
                                file_size=len(video_bytes),
                                scene_images=None,
                                generation_method="veo"
                            )
                        else:
                            print(f"[VideoGenerator] ç„¡æ³•ç²å–å½±ç‰‡ bytes")
                    else:
                        print(f"[VideoGenerator] ç„¡å½±ç‰‡çµæœ")
                
            except asyncio.TimeoutError:
                print(f"[VideoGenerator] Veo {model_name} è¶…æ™‚")
                continue
            except Exception as e:
                error_msg = str(e)
                # æ‰“å°å®Œæ•´éŒ¯èª¤ä»¥ä¾¿èª¿è©¦
                print(f"[VideoGenerator] Veo {model_name} å®Œæ•´éŒ¯èª¤: {error_msg}")
                if "not found" in error_msg.lower() or "404" in error_msg:
                    print(f"[VideoGenerator] â†’ æ¨¡å‹ä¸å­˜åœ¨æˆ–æœªå•Ÿç”¨")
                elif "permission" in error_msg.lower() or "403" in error_msg:
                    print(f"[VideoGenerator] â†’ ç„¡æ¬Šé™å­˜å–æ­¤æ¨¡å‹")
                elif "quota" in error_msg.lower():
                    print(f"[VideoGenerator] â†’ é…é¡ä¸è¶³")
                continue
        
        print("[VideoGenerator] æ‰€æœ‰ Veo æ¨¡å‹éƒ½ä¸å¯ç”¨")
        return None
    
    def _build_veo_prompt(self, script: Dict[str, Any]) -> str:
        """
        æ§‹å»ºå°ˆæ¥­ç´š Veo å½±ç‰‡æç¤ºè©
        æ¡ç”¨ Google Veo æœ€ä½³å¯¦è¸ + é›»å½±ç´šæ•˜äº‹çµæ§‹ + è² é¢æç¤ºè©
        """
        import random
        
        scenes = script.get("scenes", [])
        title = script.get("title", "")
        description = script.get("description", "")
        style = script.get("overall_style", "modern, professional")
        color_palette = script.get("color_palette", ["#6366F1", "#8B5CF6"])
        personality = script.get("personality", "professional")
        target_platform = script.get("target_platform", "tiktok")
        music_genre = script.get("music_genre", "upbeat")
        
        # é›»å½±ç´šé¢¨æ ¼æ˜ å°„ - å°ˆæ¥­å»£å‘Šç´šè¦–è¦ºæè¿°
        CINEMATIC_STYLES = {
            "professional": {
                "visual": "clean corporate aesthetic with polished glass and metal surfaces, geometric architectural compositions, premium office environment",
                "lighting": "soft three-point lighting setup with gentle fill, subtle rim lights creating depth, professional studio quality, even exposure",
                "camera": ["Smooth dolly forward", "Steady tracking shot", "Elegant crane descent", "Professional steadicam glide"],
                "atmosphere": "sophisticated confidence, premium quality, trustworthy, authoritative yet approachable",
                "color_grade": "neutral tones with subtle warm highlights, corporate blue accents, clean whites, professional color science",
                "reference": "Apple product videos, corporate brand films, TED talk cinematography"
            },
            "friendly": {
                "visual": "warm lifestyle scenes with natural textures, authentic human moments, cozy interiors, soft fabrics and wood elements",
                "lighting": "soft golden hour glow streaming through windows, natural daylight, warm practical lamps, flattering skin tones",
                "camera": ["Gentle handheld movement", "Intimate close-up", "Smooth follow shot", "Natural observational pan"],
                "atmosphere": "welcoming warmth, genuine human connection, approachable comfort, relatable authenticity",
                "color_grade": "warm orange and amber tones, soft lifted shadows, nostalgic film emulation, creamy highlights",
                "reference": "Google Pixel ads, Airbnb films, lifestyle brand content"
            },
            "luxurious": {
                "visual": "premium materials including marble, gold leaf, velvet, crystal reflections, high-end architectural details, fashion editorial aesthetic",
                "lighting": "dramatic key lighting with sparkling highlights, deep cinematic shadows, chiaroscuro technique, jewelry-style spotlights",
                "camera": ["Slow majestic crane shot", "Elegant orbit around subject", "Cinematic reveal", "Luxurious tracking dolly"],
                "atmosphere": "opulent grandeur, exclusive sophistication, timeless elegance, aspirational luxury",
                "color_grade": "rich blacks, golden highlights, deep contrast, film noir influence, desaturated with selective color",
                "reference": "Chanel No. 5, Rolex, Louis Vuitton campaigns, haute couture films"
            },
            "playful": {
                "visual": "vibrant saturated colors, dynamic geometric shapes, energetic compositions, bold graphic elements, pop art influence",
                "lighting": "bright even lighting with colorful gels, neon accents, RGB LED effects, festival atmosphere",
                "camera": ["Dynamic whip pan", "Energetic tracking shot", "Playful zoom in", "Quick-cut montage movement"],
                "atmosphere": "joyful energy, youthful excitement, creative fun, infectious enthusiasm",
                "color_grade": "highly saturated colors, boosted contrast, candy-colored palette, punchy processing",
                "reference": "Spotify Wrapped, Nintendo ads, Gen-Z brand content"
            },
            "minimalist": {
                "visual": "clean negative space dominating frame, simple geometric forms, Zen-like simplicity, single subject isolation",
                "lighting": "soft diffused light from large sources, minimal shadows, ethereal glow, clean studio environment",
                "camera": ["Static contemplative shot", "Slow subtle push in", "Clean tilt reveal", "Meditative static frame"],
                "atmosphere": "serene calm, thoughtful stillness, intentional simplicity, peaceful focus",
                "color_grade": "muted desaturated tones, pure whites and soft grays, subtle pastel accents, high-key processing",
                "reference": "Muji campaigns, Apple product photography, Scandinavian design films"
            },
            "innovative": {
                "visual": "futuristic elements, floating UI interfaces, holographic effects, sci-fi aesthetic, data visualization, tech environments",
                "lighting": "cool blue and cyan tech lighting, LED strips, screen glow effects, neon accents, volumetric light rays",
                "camera": ["Dynamic drone descent", "Matrix-style smooth motion", "Sci-fi tracking shot", "Tech reveal dolly"],
                "atmosphere": "cutting-edge innovation, future-forward vision, technological wonder, digital frontier",
                "color_grade": "cool blue tones, electric cyan accents, digital color banding, cyberpunk influence",
                "reference": "Tesla Cybertruck reveal, tech keynotes, sci-fi film aesthetics"
            },
            "trustworthy": {
                "visual": "authentic real-world moments, genuine unposed expressions, documentary-style framing, real locations",
                "lighting": "natural available light only, honest shadows, no artificial enhancement, true-to-life conditions",
                "camera": ["Documentary handheld", "Observational steady shot", "Authentic follow", "VÃ©ritÃ© style capture"],
                "atmosphere": "reliable authenticity, honest integrity, genuine trust, real human stories",
                "color_grade": "natural realistic colors, minimal grading, true-to-life tones, documentary processing",
                "reference": "Nike real athlete stories, Patagonia documentaries, P&G emotional ads"
            },
            "energetic": {
                "visual": "action-packed scenes with controlled motion blur, dynamic Dutch angles, sports aesthetic, physical movement",
                "lighting": "dramatic backlighting creating silhouettes, sun flares, high contrast action lighting, stadium lights",
                "camera": ["Fast tracking shot", "Dynamic steadicam run", "Explosive zoom", "Action sequence dolly"],
                "atmosphere": "adrenaline rush, powerful momentum, unstoppable energy, peak performance",
                "color_grade": "high contrast processing, punchy saturated colors, teal and orange blockbuster look",
                "reference": "Red Bull extreme sports, Nike Just Do It, action movie trailers"
            },
        }
        
        # ç²å–é¢¨æ ¼é…ç½®
        style_config = CINEMATIC_STYLES.get(personality, CINEMATIC_STYLES["professional"])
        
        # æå–å ´æ™¯è¦–è¦ºç²¾è¯å’Œè² é¢æç¤ºè©
        scene_visuals = []
        scene_negatives = []
        for scene in scenes[:4]:
            visual = scene.get("visual_prompt", "")
            if visual:
                scene_visuals.append(visual)
            negative = scene.get("negative_prompt", "")
            if negative:
                scene_negatives.append(negative)
        
        primary_color = color_palette[0] if color_palette else "#6366F1"
        secondary_color = color_palette[1] if len(color_palette) > 1 else primary_color
        
        # é¸æ“‡ç›¸æ‡‰çš„æ”å½±æ©Ÿé‹å‹•
        camera_move = random.choice(style_config["camera"])
        
        # æ ¹æ“šå ´æ™¯å…§å®¹æ±ºå®šä¸»é«”
        main_subject = scene_visuals[0] if scene_visuals else description or "elegant product presentation"
        
        # éŸ³æ¨‚æ°›åœæ˜ å°„
        MUSIC_VIBES = {
            "upbeat": "energetic rhythm driving the visual pace, upbeat tempo sync",
            "calm": "peaceful ambient soundscape, gentle flow",
            "emotional": "touching cinematic score building emotion, swelling strings",
            "epic": "powerful orchestral crescendo, dramatic build",
            "minimal": "subtle electronic beats, understated pulse",
            "inspirational": "uplifting motivational music, hopeful progression",
        }
        music_vibe = MUSIC_VIBES.get(music_genre, "modern contemporary soundtrack")
        
        # æ§‹å»ºå°ˆæ¥­ç´šæç¤ºè© - Google Veo æœ€ä½³æ ¼å¼
        prompt = f"""{camera_move} revealing {main_subject}.

VISUAL STYLE:
{style_config["visual"]}
Overall aesthetic: {style}, cinematic commercial quality
Style reference: {style_config["reference"]}

LIGHTING DESIGN:
{style_config["lighting"]}
Color temperature: {style_config["color_grade"]}

ATMOSPHERE & EMOTION:
{style_config["atmosphere"]}
Narrative context: {description}

TECHNICAL SPECIFICATIONS:
- Format: Vertical 9:16 optimized for {target_platform}
- Resolution: 1080x1920 Full HD, crisp and sharp
- Frame rate: 24fps cinematic motion with natural motion blur
- Depth of field: Shallow with beautiful circular bokeh
- Motion: {camera_move.lower()}, smooth and professional, no jitter
- Color palette: Primary {primary_color}, Secondary {secondary_color}
- Aspect ratio: Perfect 9:16 framing, no cropping needed

AUDIO SYNC:
Visual rhythm matching {music_vibe}

QUALITY REQUIREMENTS (MUST HAVE):
- 8K source quality downsampled to 1080p for maximum clarity
- Professional cinematography standards, broadcast ready
- Perfect color grading with accurate skin tones
- Subtle film grain for organic texture
- Sharp focus on subject with smooth focus transitions
- Clean plate, no compression artifacts
- Professional production value, advertising agency standard
- Natural movement, no AI jitter or morphing artifacts

AVOID (CRITICAL - DO NOT GENERATE):
- Blurry or out of focus footage
- Pixelated or low resolution output
- Distorted or morphing shapes
- Unnatural human movements or expressions
- Watermarks, logos, or text overlays
- Compression artifacts or banding
- Overexposed or underexposed areas
- Amateur or stock footage appearance
- AI-generated look or uncanny valley effect
- Jittery camera movement or frame drops
- Color banding in gradients
- Noisy or grainy footage (unless intentional film grain)"""

        print(f"[VideoGenerator] ğŸ“ Veo æç¤ºè© (é¢¨æ ¼: {personality}):\n{prompt[:400]}...")
        
        return prompt.strip()
    
    async def _generate_with_imagen_ffmpeg(
        self,
        script: Dict[str, Any],
        project_id: str
    ) -> VideoResult:
        """
        ä½¿ç”¨ Imagen ç”Ÿæˆåœ–ç‰‡ + FFmpeg åˆæˆå½±ç‰‡
        """
        scenes = script.get("scenes", [])
        format_str = script.get("format", "9:16")
        color_palette = script.get("color_palette", ["#6366F1", "#8B5CF6"])
        total_duration = sum(s.get("duration_seconds", 5) for s in scenes)
        
        # è¨­å®šå°ºå¯¸
        if format_str == "9:16":
            width, height = 1080, 1920
        elif format_str == "16:9":
            width, height = 1920, 1080
        else:
            width, height = 1080, 1080
        
        # 1. ç”Ÿæˆæ‰€æœ‰å ´æ™¯åœ–ç‰‡
        scene_images: List[str] = []
        scene_audios: List[Optional[str]] = []
        
        for i, scene in enumerate(scenes):
            print(f"[VideoGenerator] ğŸ“¸ ç”Ÿæˆå ´æ™¯ {i+1}/{len(scenes)}")
            
            visual_prompt = scene.get("visual_prompt", "")
            text_overlay = scene.get("text_overlay")
            narration = scene.get("narration_text", "")
            negative_prompt = scene.get("negative_prompt", "")
            quality_tags = scene.get("quality_tags", "")
            
            # ç”Ÿæˆåœ–ç‰‡ (å¸¶å“è³ªæ¨™ç±¤å’Œè² é¢æç¤ºè©)
            image_base64 = await self._generate_image(
                visual_prompt,
                color_palette,
                width,
                height,
                text_overlay,
                i + 1,
                len(scenes),
                negative_prompt,
                quality_tags
            )
            
            if image_base64:
                scene_images.append(image_base64)
            
            # ç”ŸæˆèªéŸ³
            audio_path = None
            if narration and EDGE_TTS_AVAILABLE:
                voice_style = scene.get("voice_emotion", "friendly")
                audio_path = await self._generate_tts(narration, project_id, i, voice_style)
            scene_audios.append(audio_path)
        
        print(f"[VideoGenerator] âœ… åœ–ç‰‡ç”Ÿæˆå®Œæˆï¼Œå…± {len(scene_images)} å¼µ")
        
        # 2. ç”ŸæˆèƒŒæ™¯éŸ³æ¨‚
        music_path = await self._generate_background_music(
            script.get("music_genre", "upbeat"),
            total_duration,
            project_id
        )
        
        # 3. ä½¿ç”¨ FFmpeg åˆæˆå½±ç‰‡
        video_path = await self._create_video_ffmpeg(
            scene_images,
            scenes,
            scene_audios,
            music_path,
            project_id,
            width,
            height
        )
        
        # 4. è™•ç†å½±ç‰‡è¼¸å‡º
        video_base64 = None
        video_url = ""
        file_size = 0
        generation_method = "imagen+ffmpeg"
        
        if video_path and os.path.exists(video_path):
            file_size = os.path.getsize(video_path)
            print(f"[VideoGenerator] ğŸ‰ å½±ç‰‡åˆæˆæˆåŠŸï¼Œå¤§å°: {file_size / 1024 / 1024:.2f} MB")
            
            # ç§»å‹•åˆ°éœæ…‹ç›®éŒ„ä¾›ä¸‹è¼‰
            static_dir = Path("/app/static/videos")
            static_dir.mkdir(parents=True, exist_ok=True)
            
            video_filename = f"video_{project_id}.mp4"
            static_path = static_dir / video_filename
            
            import shutil
            shutil.move(video_path, static_path)
            
            # è¿”å›å¯ä¸‹è¼‰çš„ URLï¼ˆç›¸å°è·¯å¾‘ï¼Œå‰ç«¯æœƒé€é API è«‹æ±‚ï¼‰
            video_url = f"/video/download/{video_filename}"
            
            print(f"[VideoGenerator] ğŸ“ å½±ç‰‡å·²ä¿å­˜: {static_path}")
        else:
            video_url = scene_images[0] if scene_images else ""
            generation_method = "placeholder"
        
        return VideoResult(
            video_url=video_url,
            video_base64=video_base64,
            thumbnail_url=scene_images[0] if scene_images else None,
            duration=total_duration,
            format=format_str,
            file_size=file_size,
            scene_images=scene_images,
            generation_method=generation_method
        )
    
    async def _generate_image(
        self,
        visual_prompt: str,
        color_palette: List[str],
        width: int,
        height: int,
        text_overlay: Optional[str],
        scene_num: int,
        total_scenes: int,
        negative_prompt: str = "",
        quality_tags: str = ""
    ) -> Optional[str]:
        """ç”Ÿæˆå ´æ™¯åœ–ç‰‡ - å°ˆæ¥­ç´šå“è³ª"""
        
        aspect_ratio = f"{width}:{height}"
        if width == 1080 and height == 1920:
            aspect_ratio = "9:16"
        elif width == 1920 and height == 1080:
            aspect_ratio = "16:9"
        elif width == height:
            aspect_ratio = "1:1"
        
        # é è¨­å“è³ªæ¨™ç±¤
        default_quality = "8K resolution, professional photography, cinematic quality, sharp focus, perfect exposure, broadcast ready, advertising standard, color graded, pristine image quality"
        
        # é è¨­è² é¢æç¤ºè©
        default_negative = "blurry, pixelated, low quality, distorted, deformed, bad anatomy, extra limbs, cropped, watermark, text, logo, amateur, stock photo, generic, overexposed, underexposed, noisy, grainy, jpeg artifacts, compression, bad lighting, harsh shadows, AI-generated look, uncanny valley"
        
        # åˆä½µå“è³ªæ¨™ç±¤
        final_quality = quality_tags if quality_tags else default_quality
        final_negative = negative_prompt if negative_prompt else default_negative
        
        # 1. å˜—è©¦ä½¿ç”¨ Imagen
        client = vertexai_client or genai_client
        if client and visual_prompt:
            # æ§‹å»ºå°ˆæ¥­ç´šå¢å¼·æç¤ºè©
            enhanced_prompt = f"""{visual_prompt}

STYLE & QUALITY:
Professional video frame for short-form content, {aspect_ratio} vertical format.
{final_quality}
Cinematic color grading, perfect white balance, professional studio lighting.
Sharp subject focus with beautiful bokeh background.
Clean composition optimized for mobile viewing.

TECHNICAL REQUIREMENTS:
- Resolution: {width}x{height} pixels, crisp and detailed
- Aspect ratio: {aspect_ratio} perfectly framed
- Color depth: Rich, accurate colors with smooth gradients
- Focus: Tack sharp on subject, gentle falloff
- Exposure: Perfect, no clipping in highlights or shadows
- Noise: Clean, grain-free image (or subtle film grain if appropriate)

MUST AVOID: {final_negative}"""
            
            imagen_models = [
                "models/imagen-4.0-fast-generate-001",
                "models/gemini-2.0-flash-exp-image-generation",
                "models/imagen-4.0-generate-001",
            ]
            
            for model_name in imagen_models:
                try:
                    if hasattr(client.models, 'generate_images'):
                        response = await asyncio.wait_for(
                            asyncio.to_thread(
                                client.models.generate_images,
                                model=model_name,
                                prompt=enhanced_prompt
                            ),
                            timeout=60.0
                        )
                        
                        if response.generated_images:
                            image_data = response.generated_images[0].image.image_bytes
                            
                            # èª¿æ•´å°ºå¯¸
                            img = Image.open(io.BytesIO(image_data))
                            img = self._resize_image(img, width, height)
                            
                            # æ·»åŠ æ–‡å­—
                            if text_overlay:
                                img = self._add_text_overlay(img, text_overlay, color_palette)
                            
                            buffer = io.BytesIO()
                            img.save(buffer, format='PNG', quality=95)
                            
                            print(f"[VideoGenerator] âœ“ Imagen åœ–ç‰‡ç”ŸæˆæˆåŠŸ (å ´æ™¯ {scene_num})")
                            return f"data:image/png;base64,{base64.b64encode(buffer.getvalue()).decode()}"
                        
                except asyncio.TimeoutError:
                    continue
                except Exception as e:
                    if "not found" not in str(e).lower():
                        print(f"[VideoGenerator] Imagen éŒ¯èª¤: {str(e)[:80]}")
                    continue
        
        # 2. ç”Ÿæˆè¨­è¨ˆåœ–
        print(f"[VideoGenerator] ğŸ¨ å ´æ™¯ {scene_num}: ä½¿ç”¨è¨­è¨ˆåœ–")
        return self._generate_designed_image(color_palette, width, height, text_overlay, scene_num, total_scenes)
    
    def _resize_image(self, img: Image.Image, target_width: int, target_height: int) -> Image.Image:
        """èª¿æ•´åœ–ç‰‡å°ºå¯¸"""
        original_ratio = img.width / img.height
        target_ratio = target_width / target_height
        
        if original_ratio > target_ratio:
            new_width = int(img.height * target_ratio)
            left = (img.width - new_width) // 2
            img = img.crop((left, 0, left + new_width, img.height))
        else:
            new_height = int(img.width / target_ratio)
            top = (img.height - new_height) // 2
            img = img.crop((0, top, img.width, top + new_height))
        
        return img.resize((target_width, target_height), Image.Resampling.LANCZOS)
    
    def _add_text_overlay(self, img: Image.Image, text: str, color_palette: List[str]) -> Image.Image:
        """æ·»åŠ æ–‡å­—ç–ŠåŠ """
        draw = ImageDraw.Draw(img)
        width, height = img.size
        
        try:
            font_size = width // 18
            try:
                font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", font_size)
            except:
                font = ImageFont.load_default()
            
            text = text[:40] if len(text) > 40 else text
            bbox = draw.textbbox((0, 0), text, font=font)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]
            
            x = (width - text_width) // 2
            y = height // 2 - text_height // 2
            
            padding = 30
            draw.rounded_rectangle(
                [(x - padding, y - padding), (x + text_width + padding, y + text_height + padding)],
                radius=15,
                fill=(0, 0, 0, 180)
            )
            draw.text((x, y), text, fill=(255, 255, 255), font=font)
            
        except Exception as e:
            print(f"[VideoGenerator] æ–‡å­—ç¹ªè£½éŒ¯èª¤: {e}")
        
        return img
    
    def _desaturate_color(self, rgb: tuple, factor: float = 0.4) -> tuple:
        """
        é™ä½é¡è‰²é£½å’Œåº¦
        factor: 0 = å®Œå…¨ç°åº¦, 1 = åŸè‰²
        """
        r, g, b = rgb
        gray = int(0.299 * r + 0.587 * g + 0.114 * b)
        return (
            int(gray + (r - gray) * factor),
            int(gray + (g - gray) * factor),
            int(gray + (b - gray) * factor)
        )
    
    def _generate_designed_image(
        self,
        color_palette: List[str],
        width: int,
        height: int,
        text_overlay: Optional[str],
        scene_num: int,
        total_scenes: int
    ) -> str:
        """
        ç”Ÿæˆé«˜ç´šè¨­è¨ˆæ„Ÿåœ–ç‰‡ - ä½å½©åº¦ã€æ¥µç°¡ã€ç¾ä»£é¢¨æ ¼
        
        è¨­è¨ˆé¢¨æ ¼åƒè€ƒï¼š
        - Apple æ¥µç°¡ä¸»ç¾©
        - é«˜ç«¯å“ç‰Œè¦–è¦º
        - åŒ—æ­è¨­è¨ˆç¾å­¸
        """
        if not PIL_AVAILABLE:
            return ""
        
        try:
            import random
            import math
            
            # ========== ä½å½©åº¦é…è‰²ç³»çµ± ==========
            
            # å ´æ™¯é…è‰²ä¸»é¡Œï¼ˆæ¯å€‹å ´æ™¯ä½¿ç”¨ä¸åŒä¸»é¡Œï¼‰
            DESIGN_THEMES = [
                {
                    "name": "Midnight",
                    "bg_start": (28, 32, 38),      # æ·±é‚ƒè—ç°
                    "bg_end": (18, 20, 24),        # è¿‘é»‘è‰²
                    "accent": (90, 95, 105),       # ä¸­æ€§ç°
                    "highlight": (160, 165, 175),  # æ·ºç°
                    "text": (235, 235, 240),       # è¿‘ç™½
                },
                {
                    "name": "Warm Stone",
                    "bg_start": (45, 42, 40),      # æš–ç°æ£•
                    "bg_end": (28, 26, 24),        # æ·±æ£•ç°
                    "accent": (120, 110, 100),     # æ²™è‰²
                    "highlight": (180, 170, 160),  # ç±³è‰²
                    "text": (240, 238, 235),       # æš–ç™½
                },
                {
                    "name": "Cool Slate",
                    "bg_start": (35, 40, 48),      # å†·ç°è—
                    "bg_end": (20, 22, 28),        # æ·±è—ç°
                    "accent": (80, 100, 120),      # é‹¼è—
                    "highlight": (140, 160, 180),  # æ·ºé‹¼è—
                    "text": (230, 235, 245),       # å†·ç™½
                },
                {
                    "name": "Forest",
                    "bg_start": (32, 38, 35),      # æ·±æ£®æ—ç¶ 
                    "bg_end": (18, 22, 20),        # è¿‘é»‘ç¶ 
                    "accent": (70, 90, 80),        # ç°ç¶ 
                    "highlight": (130, 150, 140),  # æ·ºç°ç¶ 
                    "text": (235, 240, 238),       # ç¶ ç™½
                },
                {
                    "name": "Dusty Rose",
                    "bg_start": (42, 36, 38),      # ç°ç«ç‘°
                    "bg_end": (24, 20, 22),        # æ·±ç«ç‘°ç°
                    "accent": (100, 80, 85),       # æš—ç«ç‘°
                    "highlight": (165, 145, 150),  # æ·ºç«ç‘°
                    "text": (242, 238, 240),       # ç²‰ç™½
                },
                {
                    "name": "Charcoal",
                    "bg_start": (38, 38, 38),      # ç´”ç°
                    "bg_end": (22, 22, 22),        # ç‚­é»‘
                    "accent": (85, 85, 85),        # ä¸­ç°
                    "highlight": (150, 150, 150),  # æ·ºç°
                    "text": (240, 240, 240),       # ç´”ç™½
                },
            ]
            
            # é¸æ“‡ä¸»é¡Œï¼ˆæ ¹æ“šå ´æ™¯ç·¨è™Ÿè¼ªæ›ï¼‰
            theme = DESIGN_THEMES[(scene_num - 1) % len(DESIGN_THEMES)]
            
            # ä¹Ÿå¯ä»¥æ ¹æ“šå“ç‰Œè‰²èª¿æ•´ä¸»é¡Œ
            if color_palette:
                brand_rgb = self._hex_to_rgb(color_palette[0])
                # å°‡å“ç‰Œè‰²é™ä½å½©åº¦å¾Œèå…¥
                desaturated = self._desaturate_color(brand_rgb, 0.25)
                theme["accent"] = desaturated
                theme["highlight"] = self._desaturate_color(brand_rgb, 0.35)
            
            img = Image.new('RGB', (width, height))
            
            # ========== é«˜ç´šæ¼¸å±¤èƒŒæ™¯ ==========
            bg_start = theme["bg_start"]
            bg_end = theme["bg_end"]
            
            for y in range(height):
                for x in range(width):
                    # å¤šå±¤æ¼¸å±¤æ··åˆ
                    # 1. åŸºç¤å‚ç›´æ¼¸å±¤
                    v_ratio = y / height
                    # 2. è¼•å¾®å¾‘å‘æ¼¸å±¤ï¼ˆä¸­å¿ƒç¨äº®ï¼‰
                    cx, cy = width / 2, height * 0.4
                    dist = math.sqrt((x - cx) ** 2 + (y - cy) ** 2)
                    max_dist = math.sqrt(cx ** 2 + cy ** 2)
                    r_ratio = dist / max_dist
                    
                    # æ··åˆæ¯”ä¾‹
                    ratio = v_ratio * 0.7 + r_ratio * 0.3
                    ratio = min(1.0, max(0.0, ratio))
                    
                    # è¼•å¾®å™ªé»ç´‹ç†ï¼ˆé«˜ç´šè³ªæ„Ÿï¼‰
                    noise = (random.random() - 0.5) * 4
                    
                    r = int(bg_start[0] + (bg_end[0] - bg_start[0]) * ratio + noise)
                    g = int(bg_start[1] + (bg_end[1] - bg_start[1]) * ratio + noise)
                    b = int(bg_start[2] + (bg_end[2] - bg_start[2]) * ratio + noise)
                    
                    # ç¢ºä¿åœ¨æœ‰æ•ˆç¯„åœ
                    r = max(0, min(255, r))
                    g = max(0, min(255, g))
                    b = max(0, min(255, b))
                    
                    img.putpixel((x, y), (r, g, b))
            
            draw = ImageDraw.Draw(img, 'RGBA')
            
            # ========== æ¥µç°¡å¹¾ä½•è£é£¾ ==========
            
            accent = theme["accent"]
            highlight = theme["highlight"]
            
            # è¨­è¨ˆé¢¨æ ¼é¸æ“‡ï¼ˆæ¯å€‹å ´æ™¯ä¸åŒï¼‰
            design_style = scene_num % 4
            
            if design_style == 0:
                # é¢¨æ ¼ 1: å¤§å‹åœ“å¼§
                arc_cx = width * 0.7
                arc_cy = height * 0.3
                arc_r = min(width, height) * 0.6
                for offset in range(3):
                    alpha = 15 - offset * 4
                    draw.arc(
                        [(arc_cx - arc_r - offset * 40, arc_cy - arc_r - offset * 40),
                         (arc_cx + arc_r + offset * 40, arc_cy + arc_r + offset * 40)],
                        start=180, end=300,
                        fill=(*highlight, alpha),
                        width=2
                    )
                    
            elif design_style == 1:
                # é¢¨æ ¼ 2: å°è§’ç·šæ¢
                line_count = 5
                for i in range(line_count):
                    offset = i * 80 - 100
                    alpha = 20 - i * 3
                    draw.line(
                        [(0, height * 0.3 + offset), (width, height * 0.7 + offset)],
                        fill=(*accent, max(5, alpha)),
                        width=1
                    )
                    
            elif design_style == 2:
                # é¢¨æ ¼ 3: åœ“å½¢è£é£¾ï¼ˆå³ä¸‹è§’ï¼‰
                circle_cx = width * 0.85
                circle_cy = height * 0.75
                for r in range(3):
                    radius = 150 + r * 60
                    alpha = 25 - r * 7
                    draw.ellipse(
                        [(circle_cx - radius, circle_cy - radius),
                         (circle_cx + radius, circle_cy + radius)],
                        outline=(*highlight, max(5, alpha)),
                        width=1
                    )
                    
            else:
                # é¢¨æ ¼ 4: æ¥µç°¡çŸ©å½¢
                rect_x = width * 0.1
                rect_y = height * 0.6
                rect_w = width * 0.3
                rect_h = height * 0.25
                draw.rectangle(
                    [(rect_x, rect_y), (rect_x + rect_w, rect_y + rect_h)],
                    outline=(*accent, 20),
                    width=1
                )
            
            # ========== å¾®å¦™å…‰æšˆï¼ˆèšå…‰ç‡ˆæ•ˆæœï¼‰==========
            glow_cx = width * 0.5
            glow_cy = height * 0.35
            for radius in range(300, 600, 30):
                alpha = int(8 * (600 - radius) / 300)
                draw.ellipse(
                    [(glow_cx - radius, glow_cy - radius),
                     (glow_cx + radius, glow_cy + radius)],
                    fill=(*highlight, max(1, alpha))
                )
            
            # ========== ç´”è¦–è¦ºè¨­è¨ˆï¼ˆç„¡æ–‡å­—ï¼‰==========
            # ä¸­å¤®è£é£¾å…ƒç´ ï¼ˆæ›¿ä»£æ–‡å­—ï¼‰
            center_x = width // 2
            center_y = int(height * 0.42)
            
            # ä¸­å¤®åœ“å½¢è£é£¾
            for r in range(3):
                radius = 60 + r * 25
                alpha = 30 - r * 8
                draw.ellipse(
                    [(center_x - radius, center_y - radius),
                     (center_x + radius, center_y + radius)],
                    outline=(*highlight, max(8, alpha)),
                    width=1
                )
            
            # ä¸­å¤®æ°´å¹³ç·š
            line_width = 120
            draw.line(
                [(center_x - line_width, center_y),
                 (center_x + line_width, center_y)],
                fill=(*accent, 40),
                width=1
            )
            
            # ä¸­å¤®å‚ç›´ç·š
            line_height = 80
            draw.line(
                [(center_x, center_y - line_height),
                 (center_x, center_y + line_height)],
                fill=(*accent, 40),
                width=1
            )
            
            # ========== é ‚éƒ¨æ¼¸å±¤é®ç½© ==========
            for y_pos in range(120):
                alpha = int((120 - y_pos) / 120 * 40)
                draw.line([(0, y_pos), (width, y_pos)], fill=(*bg_start, alpha))
            
            # ========== åº•éƒ¨æ¼¸å±¤é®ç½© ==========
            for y_pos in range(height - 150, height):
                alpha = int((y_pos - (height - 150)) / 150 * 60)
                draw.line([(0, y_pos), (width, y_pos)], fill=(*bg_end, alpha))
            
            # ========== é‚Šæ¡†è£é£¾ç·šï¼ˆæ¥µç´°ï¼‰==========
            margin = 40
            draw.rectangle(
                [(margin, margin), (width - margin, height - margin)],
                outline=(*accent, 15),
                width=1
            )
            
            buffer = io.BytesIO()
            img.save(buffer, format='PNG', quality=95)
            return f"data:image/png;base64,{base64.b64encode(buffer.getvalue()).decode()}"
            
        except Exception as e:
            print(f"[VideoGenerator] è¨­è¨ˆåœ–ç”ŸæˆéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return ""
    
    async def _generate_tts(
        self,
        text: str,
        project_id: str,
        scene_idx: int,
        voice_style: str = "friendly"
    ) -> Optional[str]:
        """ç”Ÿæˆ TTS èªéŸ³"""
        if not EDGE_TTS_AVAILABLE:
            return None
        
        try:
            voice = self.TTS_VOICES.get(voice_style, self.TTS_VOICES["friendly"])
            audio_path = self.output_dir / f"tts_{project_id}_{scene_idx}.mp3"
            
            communicate = edge_tts.Communicate(text, voice)
            await communicate.save(str(audio_path))
            
            print(f"[VideoGenerator] ğŸ¤ TTS: {text[:30]}...")
            return str(audio_path)
            
        except Exception as e:
            print(f"[VideoGenerator] TTS éŒ¯èª¤: {e}")
            return None
    
    async def _generate_background_music(
        self,
        mood: str,
        duration: float,
        project_id: str
    ) -> Optional[str]:
        """
        ç”Ÿæˆå°ˆæ¥­ç´šèƒŒæ™¯éŸ³æ¨‚
        
        åŒ…å«ï¼š
        - å’Œå¼¦é€²è¡Œ
        - ç¯€å¥å‹æ…‹
        - ä½éŸ³ç·š
        - ç’°å¢ƒéŸ³æ•ˆ
        - å‹•æ…‹è®ŠåŒ–
        """
        try:
            music_path = self.output_dir / f"bgm_{project_id}.wav"
            
            sample_rate = 44100
            total_samples = int(duration * sample_rate)
            
            # å°ˆæ¥­å’Œå¼¦é…ç½® - ä¸åŒæƒ…ç·’çš„å’Œå¼¦é€²è¡Œ
            MOOD_CONFIGS = {
                "upbeat": {
                    "chords": [
                        [261.63, 329.63, 392.00],  # C major
                        [293.66, 369.99, 440.00],  # D major
                        [329.63, 415.30, 493.88],  # E minor
                        [349.23, 440.00, 523.25],  # F major
                    ],
                    "bpm": 120,
                    "bass_octave": 0.5,
                    "brightness": 1.2,
                    "rhythm_intensity": 0.8,
                },
                "calm": {
                    "chords": [
                        [220.00, 277.18, 329.63],  # A minor
                        [246.94, 311.13, 369.99],  # B diminished
                        [261.63, 329.63, 392.00],  # C major
                        [293.66, 349.23, 440.00],  # D minor
                    ],
                    "bpm": 70,
                    "bass_octave": 0.25,
                    "brightness": 0.7,
                    "rhythm_intensity": 0.3,
                },
                "energetic": {
                    "chords": [
                        [329.63, 415.30, 493.88],  # E major
                        [369.99, 466.16, 554.37],  # F# minor
                        [392.00, 493.88, 587.33],  # G major
                        [440.00, 554.37, 659.25],  # A major
                    ],
                    "bpm": 140,
                    "bass_octave": 0.5,
                    "brightness": 1.4,
                    "rhythm_intensity": 1.0,
                },
                "emotional": {
                    "chords": [
                        [261.63, 311.13, 392.00],  # C sus2
                        [293.66, 349.23, 440.00],  # D minor
                        [220.00, 277.18, 329.63],  # A minor
                        [246.94, 293.66, 369.99],  # B minor 7
                    ],
                    "bpm": 80,
                    "bass_octave": 0.25,
                    "brightness": 0.9,
                    "rhythm_intensity": 0.4,
                },
                "epic": {
                    "chords": [
                        [261.63, 329.63, 392.00],  # C major
                        [220.00, 277.18, 329.63],  # A minor
                        [349.23, 440.00, 523.25],  # F major
                        [392.00, 493.88, 587.33],  # G major
                    ],
                    "bpm": 100,
                    "bass_octave": 0.5,
                    "brightness": 1.3,
                    "rhythm_intensity": 0.9,
                },
                "minimal": {
                    "chords": [
                        [261.63, 392.00],  # C5
                        [293.66, 440.00],  # D5
                        [329.63, 493.88],  # E5
                        [261.63, 392.00],  # C5
                    ],
                    "bpm": 90,
                    "bass_octave": 0.25,
                    "brightness": 0.6,
                    "rhythm_intensity": 0.2,
                },
                "inspirational": {
                    "chords": [
                        [261.63, 329.63, 392.00],  # C major
                        [329.63, 392.00, 493.88],  # E minor
                        [349.23, 440.00, 523.25],  # F major
                        [392.00, 493.88, 587.33],  # G major
                    ],
                    "bpm": 95,
                    "bass_octave": 0.5,
                    "brightness": 1.1,
                    "rhythm_intensity": 0.6,
                },
            }
            
            config = MOOD_CONFIGS.get(mood, MOOD_CONFIGS["upbeat"])
            chords = config["chords"]
            bpm = config["bpm"]
            bass_octave = config["bass_octave"]
            brightness = config["brightness"]
            rhythm_intensity = config["rhythm_intensity"]
            
            # è¨ˆç®—ç¯€æ‹
            beat_duration = 60.0 / bpm
            samples_per_beat = int(beat_duration * sample_rate)
            chord_duration = beat_duration * 4  # æ¯å€‹å’Œå¼¦æŒçºŒ 4 æ‹
            samples_per_chord = int(chord_duration * sample_rate)
            
            audio_data = []
            
            for i in range(total_samples):
                t = i / sample_rate
                
                # ç•¶å‰å’Œå¼¦
                chord_idx = int(t / chord_duration) % len(chords)
                freqs = chords[chord_idx]
                
                # å’Œå¼¦å¢ŠéŸ³ï¼ˆæŸ”å’Œçš„èƒŒæ™¯ï¼‰
                pad = 0.0
                for f in freqs:
                    # ä½¿ç”¨æ­£å¼¦æ³¢ + è¼•å¾®è«§æ³¢
                    pad += math.sin(2 * math.pi * f * t) * 0.08
                    pad += math.sin(2 * math.pi * f * 2 * t) * 0.02 * brightness  # å…«åº¦è«§æ³¢
                    pad += math.sin(2 * math.pi * f * 0.5 * t) * 0.04  # ä½å…«åº¦
                
                # ä½éŸ³ç·š
                bass_freq = freqs[0] * bass_octave
                bass = math.sin(2 * math.pi * bass_freq * t) * 0.12
                # ä½éŸ³åŒ…çµ¡ - åœ¨æ¯æ‹é–‹å§‹æ™‚å¼·èª¿
                beat_position = (t % beat_duration) / beat_duration
                bass_envelope = math.exp(-beat_position * 3) * 0.8 + 0.2
                bass *= bass_envelope
                
                # ç¯€å¥å…ƒç´  - è¼•å¾®çš„è„ˆè¡
                rhythm = 0.0
                if rhythm_intensity > 0.3:
                    pulse_freq = bpm / 60  # æ¯ç§’æ‹æ•¸
                    rhythm = math.sin(2 * math.pi * pulse_freq * t) * 0.05 * rhythm_intensity
                    # æ·»åŠ é«˜å¸½æ„Ÿè¦ºçš„é«˜é »
                    hihat_t = t % (beat_duration / 2)
                    if hihat_t < 0.01:
                        rhythm += 0.03 * rhythm_intensity
                
                # ç’°å¢ƒå±¤ - éå¸¸è¼•å¾®çš„å™ªéŸ³æ„Ÿ
                import random
                ambient = (random.random() - 0.5) * 0.01
                
                # å‹•æ…‹è®ŠåŒ– - æ ¹æ“šæ™‚é–“ä½ç½®èª¿æ•´éŸ³é‡
                progress = i / total_samples
                
                # é–‹å ´æ¼¸å…¥ï¼ˆå‰ 10%ï¼‰
                if progress < 0.1:
                    dynamics = progress / 0.1
                # çµå°¾æ¼¸å‡ºï¼ˆå¾Œ 15%ï¼‰
                elif progress > 0.85:
                    dynamics = (1.0 - progress) / 0.15
                # ä¸­é–“é«˜æ½®é»
                elif 0.4 < progress < 0.6:
                    dynamics = 1.0 + (0.5 - abs(progress - 0.5)) * 0.3
                else:
                    dynamics = 1.0
                
                # æ··åˆæ‰€æœ‰å…ƒç´ 
                sample = (pad + bass + rhythm + ambient) * dynamics * 0.7
                
                # è»Ÿé™å¹…
                if sample > 0.95:
                    sample = 0.95
                elif sample < -0.95:
                    sample = -0.95
                
                audio_data.append(int(sample * 32767))
            
            # å¯«å…¥ WAV æ–‡ä»¶ï¼ˆç«‹é«”è²ï¼‰
            with open(music_path, 'wb') as f:
                num_channels = 1
                bits_per_sample = 16
                byte_rate = sample_rate * num_channels * bits_per_sample // 8
                block_align = num_channels * bits_per_sample // 8
                data_size = len(audio_data) * bits_per_sample // 8
                
                f.write(b'RIFF')
                f.write(struct.pack('<I', 36 + data_size))
                f.write(b'WAVE')
                f.write(b'fmt ')
                f.write(struct.pack('<I', 16))  # PCM
                f.write(struct.pack('<H', 1))   # Audio format (PCM)
                f.write(struct.pack('<H', num_channels))
                f.write(struct.pack('<I', sample_rate))
                f.write(struct.pack('<I', byte_rate))
                f.write(struct.pack('<H', block_align))
                f.write(struct.pack('<H', bits_per_sample))
                f.write(b'data')
                f.write(struct.pack('<I', data_size))
                for s in audio_data:
                    f.write(struct.pack('<h', s))
            
            print(f"[VideoGenerator] ğŸµ å°ˆæ¥­èƒŒæ™¯éŸ³æ¨‚ ({mood}, {bpm}BPM, {duration:.1f}ç§’)")
            return str(music_path)
            
        except Exception as e:
            print(f"[VideoGenerator] èƒŒæ™¯éŸ³æ¨‚éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    async def _create_video_ffmpeg(
        self,
        scene_images: List[str],
        scenes: List[Dict],
        scene_audios: List[Optional[str]],
        music_path: Optional[str],
        project_id: str,
        width: int,
        height: int
    ) -> Optional[str]:
        """
        ä½¿ç”¨ FFmpeg åˆæˆå°ˆæ¥­ç´šå½±ç‰‡
        
        ç‰¹æ•ˆåŒ…å«ï¼š
        - Ken Burns æ•ˆæœï¼ˆç·©æ…¢ç¸®æ”¾/å¹³ç§»å‹•æ…‹ï¼‰
        - å ´æ™¯è½‰å ´ï¼ˆäº¤å‰æ·¡åŒ–ï¼‰
        - TTS èªéŸ³æ··åˆ
        - é«˜å“è³ªç·¨ç¢¼
        """
        
        # æª¢æŸ¥ FFmpeg
        try:
            result = await asyncio.create_subprocess_exec(
                "ffmpeg", "-version",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            await result.communicate()
            if result.returncode != 0:
                return None
        except:
            print("[VideoGenerator] FFmpeg æœªå®‰è£")
            return None
        
        try:
            import random
            
            # Ken Burns æ•ˆæœé…ç½® - è®“éœæ…‹åœ–ç‰‡ç”¢ç”Ÿå‹•æ…‹æ„Ÿ
            KEN_BURNS_EFFECTS = [
                # (èµ·å§‹ç¸®æ”¾, çµæŸç¸®æ”¾, Xåç§»æ–¹å‘, Yåç§»æ–¹å‘)
                (1.0, 1.15, 0.5, 0.5),    # ç·©æ…¢æ”¾å¤§ï¼Œä¸­å¿ƒ
                (1.15, 1.0, 0.5, 0.5),    # ç·©æ…¢ç¸®å°ï¼Œä¸­å¿ƒ
                (1.0, 1.12, 0.0, 0.0),    # æ”¾å¤§ï¼Œå·¦ä¸Šè§’
                (1.0, 1.12, 1.0, 1.0),    # æ”¾å¤§ï¼Œå³ä¸‹è§’
                (1.12, 1.0, 0.0, 1.0),    # ç¸®å°ï¼Œå·¦ä¸‹è§’
                (1.12, 1.0, 1.0, 0.0),    # ç¸®å°ï¼Œå³ä¸Šè§’
                (1.0, 1.08, 0.5, 0.0),    # å¾®æ”¾å¤§ï¼Œä¸Šä¸­
                (1.0, 1.08, 0.5, 1.0),    # å¾®æ”¾å¤§ï¼Œä¸‹ä¸­
            ]
            
            # è½‰å ´æ™‚é•·ï¼ˆç§’ï¼‰
            TRANSITION_DURATION = 0.5
            
            # ä¿å­˜åœ–ç‰‡ä¸¦ç”Ÿæˆå¸¶æ•ˆæœçš„ç‰‡æ®µ
            image_paths = []
            for i, img_base64 in enumerate(scene_images):
                img_data = img_base64.split(",")[1] if "," in img_base64 else img_base64
                img_bytes = base64.b64decode(img_data)
                
                img_path = self.output_dir / f"scene_{project_id}_{i}.png"
                with open(img_path, "wb") as f:
                    f.write(img_bytes)
                
                duration = scenes[i].get("duration_seconds", 5) if i < len(scenes) else 5
                camera_movement = scenes[i].get("camera_movement", "static") if i < len(scenes) else "static"
                image_paths.append((str(img_path), duration, camera_movement))
            
            # ç”Ÿæˆæ¯å€‹å ´æ™¯çš„è¦–é »ç‰‡æ®µï¼ˆå¸¶ Ken Burns æ•ˆæœï¼‰
            segment_files = []
            for i, (img_path, duration, camera_move) in enumerate(image_paths):
                segment_path = self.output_dir / f"segment_{project_id}_{i}.mp4"
                
                # æ ¹æ“šå ´æ™¯ç·¨è™Ÿé¸æ“‡ä¸åŒçš„ Ken Burns æ•ˆæœ
                effect = KEN_BURNS_EFFECTS[i % len(KEN_BURNS_EFFECTS)]
                start_scale, end_scale, x_dir, y_dir = effect
                
                # æ ¹æ“š camera_movement èª¿æ•´æ•ˆæœ
                if camera_move == "dolly_in":
                    start_scale, end_scale = 1.0, 1.2
                elif camera_move == "dolly_out":
                    start_scale, end_scale = 1.2, 1.0
                elif camera_move == "tracking":
                    x_dir = 0.0 if random.random() > 0.5 else 1.0
                elif camera_move == "crane_up":
                    y_dir = 1.0
                    start_scale, end_scale = 1.0, 1.1
                elif camera_move == "crane_down":
                    y_dir = 0.0
                    start_scale, end_scale = 1.1, 1.0
                elif camera_move == "orbit":
                    x_dir = 0.0
                    start_scale, end_scale = 1.05, 1.05
                
                # è¨ˆç®— FFmpeg zoompan æ¿¾é¡åƒæ•¸
                # zoompan åƒæ•¸ï¼šz=ç¸®æ”¾, x=Xä½ç½®, y=Yä½ç½®, d=ç¸½å¹€æ•¸, s=è¼¸å‡ºå°ºå¯¸, fps=å¹€ç‡
                fps = 30
                total_frames = int(duration * fps)
                
                # è¨ˆç®—ç¸®æ”¾å‹•ç•«
                # z å¾ start_scale åˆ° end_scale
                # ä½¿ç”¨ easing è®“å‹•ç•«æ›´æµæš¢
                zoom_expr = f"if(lte(on,1),{start_scale},{start_scale}+(on/{total_frames})*({end_scale}-{start_scale}))"
                
                # è¨ˆç®—ä½ç½®ï¼ˆè®“åœ–ç‰‡åœ¨æ”¾å¤§æ™‚é©ç•¶åç§»ï¼‰
                # ç•¶æ”¾å¤§æ™‚ï¼Œä½ç½®å¾ä¸­å¿ƒå‘æŒ‡å®šæ–¹å‘åç§»
                x_offset = f"(iw-iw/zoom)/2 + (iw/zoom-iw)*{x_dir}*(on/{total_frames})"
                y_offset = f"(ih-ih/zoom)/2 + (ih/zoom-ih)*{y_dir}*(on/{total_frames})"
                
                # æ§‹å»º zoompan æ¿¾é¡
                zoompan_filter = f"zoompan=z='{zoom_expr}':x='{x_offset}':y='{y_offset}':d={total_frames}:s={width}x{height}:fps={fps}"
                
                # æ·»åŠ æ·¡å…¥æ•ˆæœï¼ˆç¬¬ä¸€å€‹å ´æ™¯ï¼‰å’Œæ·¡å‡ºæ•ˆæœï¼ˆæœ€å¾Œä¸€å€‹å ´æ™¯ï¼‰
                fade_filter = ""
                if i == 0:
                    fade_filter = f",fade=t=in:st=0:d=0.5"
                if i == len(image_paths) - 1:
                    fade_filter += f",fade=t=out:st={duration - 0.5}:d=0.5"
                
                # å®Œæ•´çš„è¦–è¦ºæ¿¾é¡éˆ
                video_filter = f"{zoompan_filter}{fade_filter},format=yuv420p"
                
                cmd = [
                    "ffmpeg", "-y",
                    "-loop", "1",
                    "-i", img_path,
                    "-t", str(duration),
                    "-vf", video_filter,
                    "-c:v", "libx264",
                    "-preset", "slow",       # æ›´é«˜å“è³ª
                    "-crf", "18",            # é«˜å“è³ª
                    "-profile:v", "high",    # H.264 High Profile
                    "-level", "4.1",         # æ”¯æ´ 1080p@30fps
                    "-r", str(fps),
                    "-pix_fmt", "yuv420p",
                    "-movflags", "+faststart",  # æ”¯æ´ç¶²é ä¸²æµ
                    str(segment_path)
                ]
                
                print(f"[VideoGenerator] ğŸï¸ å ´æ™¯ {i+1}: Ken Burns æ•ˆæœ ({start_scale:.2f}â†’{end_scale:.2f})")
                
                process = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                stdout, stderr = await process.communicate()
                
                if process.returncode != 0:
                    print(f"[VideoGenerator] å ´æ™¯ {i+1} FFmpeg éŒ¯èª¤: {stderr.decode()[:200]}")
                    # é™ç´šåˆ°ç°¡å–®æ¨¡å¼
                    simple_cmd = [
                        "ffmpeg", "-y",
                        "-loop", "1",
                        "-i", img_path,
                        "-t", str(duration),
                        "-vf", f"scale={width}:{height}:force_original_aspect_ratio=decrease,pad={width}:{height}:(ow-iw)/2:(oh-ih)/2:black,format=yuv420p",
                        "-c:v", "libx264",
                        "-preset", "medium",
                        "-crf", "20",
                        "-r", "30",
                        "-pix_fmt", "yuv420p",
                        str(segment_path)
                    ]
                    process = await asyncio.create_subprocess_exec(
                        *simple_cmd,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )
                    await process.communicate()
                
                if os.path.exists(segment_path):
                    segment_files.append(str(segment_path))
            
            if not segment_files:
                print("[VideoGenerator] âŒ æ²’æœ‰æˆåŠŸç”Ÿæˆçš„ç‰‡æ®µ")
                return None
            
            print(f"[VideoGenerator] âœ… {len(segment_files)} å€‹å ´æ™¯ç‰‡æ®µç”Ÿæˆå®Œæˆ")
            
            # ä½¿ç”¨ xfade è½‰å ´åˆä½µç‰‡æ®µï¼ˆäº¤å‰æ·¡åŒ–æ•ˆæœï¼‰
            merged_video = self.output_dir / f"merged_{project_id}.mp4"
            
            if len(segment_files) == 1:
                # åªæœ‰ä¸€å€‹ç‰‡æ®µï¼Œç›´æ¥è¤‡è£½
                import shutil
                shutil.copy(segment_files[0], str(merged_video))
            else:
                # æ§‹å»º xfade æ¿¾é¡éˆé€²è¡Œäº¤å‰æ·¡åŒ–è½‰å ´
                # è¨ˆç®—æ¯å€‹ç‰‡æ®µçš„æ™‚é•·ï¼ˆç”¨æ–¼è¨­ç½® offsetï¼‰
                offsets = []
                cumulative = 0
                for i, (_, dur, _) in enumerate(image_paths[:-1]):
                    cumulative += dur - TRANSITION_DURATION
                    offsets.append(cumulative)
                
                # æ§‹å»ºè¤‡é›œæ¿¾é¡
                inputs = " ".join([f"-i {seg}" for seg in segment_files])
                
                # ç”Ÿæˆ xfade æ¿¾é¡éˆ
                filter_complex = []
                prev_label = "[0:v]"
                
                for i in range(len(segment_files) - 1):
                    next_label = f"[{i+1}:v]"
                    output_label = f"[v{i}]" if i < len(segment_files) - 2 else "[vout]"
                    offset = offsets[i]
                    
                    # xfade è½‰å ´æ•ˆæœï¼šfade, fadeblack, fadewhite, distance, wipeleft, slideleft, etc.
                    transition_type = ["fade", "fadeblack", "slideleft", "slideright", "circlecrop"][i % 5]
                    
                    filter_complex.append(
                        f"{prev_label}{next_label}xfade=transition={transition_type}:duration={TRANSITION_DURATION}:offset={offset}{output_label}"
                    )
                    prev_label = output_label
                
                filter_str = ";".join(filter_complex)
                
                # åŸ·è¡Œå¸¶è½‰å ´çš„åˆä½µ
                cmd_str = f'ffmpeg -y {inputs} -filter_complex "{filter_str}" -map "[vout]" -c:v libx264 -preset slow -crf 18 -pix_fmt yuv420p {str(merged_video)}'
                
                process = await asyncio.create_subprocess_shell(
                    cmd_str,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                stdout, stderr = await process.communicate()
                
                if process.returncode != 0:
                    print(f"[VideoGenerator] xfade è½‰å ´å¤±æ•—ï¼Œä½¿ç”¨ç°¡å–®åˆä½µ: {stderr.decode()[:200]}")
                    # é™ç´šåˆ°ç°¡å–® concat
                    concat_file = self.output_dir / f"concat_{project_id}.txt"
                    with open(concat_file, "w") as f:
                        for seg in segment_files:
                            f.write(f"file '{seg}'\n")
                    
                    cmd = [
                        "ffmpeg", "-y",
                        "-f", "concat",
                        "-safe", "0",
                        "-i", str(concat_file),
                        "-c:v", "libx264",
                        "-preset", "slow",
                        "-crf", "18",
                        str(merged_video)
                    ]
                    
                    process = await asyncio.create_subprocess_exec(
                        *cmd,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )
                    await process.communicate()
                    
                    if concat_file.exists():
                        os.remove(concat_file)
            
            if not os.path.exists(merged_video):
                print("[VideoGenerator] âŒ å½±ç‰‡åˆä½µå¤±æ•—")
                return None
            
            print("[VideoGenerator] âœ… å½±ç‰‡è½‰å ´åˆä½µå®Œæˆ")
            
            # æ··åˆéŸ³è¨Šï¼ˆTTS + èƒŒæ™¯éŸ³æ¨‚ï¼‰
            output_path = self.output_dir / f"video_{project_id}.mp4"
            
            # åˆä½µæ‰€æœ‰ TTS éŸ³è¨Š
            tts_combined = None
            valid_audios = [(i, a) for i, a in enumerate(scene_audios) if a and os.path.exists(a)]
            
            if valid_audios:
                tts_combined = self.output_dir / f"tts_combined_{project_id}.mp3"
                
                # å‰µå»ºéœéŸ³ç‰‡æ®µå¡«å……
                audio_segments = []
                current_time = 0
                
                for i, (scene_idx, audio_path) in enumerate(valid_audios):
                    scene_start = sum(s.get("duration_seconds", 5) for s in scenes[:scene_idx])
                    
                    # å¦‚æœéœ€è¦åœ¨ TTS å‰æ·»åŠ éœéŸ³
                    if scene_start > current_time:
                        silence_duration = scene_start - current_time
                        silence_path = self.output_dir / f"silence_{project_id}_{i}.mp3"
                        cmd = [
                            "ffmpeg", "-y",
                            "-f", "lavfi",
                            "-i", f"anullsrc=r=44100:cl=mono",
                            "-t", str(silence_duration),
                            "-c:a", "libmp3lame",
                            str(silence_path)
                        ]
                        process = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)
                        await process.communicate()
                        if os.path.exists(silence_path):
                            audio_segments.append(str(silence_path))
                    
                    audio_segments.append(audio_path)
                    
                    # ç²å– TTS éŸ³è¨Šæ™‚é•·
                    probe_cmd = ["ffprobe", "-v", "error", "-show_entries", "format=duration", "-of", "csv=p=0", audio_path]
                    probe = await asyncio.create_subprocess_exec(*probe_cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)
                    probe_out, _ = await probe.communicate()
                    try:
                        tts_duration = float(probe_out.decode().strip())
                    except:
                        tts_duration = 3
                    current_time = scene_start + tts_duration
                
                # åˆä½µæ‰€æœ‰éŸ³è¨Šæ®µ
                if audio_segments:
                    audio_concat = self.output_dir / f"audio_concat_{project_id}.txt"
                    with open(audio_concat, "w") as f:
                        for seg in audio_segments:
                            f.write(f"file '{seg}'\n")
                    
                    cmd = [
                        "ffmpeg", "-y",
                        "-f", "concat",
                        "-safe", "0",
                        "-i", str(audio_concat),
                        "-c:a", "libmp3lame",
                        "-b:a", "192k",
                        str(tts_combined)
                    ]
                    process = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)
                    await process.communicate()
                    
                    # æ¸…ç†
                    if audio_concat.exists():
                        os.remove(audio_concat)
                    for seg in audio_segments:
                        if seg not in [a for _, a in valid_audios] and os.path.exists(seg):
                            os.remove(seg)
            
            # æœ€çµ‚æ··éŸ³
            if music_path and os.path.exists(music_path) and os.path.exists(merged_video):
                if tts_combined and os.path.exists(tts_combined):
                    # æ··åˆ TTS + èƒŒæ™¯éŸ³æ¨‚
                    cmd = [
                        "ffmpeg", "-y",
                        "-i", str(merged_video),
                        "-i", str(tts_combined),
                        "-i", music_path,
                        "-filter_complex",
                        "[1:a]volume=1.2[tts];[2:a]volume=0.3[bgm];[tts][bgm]amix=inputs=2:duration=longest[aout]",
                        "-map", "0:v:0",
                        "-map", "[aout]",
                        "-c:v", "copy",
                        "-c:a", "aac",
                        "-b:a", "192k",
                        "-shortest",
                        str(output_path)
                    ]
                else:
                    # åªæœ‰èƒŒæ™¯éŸ³æ¨‚
                    cmd = [
                        "ffmpeg", "-y",
                        "-i", str(merged_video),
                        "-i", music_path,
                        "-filter_complex", "[1:a]volume=0.5[bgm]",
                        "-map", "0:v:0",
                        "-map", "[bgm]",
                        "-c:v", "copy",
                        "-c:a", "aac",
                        "-b:a", "192k",
                        "-shortest",
                        str(output_path)
                    ]
                
                process = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                stdout, stderr = await process.communicate()
                
                if process.returncode != 0:
                    print(f"[VideoGenerator] éŸ³è¨Šæ··åˆå¤±æ•—: {stderr.decode()[:200]}")
                    output_path = merged_video
            elif tts_combined and os.path.exists(tts_combined):
                # åªæœ‰ TTS
                cmd = [
                    "ffmpeg", "-y",
                    "-i", str(merged_video),
                    "-i", str(tts_combined),
                    "-c:v", "copy",
                    "-c:a", "aac",
                    "-b:a", "192k",
                    "-map", "0:v:0",
                    "-map", "1:a:0",
                    "-shortest",
                    str(output_path)
                ]
                process = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)
                await process.communicate()
            else:
                output_path = merged_video
            
            # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
            try:
                for seg in segment_files:
                    if os.path.exists(seg):
                        os.remove(seg)
                for i in range(len(scene_images)):
                    img_path = self.output_dir / f"scene_{project_id}_{i}.png"
                    if img_path.exists():
                        os.remove(img_path)
                if music_path and os.path.exists(music_path):
                    os.remove(music_path)
                if tts_combined and os.path.exists(tts_combined):
                    os.remove(tts_combined)
                if merged_video.exists() and str(merged_video) != str(output_path):
                    os.remove(merged_video)
                # æ¸…ç† TTS éŸ³è¨Š
                for audio in scene_audios:
                    if audio and os.path.exists(audio):
                        os.remove(audio)
            except Exception as cleanup_err:
                print(f"[VideoGenerator] æ¸…ç†è­¦å‘Š: {cleanup_err}")
            
            if os.path.exists(output_path):
                print(f"[VideoGenerator] ğŸ¬ å°ˆæ¥­ç´šå½±ç‰‡åˆæˆæˆåŠŸï¼")
                return str(output_path)
            
            return None
            
        except Exception as e:
            print(f"[VideoGenerator] FFmpeg éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _hex_to_rgb(self, hex_color: str) -> tuple:
        """HEX è½‰ RGB"""
        hex_color = hex_color.lstrip('#')
        return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))


# å–®ä¾‹å¯¦ä¾‹
video_generator = VideoGeneratorService()

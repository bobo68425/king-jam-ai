"""
Video Generator Service - å½±ç‰‡ç”Ÿæˆæœå‹™ v3.1
============================================
æ”¯æ´å¤šç¨® AI å½±ç‰‡ç”Ÿæˆæ¨¡å‹ï¼š
- Google Veo 3 / Veo 3 Fastï¼ˆé ‚ç´šå“è³ªï¼‰
- Kling AIï¼ˆé«˜æ€§åƒ¹æ¯”ï¼‰
- Imagen + FFmpegï¼ˆåŸºç¤åˆæˆï¼‰
"""

import os
import uuid
import asyncio
import base64
import io
import tempfile
import struct
import math
import time
from pathlib import Path
from typing import Optional, List, Dict, Any, Callable
from pydantic import BaseModel

# é…ç½®
GOOGLE_GEMINI_KEY = os.getenv("GOOGLE_GEMINI_KEY")
GOOGLE_CLOUD_PROJECT = os.getenv("GOOGLE_CLOUD_PROJECT", "veo-saas-backend")
GOOGLE_CLOUD_LOCATION = os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")
GOOGLE_APPLICATION_CREDENTIALS = os.getenv("GOOGLE_APPLICATION_CREDENTIALS", "")

# Kling AI é…ç½®ï¼ˆé€é Replicateï¼‰
REPLICATE_API_TOKEN = os.getenv("REPLICATE_API_TOKEN", "")

# åˆå§‹åŒ– Google GenAI Client
genai_client = None
vertexai_client = None

# æª¢æŸ¥æ˜¯å¦åœ¨ Cloud Run ç’°å¢ƒï¼ˆæœ‰é è¨­æœå‹™å¸³æˆ¶ï¼‰
IS_CLOUD_RUN = os.getenv("K_SERVICE") is not None
GOOGLE_CLOUD_PROJECT_ACTUAL = os.getenv("GOOGLE_CLOUD_PROJECT") or os.getenv("GCP_PROJECT") or "king-jam-ai"

print(f"[VideoGenerator] ç’°å¢ƒæª¢æ¸¬: Cloud Run={IS_CLOUD_RUN}, Project={GOOGLE_CLOUD_PROJECT_ACTUAL}")

# æ–¹æ³• 1: å˜—è©¦ä½¿ç”¨ Vertex AI SDKï¼ˆCloud Run è‡ªå‹•æœ‰æœå‹™å¸³æˆ¶èªè­‰ï¼‰
try:
    from google import genai
    from google.genai import types
    
    # ä½¿ç”¨ Vertex AI æ¨¡å¼ï¼ˆCloud Run æœƒè‡ªå‹•ä½¿ç”¨æœå‹™å¸³æˆ¶ï¼‰
    vertexai_client = genai.Client(
        vertexai=True,
        project=GOOGLE_CLOUD_PROJECT_ACTUAL,
        location=GOOGLE_CLOUD_LOCATION,
    )
    print(f"[VideoGenerator] âœ“ Vertex AI Client åˆå§‹åŒ–æˆåŠŸ (å°ˆæ¡ˆ: {GOOGLE_CLOUD_PROJECT_ACTUAL})")
except ImportError as e:
    print(f"[VideoGenerator] Vertex AI å°å…¥å¤±æ•—: {e}")
except Exception as e:
    print(f"[VideoGenerator] Vertex AI åˆå§‹åŒ–å¤±æ•—: {e}")

# æ–¹æ³• 2: ä½¿ç”¨ API Keyï¼ˆå‚™é¸ï¼‰
if not vertexai_client and GOOGLE_GEMINI_KEY:
    try:
        from google import genai
        genai_client = genai.Client(api_key=GOOGLE_GEMINI_KEY)
        print("[VideoGenerator] âœ“ GenAI Client (API Key) åˆå§‹åŒ–æˆåŠŸ")
    except ImportError:
        try:
            import google.genai as genai
            genai_client = genai.Client(api_key=GOOGLE_GEMINI_KEY)
            print("[VideoGenerator] âœ“ GenAI Client (API Key) åˆå§‹åŒ–æˆåŠŸ (fallback)")
        except ImportError:
            print("[VideoGenerator] âœ— google-genai SDK æœªå®‰è£")

# é¸æ“‡å¯ç”¨çš„ client
active_client = vertexai_client or genai_client
if active_client:
    print(f"[VideoGenerator] ä½¿ç”¨ {'Vertex AI' if vertexai_client else 'API Key'} æ¨¡å¼")

# å˜—è©¦å°å…¥ PIL
try:
    from PIL import Image, ImageDraw, ImageFont
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

# å˜—è©¦å°å…¥ edge-tts
try:
    import edge_tts
    EDGE_TTS_AVAILABLE = True
except ImportError:
    EDGE_TTS_AVAILABLE = False

# Replicate Clientï¼ˆç”¨æ–¼ Kling AIï¼‰- å»¶é²åˆå§‹åŒ–ä»¥é¿å…ç›¸å®¹æ€§å•é¡Œ
replicate_client = None
REPLICATE_AVAILABLE = False

def get_replicate_client():
    """å»¶é²åˆå§‹åŒ– Replicate Client"""
    global replicate_client, REPLICATE_AVAILABLE
    if replicate_client is not None:
        return replicate_client
    
    if not REPLICATE_API_TOKEN:
        print("[VideoGenerator] âš ï¸ REPLICATE_API_TOKEN æœªè¨­å®šï¼ŒKling AI ä¸å¯ç”¨")
        return None
    
    try:
        import replicate
        replicate_client = replicate.Client(api_token=REPLICATE_API_TOKEN)
        REPLICATE_AVAILABLE = True
        print("[VideoGenerator] âœ“ Replicate Client åˆå§‹åŒ–æˆåŠŸ (Kling AI å¯ç”¨)")
        return replicate_client
    except ImportError:
        print("[VideoGenerator] âš ï¸ replicate SDK æœªå®‰è£ï¼ŒKling AI ä¸å¯ç”¨")
        return None
    except Exception as e:
        print(f"[VideoGenerator] âš ï¸ Replicate åˆå§‹åŒ–å¤±æ•—: {e}")
        return None


# ============================================================
# å…è²»å•†ç”¨èƒŒæ™¯éŸ³æ¨‚åº«
# ä¾†æºï¼šMixkit (https://mixkit.co) - å…è²»å•†ç”¨ï¼Œç„¡éœ€ç½²å
# æˆæ¬Šï¼šMixkit License - å¯ç”¨æ–¼å•†æ¥­é …ç›®ï¼Œç„¡éœ€æ¨™è¨»ä¾†æº
# ============================================================

FREE_MUSIC_LIBRARY = {
    # æ´»åŠ›å‹•æ„Ÿé¢¨æ ¼ (Upbeat / Energetic)
    "upbeat": [
        "https://assets.mixkit.co/music/preview/mixkit-tech-house-vibes-130.mp3",
        "https://assets.mixkit.co/music/preview/mixkit-hip-hop-02-738.mp3",
        "https://assets.mixkit.co/music/preview/mixkit-driving-ambition-32.mp3",
    ],
    # å‹µå¿—æŒ¯å¥®é¢¨æ ¼ (Inspirational / Motivational)
    "inspirational": [
        "https://assets.mixkit.co/music/preview/mixkit-spirit-of-the-game-132.mp3",
        "https://assets.mixkit.co/music/preview/mixkit-a-very-happy-christmas-897.mp3",
        "https://assets.mixkit.co/music/preview/mixkit-life-is-a-dream-837.mp3",
    ],
    # æ‚ é–’æ”¾é¬†é¢¨æ ¼ (Calm / Relaxing)
    "calm": [
        "https://assets.mixkit.co/music/preview/mixkit-serene-view-443.mp3",
        "https://assets.mixkit.co/music/preview/mixkit-sleepy-cat-135.mp3",
        "https://assets.mixkit.co/music/preview/mixkit-beautiful-dream-493.mp3",
    ],
    # é›»å½±å²è©©é¢¨æ ¼ (Epic / Cinematic)
    "epic": [
        "https://assets.mixkit.co/music/preview/mixkit-epic-orchestra-transition-2290.mp3",
        "https://assets.mixkit.co/music/preview/mixkit-cinematic-mystery-suspense-story-trailer-608.mp3",
        "https://assets.mixkit.co/music/preview/mixkit-epic-cinematic-trailer-115.mp3",
    ],
    # æƒ…æ„Ÿç´°è†©é¢¨æ ¼ (Emotional / Piano)
    "emotional": [
        "https://assets.mixkit.co/music/preview/mixkit-piano-reflections-22.mp3",
        "https://assets.mixkit.co/music/preview/mixkit-sad-piano-hope-464.mp3",
        "https://assets.mixkit.co/music/preview/mixkit-feeling-happy-5.mp3",
    ],
    # ç§‘æŠ€æœªä¾†é¢¨æ ¼ (Tech / Electronic)
    "minimal": [
        "https://assets.mixkit.co/music/preview/mixkit-games-worldbeat-466.mp3",
        "https://assets.mixkit.co/music/preview/mixkit-deep-urban-623.mp3",
        "https://assets.mixkit.co/music/preview/mixkit-complicated-305.mp3",
    ],
    # ä¼æ¥­å½¢è±¡ (Corporate)
    "corporate": [
        "https://assets.mixkit.co/music/preview/mixkit-tech-house-vibes-130.mp3",
        "https://assets.mixkit.co/music/preview/mixkit-a-very-happy-christmas-897.mp3",
        "https://assets.mixkit.co/music/preview/mixkit-driving-ambition-32.mp3",
    ],
}

def get_music_url_for_style(style: str) -> str:
    """
    æ ¹æ“šé¢¨æ ¼ç²å–å…è²»å•†ç”¨éŸ³æ¨‚ URL
    
    ä¾†æºï¼šMixkit - å…è²»å•†ç”¨éŸ³æ¨‚ï¼Œç„¡éœ€ç½²å
    æˆæ¬Šï¼šå¯ç”¨æ–¼å•†æ¥­é …ç›®ã€YouTubeã€ç¤¾äº¤åª’é«”ç­‰
    """
    import random
    
    # æ¨™æº–åŒ–é¢¨æ ¼åç¨±
    style_map = {
        "upbeat": "upbeat",
        "energetic": "upbeat", 
        "inspirational": "inspirational",
        "motivational": "inspirational",
        "faith": "inspirational",  # ä¿¡ä»°éˆæ€§é¢¨æ ¼ä½¿ç”¨å‹µå¿—éŸ³æ¨‚
        "worship": "inspirational",
        "calm": "calm",
        "relaxing": "calm",
        "chill": "calm",
        "epic": "epic",
        "cinematic": "epic",
        "dramatic": "epic",
        "emotional": "emotional",
        "piano": "emotional",
        "touching": "emotional",
        "minimal": "minimal",
        "tech": "minimal",
        "electronic": "minimal",
        "corporate": "corporate",
    }
    
    normalized_style = style_map.get(style.lower(), "upbeat")
    urls = FREE_MUSIC_LIBRARY.get(normalized_style, FREE_MUSIC_LIBRARY["upbeat"])
    
    if urls:
        return random.choice(urls)
    return FREE_MUSIC_LIBRARY["upbeat"][0]

# ============================================================
# æ¨¡å‹é…ç½®
# ============================================================

VEO_MODELS = {
    "veo-3-fast": "veo-3.0-fast-generate-preview",  # å¿«é€Ÿç”Ÿæˆ
    "veo-3": "veo-3.0-generate-preview",            # é«˜å“è³ª
    "veo-2": "veo-2.0-generate-001",                # ç©©å®šç‰ˆæœ¬
}

IMAGEN_MODELS = {
    "fast": "models/imagen-4.0-fast-generate-001",
    "standard": "models/imagen-4.0-generate-001",
}

# ç¬¬ä¸‰æ–¹ AI è¦–é »æ¨¡å‹é…ç½®ï¼ˆé€é Replicateï¼‰
THIRD_PARTY_VIDEO_MODELS = {
    # Kling v2.1 - å®˜æ–¹ Kuaishou æ¨¡å‹ (image-to-video)
    "kling": "kwaivgi/kling-v2.1",           # Kling v2.1 - 5ç§’/10ç§’, 720p/1080p
    # MiniMax å‚™ç”¨
    "minimax": "minimax/video-01",           # MiniMax Hailuo - 6ç§’é«˜å“è³ª
    "minimax-live": "minimax/video-01-live", # MiniMax Live - æ›´å¿«é€Ÿ
    "luma": "luma/ray",                      # Luma Dream Machine
}


# ============================================================
# è³‡æ–™æ¨¡å‹
# ============================================================

class VideoResult(BaseModel):
    """å½±ç‰‡ç”Ÿæˆçµæœ"""
    video_url: str
    video_base64: Optional[str] = None
    thumbnail_url: Optional[str] = None
    duration: float
    format: str
    file_size: int
    scene_images: Optional[List[str]] = None
    generation_method: str = "unknown"  # veo, imagen+ffmpeg, placeholder


# ============================================================
# Video Generator ä¸»æœå‹™
# ============================================================

class VideoGeneratorService:
    """
    å½±ç‰‡ç”Ÿæˆæœå‹™ v3.0
    
    å„ªå…ˆä½¿ç”¨ Google Veo æ¨¡å‹ç›´æ¥ç”Ÿæˆå½±ç‰‡
    å‚™é¸æ–¹æ¡ˆï¼šImagen åœ–ç‰‡ + FFmpeg åˆæˆ
    """
    
    # Edge TTS èªéŸ³æ˜ å°„ï¼ˆå·²é©—è­‰å¯ç”¨ï¼‰
    # åƒè€ƒ: https://learn.microsoft.com/azure/ai-services/speech-service/language-support
    # æ³¨æ„ï¼šåªåŒ…å«ç¶“éæ¸¬è©¦ç¢ºèªå¯ç”¨çš„èªéŸ³
    TTS_VOICES = {
        # åŸºæœ¬é¢¨æ ¼æ˜ å°„
        "female": "zh-TW-HsiaoChenNeural",
        "male": "zh-TW-YunJheNeural",
        "friendly": "zh-TW-HsiaoChenNeural",
        # ============================================================
        # é¢¨æ ¼åˆ¥åæ˜ å°„ï¼ˆæŒ‡å®šé¢¨æ ¼æœƒè‡ªå‹•é¸æ“‡é©åˆçš„èªéŸ³ï¼‰
        # ============================================================
        "professional": "zh-CN-XiaoxiaoNeural",  # æ›‰æ›‰ï¼ˆæº«æš–çŸ¥æ€§ï¼‰
        "energetic": "zh-CN-YunyangNeural",      # é›²æšï¼ˆå°ˆæ¥­æ–°èï¼‰
        "warm": "zh-TW-HsiaoYuNeural",           # æ›‰é›¨ï¼ˆæº«æŸ”ç”œç¾ï¼‰
        "gentle": "zh-TW-HsiaoYuNeural",         # æ›‰é›¨
        "news": "zh-CN-YunyangNeural",           # é›²æšï¼ˆå°ˆæ¥­æ–°èï¼‰
        "story": "zh-CN-XiaoxiaoNeural",         # æ›‰æ›‰ï¼ˆæº«æš–çŸ¥æ€§ï¼‰
        "young": "zh-CN-XiaoyiNeural",           # æ›‰ä¼Šï¼ˆæ´»æ½‘å¡é€šï¼‰
        "chat": "zh-CN-YunxiNeural",             # é›²å¸Œï¼ˆé™½å…‰æ´»åŠ›ï¼‰
        "elegant": "zh-TW-HsiaoChenNeural",      # æ›‰è‡»ï¼ˆè¦ªåˆ‡æ­£å¼ï¼‰
        "childlike": "zh-CN-YunxiaNeural",       # é›²å¤ï¼ˆå¯æ„›ç«¥è²ï¼‰
        "calm": "zh-TW-YunJheNeural",            # é›²å“²ï¼ˆå°ˆæ¥­ç©©é‡ï¼‰
        "sports": "zh-CN-YunjianNeural",         # é›²å¥ï¼ˆç†±æƒ…è§£èªªï¼‰
        "faith": "zh-TW-HsiaoYuNeural",          # æ›‰é›¨ï¼ˆæº«æŸ”å …å®šï¼Œé©åˆä¿¡ä»°å…§å®¹ï¼‰
        "worship": "zh-TW-HsiaoYuNeural",        # æ›‰é›¨
        
        # ============================================================
        # ç›´æ¥æŒ‡å®šèªéŸ³ IDï¼ˆç”¨æ–¼å“ç‰ŒåŒ…è¨­å®šï¼‰- å®˜æ–¹é©—è­‰å¯ç”¨ âœ“
        # ============================================================
        # ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ï¼‰
        "zh-TW-HsiaoChenNeural": "zh-TW-HsiaoChenNeural",
        "zh-TW-HsiaoYuNeural": "zh-TW-HsiaoYuNeural",
        "zh-TW-YunJheNeural": "zh-TW-YunJheNeural",
        # ç°¡é«”ä¸­æ–‡
        "zh-CN-XiaoxiaoNeural": "zh-CN-XiaoxiaoNeural",
        "zh-CN-XiaoyiNeural": "zh-CN-XiaoyiNeural",
        "zh-CN-YunyangNeural": "zh-CN-YunyangNeural",
        "zh-CN-YunjianNeural": "zh-CN-YunjianNeural",
        "zh-CN-YunxiNeural": "zh-CN-YunxiNeural",
        "zh-CN-YunxiaNeural": "zh-CN-YunxiaNeural",
        # ç°¡é«”ä¸­æ–‡æ–¹è¨€
        "zh-CN-liaoning-XiaobeiNeural": "zh-CN-liaoning-XiaobeiNeural",
        "zh-CN-shaanxi-XiaoniNeural": "zh-CN-shaanxi-XiaoniNeural",
        # ç²µèªï¼ˆé¦™æ¸¯ï¼‰
        "zh-HK-HiuMaanNeural": "zh-HK-HiuMaanNeural",
        "zh-HK-HiuGaaiNeural": "zh-HK-HiuGaaiNeural",
        "zh-HK-WanLungNeural": "zh-HK-WanLungNeural",
        # è‹±æ–‡
        "en-US-JennyNeural": "en-US-JennyNeural",
        "en-US-GuyNeural": "en-US-GuyNeural",
        "en-US-AriaNeural": "en-US-AriaNeural",
        "en-GB-SoniaNeural": "en-GB-SoniaNeural",
        "en-GB-RyanNeural": "en-GB-RyanNeural",
        # æ—¥æ–‡
        "ja-JP-NanamiNeural": "ja-JP-NanamiNeural",
        "ja-JP-KeitaNeural": "ja-JP-KeitaNeural",
        # éŸ“æ–‡
        "ko-KR-SunHiNeural": "ko-KR-SunHiNeural",
        "ko-KR-InJoonNeural": "ko-KR-InJoonNeural",
    }
    
    def __init__(self):
        self.output_dir = Path(tempfile.gettempdir()) / "kingjam_videos"
        self.output_dir.mkdir(exist_ok=True)
    
    async def generate_video(
        self,
        script: Dict[str, Any],
        progress_callback: Optional[Callable] = None,
        quality: str = "standard",
        custom_images: Optional[Dict[int, str]] = None,
        custom_music_base64: Optional[str] = None,
        custom_music_name: Optional[str] = None
    ) -> VideoResult:
        """
        ç”Ÿæˆå½±ç‰‡
        
        å“è³ªç­‰ç´šï¼š
        - standard: Imagen åœ–ç‰‡ + FFmpeg åˆæˆï¼ˆæ”¯æ´è‡ªè¨‚åœ–ç‰‡ï¼‰
        - kling: Kling AI 1.5ï¼ˆé«˜æ€§åƒ¹æ¯”ï¼‰
        - kling-pro: Kling AI 1.5 Proï¼ˆæ›´å¥½å“è³ªï¼‰
        - premium: Veo 3 Fast
        - ultra: Veo 3 æœ€é«˜å“è³ª
        
        custom_images: ç”¨æˆ¶è‡ªè¨‚å ´æ™¯åœ–ç‰‡ {scene_index: base64_or_url}
        custom_music_base64: ç”¨æˆ¶è‡ªè¨‚éŸ³æ¨‚ï¼ˆBase64 ç·¨ç¢¼ï¼‰
        custom_music_name: è‡ªè¨‚éŸ³æ¨‚æª”å
        """
        project_id = script.get("project_id", str(uuid.uuid4()))
        scenes = script.get("scenes", [])
        total_duration = sum(s.get("duration_seconds", 5) for s in scenes)
        format_str = script.get("format", "9:16")
        color_palette = script.get("color_palette", ["#6366F1", "#8B5CF6"])
        
        # ä¿å­˜è‡ªè¨‚åœ–ç‰‡å’ŒéŸ³æ¨‚ä¾›å¾ŒçºŒä½¿ç”¨
        self._custom_images = custom_images or {}
        self._custom_music_base64 = custom_music_base64
        self._custom_music_name = custom_music_name
        
        if not scenes:
            raise ValueError("è…³æœ¬ä¸­æ²’æœ‰å ´æ™¯")
        
        quality_names = {
            "standard": "æ¨™æº–åˆæˆ", 
            "kling": "Kling 5ç§’ 720p",
            "kling-10s": "Kling 10ç§’ 720p",
            "kling-pro": "Kling Pro 5ç§’ 1080p",
            "kling-pro-10s": "Kling Pro 10ç§’ 1080p",
            "premium": "Veo Fast 8ç§’", 
            "ultra": "Veo Pro 8ç§’"
        }
        print(f"[VideoGenerator] ğŸ¬ é–‹å§‹ç”Ÿæˆå½±ç‰‡ (æ¨¡å‹: {quality_names.get(quality, quality)})")
        print(f"[VideoGenerator] ğŸ“‹ å ´æ™¯æ•¸: {len(scenes)}, ç¸½æ™‚é•·: {total_duration}ç§’")
        
        # æ ¹æ“šå“è³ªç­‰ç´šé¸æ“‡ç”Ÿæˆæ–¹æ³•
        if quality.startswith("kling"):
            # Kling v2.1 å½±ç‰‡ç”Ÿæˆ (image-to-video)
            is_pro = "pro" in quality
            is_10s = "10s" in quality
            kling_duration = 10 if is_10s else 5
            print(f"[VideoGenerator] ğŸ¥ ä½¿ç”¨ Kling v2.1 æ¨¡å‹, Pro={is_pro}, æ™‚é•·: {kling_duration}ç§’")
            
            video_result = await self._generate_with_kling(script, project_id, model=quality, duration=kling_duration)
            if video_result:
                return video_result
            
            # Kling å¤±æ•—ï¼Œé™ç´šåˆ° Imagen + FFmpeg
            print("[VideoGenerator] âš ï¸ Kling ä¸å¯ç”¨ï¼Œé™ç´šåˆ° Imagen + FFmpeg")
        
        elif quality in ["premium", "ultra"]:
            # é«˜ç´š/é ‚ç´šï¼šä½¿ç”¨ Veo æ¨¡å‹
            veo_model = "veo-3.0-generate-preview" if quality == "ultra" else "veo-3.0-fast-generate-preview"
            print(f"[VideoGenerator] ğŸ¥ ä½¿ç”¨ Veo æ¨¡å‹: {veo_model}")
            
            video_result = await self._generate_with_veo(script, project_id, preferred_model=veo_model)
            if video_result:
                return video_result
            
            # Veo å¤±æ•—ï¼Œé™ç´šåˆ° Imagen + FFmpeg
            print("[VideoGenerator] âš ï¸ Veo ä¸å¯ç”¨ï¼Œé™ç´šåˆ° Imagen + FFmpeg")
        
        # æ¨™æº–å“è³ª æˆ– å…¶ä»–æ–¹æ¡ˆå¤±æ•—çš„é™ç´šæ–¹æ¡ˆ
        print("[VideoGenerator] ğŸ“¸ ä½¿ç”¨ Imagen + FFmpeg æ–¹æ¡ˆ")
        return await self._generate_with_imagen_ffmpeg(script, project_id)
    
    async def _generate_with_veo(
        self,
        script: Dict[str, Any],
        project_id: str,
        preferred_model: str = "veo-3.0-fast-generate-preview"
    ) -> Optional[VideoResult]:
        """
        ä½¿ç”¨ Google Veo æ¨¡å‹ç›´æ¥ç”Ÿæˆå½±ç‰‡
        
        æ¨¡å‹é¸é …ï¼š
        - veo-3.0-generate-preview: é ‚ç´šå“è³ª
        - veo-3.0-fast-generate-preview: å¿«é€Ÿç”Ÿæˆ
        """
        client = vertexai_client or genai_client
        if not client:
            print("[VideoGenerator] Veo: æ²’æœ‰å¯ç”¨çš„ Client")
            return None
        
        scenes = script.get("scenes", [])
        format_str = script.get("format", "9:16")
        color_palette = script.get("color_palette", ["#6366F1", "#8B5CF6"])
        total_duration = sum(s.get("duration_seconds", 5) for s in scenes)
        
        # æ§‹å»ºå®Œæ•´çš„å½±ç‰‡æç¤ºè©
        video_prompt = self._build_veo_prompt(script)
        
        # è¨­å®šå½±ç‰‡åƒæ•¸
        aspect_ratio = "9:16" if format_str == "9:16" else "16:9" if format_str == "16:9" else "1:1"
        
        # å„ªå…ˆä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹
        veo_models = [preferred_model]
        # å‚™é¸æ¨¡å‹
        fallback_models = [
            "veo-3.0-fast-generate-preview",
            "veo-3.0-generate-preview", 
            "veo-2.0-generate-001",
        ]
        for m in fallback_models:
            if m not in veo_models:
                veo_models.append(m)
        
        for model_name in veo_models:
            try:
                print(f"[VideoGenerator] ğŸ¥ å˜—è©¦ Veo æ¨¡å‹: {model_name}")
                
                # èª¿ç”¨ Veo API ç”Ÿæˆå½±ç‰‡
                if hasattr(client.models, 'generate_videos'):
                    # Veo åªæ”¯æŒ 4, 6, 8 ç§’çš„å½±ç‰‡
                    veo_duration = 8  # ä½¿ç”¨æœ€é•·çš„ 8 ç§’
                    
                    # ç™¼èµ·ç”Ÿæˆè«‹æ±‚ï¼ˆä¸ä½¿ç”¨ generate_audioï¼Œæˆ‘å€‘æœƒå–®ç¨è™•ç†éŸ³è¨Šï¼‰
                    config = {
                        "aspect_ratio": aspect_ratio,
                        "duration_seconds": veo_duration,
                        "number_of_videos": 1,
                    }
                    # generate_audio åªåœ¨ Vertex AI æ¨¡å¼ä¸‹æ”¯æ´
                    if client == vertexai_client:
                        config["generate_audio"] = True
                    
                    operation = await asyncio.to_thread(
                        client.models.generate_videos,
                        model=model_name,
                        prompt=video_prompt,
                        config=config
                    )
                    
                    print(f"[VideoGenerator] ğŸ“¡ Operation: {operation.name}")
                    
                    # è¼ªè©¢ç­‰å¾…æ“ä½œå®Œæˆ
                    max_wait = 180  # æœ€å¤šç­‰å¾… 3 åˆ†é˜
                    poll_interval = 5
                    waited = 0
                    
                    while waited < max_wait:
                        # ä½¿ç”¨ client.operations.get ç²å–æœ€æ–°ç‹€æ…‹
                        operation = await asyncio.to_thread(
                            client.operations.get,
                            operation=operation
                        )
                        
                        if operation.done:
                            break
                        
                        print(f"[VideoGenerator] â³ ç”Ÿæˆä¸­... ({waited}/{max_wait}s)")
                        await asyncio.sleep(poll_interval)
                        waited += poll_interval
                    
                    # æª¢æŸ¥çµæœ
                    if operation.error:
                        print(f"[VideoGenerator] âŒ Veo éŒ¯èª¤: {operation.error}")
                        continue
                    
                    if not operation.done:
                        print(f"[VideoGenerator] â±ï¸ Veo è¶…æ™‚")
                        continue
                    
                    # ç²å–å½±ç‰‡
                    response = operation.response
                    if response and hasattr(response, 'generated_videos') and response.generated_videos:
                        video_data = response.generated_videos[0]
                        
                        # ç²å–å½±ç‰‡ bytes
                        video_bytes = None
                        if hasattr(video_data, 'video') and hasattr(video_data.video, 'video_bytes'):
                            video_bytes = video_data.video.video_bytes
                        
                        if video_bytes:
                            print(f"[VideoGenerator] âœ… Veo å½±ç‰‡ç”ŸæˆæˆåŠŸï¼å¤§å°: {len(video_bytes) / 1024 / 1024:.2f} MB")
                            
                            # ä¿å­˜åˆ°éœæ…‹ç›®éŒ„
                            static_dir = Path("/app/static/videos")
                            static_dir.mkdir(parents=True, exist_ok=True)
                            
                            video_filename = f"veo_{project_id}.mp4"
                            static_path = static_dir / video_filename
                            
                            with open(static_path, "wb") as f:
                                f.write(video_bytes)
                            
                            print(f"[VideoGenerator] ğŸ“ Veo å½±ç‰‡å·²ä¿å­˜: {static_path}")
                            
                            # ä¸Šå‚³åˆ°é›²ç«¯å„²å­˜
                            video_url = f"/video/download/{video_filename}"
                            try:
                                from app.services.cloud_storage import cloud_storage
                                if cloud_storage.is_configured():
                                    print(f"[VideoGenerator] â˜ï¸ æ­£åœ¨ä¸Šå‚³ Veo å½±ç‰‡åˆ°é›²ç«¯å„²å­˜...")
                                    upload_result = cloud_storage.upload_file(
                                        file_path=str(static_path),
                                        user_id=0,
                                        file_type="videos",
                                        original_filename=video_filename
                                    )
                                    if upload_result.get("success"):
                                        video_url = upload_result["url"]
                                        print(f"[VideoGenerator] âœ… Veo é›²ç«¯ä¸Šå‚³æˆåŠŸ: {video_url}")
                                        try:
                                            os.remove(static_path)
                                        except:
                                            pass
                                    else:
                                        print(f"[VideoGenerator] âš ï¸ Veo é›²ç«¯ä¸Šå‚³å¤±æ•—: {upload_result.get('error')}")
                            except Exception as e:
                                print(f"[VideoGenerator] âš ï¸ Veo é›²ç«¯å„²å­˜ç•°å¸¸: {e}")
                            
                            return VideoResult(
                                video_url=video_url,
                                video_base64=None,
                                thumbnail_url=None,
                                duration=veo_duration,
                                format=format_str,
                                file_size=len(video_bytes),
                                scene_images=None,
                                generation_method="veo"
                            )
                        else:
                            print(f"[VideoGenerator] ç„¡æ³•ç²å–å½±ç‰‡ bytes")
                    else:
                        print(f"[VideoGenerator] ç„¡å½±ç‰‡çµæœ")
                
            except asyncio.TimeoutError:
                print(f"[VideoGenerator] Veo {model_name} è¶…æ™‚")
                continue
            except Exception as e:
                error_msg = str(e)
                # æ‰“å°å®Œæ•´éŒ¯èª¤ä»¥ä¾¿èª¿è©¦
                print(f"[VideoGenerator] Veo {model_name} å®Œæ•´éŒ¯èª¤: {error_msg}")
                if "not found" in error_msg.lower() or "404" in error_msg:
                    print(f"[VideoGenerator] â†’ æ¨¡å‹ä¸å­˜åœ¨æˆ–æœªå•Ÿç”¨")
                elif "permission" in error_msg.lower() or "403" in error_msg:
                    print(f"[VideoGenerator] â†’ ç„¡æ¬Šé™å­˜å–æ­¤æ¨¡å‹")
                elif "quota" in error_msg.lower():
                    print(f"[VideoGenerator] â†’ é…é¡ä¸è¶³")
                continue
        
        print("[VideoGenerator] æ‰€æœ‰ Veo æ¨¡å‹éƒ½ä¸å¯ç”¨")
        return None
    
    async def _generate_with_kling(
        self,
        script: Dict[str, Any],
        project_id: str,
        model: str = "kling",
        duration: int = 5
    ) -> Optional[VideoResult]:
        """
        ä½¿ç”¨ Kling v2.1 ç”Ÿæˆå½±ç‰‡ï¼ˆé€é Replicateï¼‰
        
        Kling v2.1 æ˜¯ image-to-video æ¨¡å‹ï¼Œéœ€è¦å…ˆç”Ÿæˆèµ·å§‹åœ–ç‰‡
        - standard: 720p, 24fps
        - pro: 1080p, 24fps
        - duration: 5 æˆ– 10 ç§’
        """
        client = get_replicate_client()
        if not client:
            print("[VideoGenerator] Kling: Replicate Client æœªåˆå§‹åŒ–")
            return None
        
        scenes = script.get("scenes", [])
        format_str = script.get("format", "9:16")
        title = script.get("title", "")
        
        # æ§‹å»ºæç¤ºè©
        prompt = self._build_kling_prompt(script)
        
        # Kling æ¨¡å¼ï¼šstandard (720p) æˆ– pro (1080p)
        is_pro = "pro" in model
        kling_mode = "pro" if is_pro else "standard"
        kling_duration = 10 if "10s" in model else 5
        
        try:
            # æ­¥é©Ÿ 1: ç”Ÿæˆèµ·å§‹åœ–ç‰‡
            print(f"[VideoGenerator] ğŸ–¼ï¸ ç”Ÿæˆèµ·å§‹åœ–ç‰‡...")
            
            # å–ç¬¬ä¸€å€‹å ´æ™¯çš„è¦–è¦ºæè¿°
            first_scene = scenes[0] if scenes else {}
            image_prompt = first_scene.get("visual_prompt", prompt)
            color_palette = script.get("color_palette", ["#6366F1", "#8B5CF6"])
            
            # æ ¹æ“šæ¯”ä¾‹è¨­ç½®åœ–ç‰‡å°ºå¯¸
            size_map = {
                "9:16": (720, 1280) if not is_pro else (1080, 1920),
                "16:9": (1280, 720) if not is_pro else (1920, 1080),
                "1:1": (1024, 1024),
            }
            width, height = size_map.get(format_str, (720, 1280))
            
            import base64
            import io
            start_image_data = None
            
            # æ–¹æ³• 1: å˜—è©¦ä½¿ç”¨ Imagen
            img_client = vertexai_client or genai_client
            imagen_models = [
                "models/imagen-4.0-fast-generate-001",
                "models/gemini-2.0-flash-exp-image-generation",
            ]
            
            for model_name in imagen_models:
                try:
                    if img_client and hasattr(img_client.models, 'generate_images'):
                        response = await asyncio.wait_for(
                            asyncio.to_thread(
                                img_client.models.generate_images,
                                model=model_name,
                                prompt=image_prompt
                            ),
                            timeout=60.0
                        )
                        
                        if response and hasattr(response, 'generated_images') and response.generated_images:
                            img_bytes = response.generated_images[0].image.image_bytes
                            start_image_data = f"data:image/png;base64,{base64.b64encode(img_bytes).decode()}"
                            print(f"[VideoGenerator] âœ… èµ·å§‹åœ–ç‰‡ç”Ÿæˆå®Œæˆ (Imagen)")
                            break
                except Exception as img_err:
                    continue
            
            # æ–¹æ³• 2: ä½¿ç”¨æ˜äº®èµ·å§‹åœ–ç‰‡ï¼ˆå°ˆç‚º Kling å„ªåŒ–ï¼‰
            if not start_image_data:
                print(f"[VideoGenerator] ğŸ“ ç”Ÿæˆæ˜äº®èµ·å§‹åœ–ç‰‡...")
                start_image_data = self._generate_kling_start_image(color_palette, width, height, title)
                if start_image_data:
                    print(f"[VideoGenerator] âœ… æ˜äº®èµ·å§‹åœ–ç‰‡ç”Ÿæˆå®Œæˆ")
            
            if not start_image_data:
                print("[VideoGenerator] âŒ èµ·å§‹åœ–ç‰‡ç”Ÿæˆå¤±æ•—")
                return None
            
            # æ­¥é©Ÿ 2: èª¿ç”¨ Kling v2.1 ç”Ÿæˆå½±ç‰‡
            print(f"[VideoGenerator] ğŸ¥ é–‹å§‹ Kling v2.1 å½±ç‰‡ç”Ÿæˆ...")
            print(f"[VideoGenerator] ğŸ“ æç¤ºè©: {prompt[:100]}...")
            print(f"[VideoGenerator] âš™ï¸ æ¨¡å¼: {kling_mode}, æ™‚é•·: {kling_duration}ç§’")
            
            # Kling å°ˆç”¨ negative promptï¼ˆé¿å…å¸¸è¦‹å•é¡Œï¼‰
            kling_negative = """blurry, out of focus, low resolution, pixelated, 
grainy noise, compression artifacts, watermark, logo, text overlay, 
distorted faces, unnatural movements, jittery motion, choppy animation,
amateur lighting, overexposed, underexposed, washed out colors,
static image, no motion, frozen frame, glitch, artifact"""

            output = await asyncio.to_thread(
                client.run,
                THIRD_PARTY_VIDEO_MODELS["kling"],
                input={
                    "prompt": prompt,
                    "start_image": start_image_data,
                    "mode": kling_mode,
                    "duration": kling_duration,
                    "negative_prompt": kling_negative,
                }
            )
            
            # è™•ç†è¼¸å‡ºï¼ˆå¯èƒ½æ˜¯ URLã€FileOutput æˆ– iteratorï¼‰
            video_url_remote = None
            print(f"[VideoGenerator] ğŸ“¦ Kling è¿”å›é¡å‹: {type(output)}")
            print(f"[VideoGenerator] ğŸ“¦ Kling è¿”å›å…§å®¹: {output}")
            
            if isinstance(output, str):
                video_url_remote = output
            elif hasattr(output, 'url'):
                # FileOutput å°è±¡
                video_url_remote = str(output.url) if hasattr(output.url, '__str__') else output.url
            elif hasattr(output, '__iter__'):
                for item in output:
                    print(f"[VideoGenerator] ğŸ“¦ è¿­ä»£é …ç›®: {type(item)} - {item}")
                    if isinstance(item, str) and item.startswith('http'):
                        video_url_remote = item
                        break
                    elif hasattr(item, 'url'):
                        video_url_remote = str(item.url)
                        break
            
            if not video_url_remote:
                print("[VideoGenerator] âŒ Kling æœªè¿”å›å½±ç‰‡ URL")
                return None
            
            print(f"[VideoGenerator] âœ… Kling å½±ç‰‡ç”ŸæˆæˆåŠŸï¼URL: {video_url_remote[:80]}...")
            
            # ä¸‹è¼‰å½±ç‰‡åˆ°æœ¬åœ°
            import httpx
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.get(video_url_remote)
                if response.status_code != 200:
                    print(f"[VideoGenerator] âŒ ä¸‹è¼‰ Kling å½±ç‰‡å¤±æ•—: {response.status_code}")
                    return None
                video_bytes = response.content
            
            # ä¿å­˜åˆ°éœæ…‹ç›®éŒ„
            static_dir = Path("/app/static/videos")
            static_dir.mkdir(parents=True, exist_ok=True)
            
            video_filename = f"kling_{project_id}.mp4"
            static_path = static_dir / video_filename
            
            with open(static_path, "wb") as f:
                f.write(video_bytes)
            
            print(f"[VideoGenerator] ğŸ“ Kling å½±ç‰‡å·²ä¿å­˜: {static_path}, å¤§å°: {len(video_bytes) / 1024 / 1024:.2f} MB")
            
            # ğŸ”Š æ·»åŠ éŸ³è¨Šè™•ç†ï¼ˆTTS + èƒŒæ™¯éŸ³æ¨‚ï¼‰
            final_video_path = await self._add_audio_to_video(
                video_path=str(static_path),
                script=script,
                project_id=project_id,
                duration=kling_duration
            )
            
            if final_video_path and final_video_path != str(static_path):
                # å¦‚æœç”Ÿæˆäº†æ–°çš„å¸¶éŸ³è¨Šå½±ç‰‡ï¼Œæ›´æ–°è·¯å¾‘
                video_filename = os.path.basename(final_video_path)
                final_size = os.path.getsize(final_video_path)
                print(f"[VideoGenerator] ğŸ”Š éŸ³è¨Šå·²æ·»åŠ ï¼Œæœ€çµ‚å½±ç‰‡: {final_video_path}, å¤§å°: {final_size / 1024 / 1024:.2f} MB")
                upload_path = final_video_path
            else:
                final_size = len(video_bytes)
                upload_path = str(static_path)
            
            # ä¸Šå‚³åˆ°é›²ç«¯å„²å­˜
            video_url = f"/video/download/{video_filename}"
            try:
                from app.services.cloud_storage import cloud_storage
                if cloud_storage.is_configured():
                    print(f"[VideoGenerator] â˜ï¸ æ­£åœ¨ä¸Šå‚³ Kling å½±ç‰‡åˆ°é›²ç«¯å„²å­˜...")
                    upload_result = cloud_storage.upload_file(
                        file_path=upload_path,
                        user_id=0,
                        file_type="videos",
                        original_filename=f"kling_{project_id}.mp4"
                    )
                    if upload_result.get("success"):
                        video_url = upload_result["url"]
                        print(f"[VideoGenerator] âœ… Kling é›²ç«¯ä¸Šå‚³æˆåŠŸ: {video_url}")
                        # åˆªé™¤æœ¬åœ°æª”æ¡ˆ
                        try:
                            os.remove(upload_path)
                            if upload_path != str(static_path) and os.path.exists(static_path):
                                os.remove(static_path)
                        except:
                            pass
                    else:
                        print(f"[VideoGenerator] âš ï¸ Kling é›²ç«¯ä¸Šå‚³å¤±æ•—: {upload_result.get('error')}")
            except Exception as e:
                print(f"[VideoGenerator] âš ï¸ Kling é›²ç«¯å„²å­˜ç•°å¸¸: {e}")
            
            return VideoResult(
                video_url=video_url,
                video_base64=None,
                thumbnail_url=None,
                duration=int(kling_duration),
                format=format_str,
                file_size=final_size,
                scene_images=None,
                generation_method="kling"
            )
            
        except Exception as e:
            print(f"[VideoGenerator] âŒ Kling ç”Ÿæˆå¤±æ•—: {e}")
            return None
    
    def _generate_kling_start_image(
        self, 
        color_palette: List[str], 
        width: int, 
        height: int, 
        title: str = ""
    ) -> Optional[str]:
        """
        ç”Ÿæˆ Kling å°ˆç”¨çš„ç´”é»‘èµ·å§‹åœ–ç‰‡
        
        Kling v2.1 æ˜¯ image-to-video æ¨¡å‹ï¼Œå¿…é ˆæœ‰èµ·å§‹åœ–ç‰‡
        ä½¿ç”¨ç´”é»‘åœ–ç‰‡ï¼Œè®“ Kling å¾é»‘å ´é–‹å§‹ç”Ÿæˆ
        """
        if not PIL_AVAILABLE:
            return None
        
        try:
            import io
            import base64
            
            # å‰µå»ºç´”é»‘åœ–ç‰‡
            img = Image.new('RGB', (width, height), color=(0, 0, 0))
            
            # è½‰æ›ç‚º base64
            buffer = io.BytesIO()
            img.save(buffer, format='PNG', quality=95)
            
            print(f"[VideoGenerator] â¬› ç”Ÿæˆç´”é»‘èµ·å§‹åœ–ç‰‡ ({width}x{height})")
            
            return f"data:image/png;base64,{base64.b64encode(buffer.getvalue()).decode()}"
            
        except Exception as e:
            print(f"[VideoGenerator] âŒ Kling èµ·å§‹åœ–ç‰‡ç”Ÿæˆå¤±æ•—: {e}")
            return None
    
    def _build_kling_prompt(self, script: Dict[str, Any]) -> str:
        """
        æ§‹å»º Kling v2.1 å½±ç‰‡æç¤ºè©
        
        Kling æœ€ä½³å¯¦è¸ï¼š
        1. æè¿°å‹•æ…‹å’Œé‹å‹•ï¼ˆcamera movement, subject motionï¼‰
        2. ä½¿ç”¨å…·é«”çš„è¦–è¦ºç´°ç¯€
        3. åŒ…å«å…‰ç·šå’Œæ°›åœæè¿°
        4. é¿å…éé•·çš„æç¤ºè©ï¼ˆå»ºè­° 100-200 å­—ï¼‰
        """
        import random
        
        scenes = script.get("scenes", [])
        title = script.get("title", "")
        description = script.get("description", "")
        style = script.get("overall_style", "modern, cinematic")
        color_palette = script.get("color_palette", ["#6366F1", "#8B5CF6"])
        personality = script.get("personality", "professional")
        
        # å¾ç¬¬ä¸€å€‹å ´æ™¯æå–è¦–è¦ºæç¤ºè©
        first_scene = scenes[0] if scenes else {}
        visual_prompt = first_scene.get("visual_prompt", "")
        camera_movement = first_scene.get("camera_movement", "smooth dolly forward")
        
        # Kling å°ˆç”¨çš„é‹é¡è©å½™
        CAMERA_MOVES = {
            "dolly": "smooth dolly forward revealing the scene",
            "pan": "elegant pan across the environment",
            "tracking": "dynamic tracking shot following the subject",
            "crane": "cinematic crane shot descending gracefully",
            "orbit": "360 degree orbit around the subject",
            "push": "slow push in towards the focal point",
            "pull": "gradual pull back revealing context",
            "static": "locked off shot with subtle subject motion",
        }
        
        # Kling å°ˆç”¨çš„å‹•æ…‹æè¿°
        MOTION_STYLES = {
            "professional": "subtle confident movements, professional gestures, purposeful actions",
            "friendly": "natural relaxed motion, warm genuine expressions, organic interactions",
            "luxurious": "elegant slow movements, refined gestures, sophisticated grace",
            "energetic": "dynamic energetic motion, vibrant movements, exciting action",
            "calm": "peaceful gentle movements, serene flow, tranquil atmosphere",
        }
        
        # Kling å°ˆç”¨çš„å…‰ç·šæè¿°
        LIGHTING_STYLES = {
            "professional": "soft diffused studio lighting with subtle shadows, clean and polished look",
            "friendly": "warm golden hour sunlight streaming through windows, cozy ambient glow",
            "luxurious": "dramatic chiaroscuro lighting with sparkling highlights, rich deep shadows",
            "energetic": "vibrant colorful lighting with dynamic contrasts, bold illumination",
            "calm": "soft ethereal light with gentle gradients, peaceful luminous atmosphere",
        }
        
        # é¸æ“‡é‹é¡
        selected_camera = CAMERA_MOVES.get(camera_movement, random.choice(list(CAMERA_MOVES.values())))
        
        # é¸æ“‡å‹•æ…‹é¢¨æ ¼
        motion_style = MOTION_STYLES.get(personality, MOTION_STYLES["professional"])
        
        # é¸æ“‡å…‰ç·šé¢¨æ ¼
        lighting_style = LIGHTING_STYLES.get(personality, LIGHTING_STYLES["professional"])
        
        # æ§‹å»ºæç¤ºè©ï¼ˆç°¡æ½”ä½†å®Œæ•´ï¼‰
        prompt_parts = []
        
        # 1. æ ¸å¿ƒè¦–è¦ºæè¿°ï¼ˆä½¿ç”¨å ´æ™¯çš„ visual_prompt æˆ–æ¨™é¡Œï¼‰
        if visual_prompt:
            # æ¸…ç†ä¸¦ä½¿ç”¨å ´æ™¯è¦–è¦ºæç¤ºè©çš„æ ¸å¿ƒå…§å®¹
            core_visual = visual_prompt.split(",")[0:3]  # å–å‰3å€‹æè¿°
            prompt_parts.append(", ".join(core_visual))
        elif title:
            prompt_parts.append(f"Cinematic scene depicting: {title}")
        elif description:
            prompt_parts.append(f"Visual story about: {description[:80]}")
        
        # 2. é‹é¡æè¿°ï¼ˆKling å°æ­¤åæ‡‰å¾ˆå¥½ï¼‰
        prompt_parts.append(selected_camera)
        
        # 3. å‹•æ…‹æè¿°
        prompt_parts.append(motion_style)
        
        # 4. å…‰ç·šå’Œæ°›åœ
        prompt_parts.append(lighting_style)
        
        # 5. å“è³ªé—œéµè©
        quality_terms = [
            "cinematic 4K quality",
            "professional color grading", 
            "film grain texture",
            "shallow depth of field",
            "broadcast quality production"
        ]
        prompt_parts.append(random.choice(quality_terms))
        
        # 6. é¢¨æ ¼ä¿®é£¾
        style_terms = [
            f"{style} aesthetic",
            "premium visual storytelling",
            "advertising quality",
        ]
        prompt_parts.append(random.choice(style_terms))
        
        # çµ„åˆæç¤ºè©ï¼ˆç”¨é€—è™Ÿåˆ†éš”ï¼ŒKling åå¥½é€™ç¨®æ ¼å¼ï¼‰
        final_prompt = ", ".join(prompt_parts)
        
        # é™åˆ¶é•·åº¦ï¼ˆKling å°éé•·æç¤ºè©æ•ˆæœä¸ä½³ï¼‰
        if len(final_prompt) > 500:
            final_prompt = final_prompt[:500].rsplit(",", 1)[0]
        
        return final_prompt
    
    def _build_veo_prompt(self, script: Dict[str, Any]) -> str:
        """
        æ§‹å»ºå°ˆæ¥­ç´š Veo å½±ç‰‡æç¤ºè©
        æ¡ç”¨ Google Veo æœ€ä½³å¯¦è¸ + é›»å½±ç´šæ•˜äº‹çµæ§‹ + è² é¢æç¤ºè©
        """
        import random
        
        scenes = script.get("scenes", [])
        title = script.get("title", "")
        description = script.get("description", "")
        style = script.get("overall_style", "modern, professional")
        color_palette = script.get("color_palette", ["#6366F1", "#8B5CF6"])
        personality = script.get("personality", "professional")
        target_platform = script.get("target_platform", "tiktok")
        music_genre = script.get("music_genre", "upbeat")
        
        # é›»å½±ç´šé¢¨æ ¼æ˜ å°„ - é ‚ç´šå»£å‘Šè¦–è¦ºæè¿° (Premium Quality v2.0)
        CINEMATIC_STYLES = {
            "professional": {
                "visual": """pristine corporate environment with floor-to-ceiling glass walls reflecting city skylines,
polished concrete and brushed aluminum surfaces, Herman Miller furniture, live edge wood accents,
geometric architectural lines creating depth, subtle branded elements integrated seamlessly,
modern art pieces adding sophistication, lush indoor plants for organic warmth""",
                "lighting": """masterfully crafted three-point lighting with large soft key creating dimensional faces,
subtle fill preserving shadow detail without flatness, elegant rim light for subject separation,
practical office lighting adding realism, window light creating natural gradients,
color temperature: 5500K daylight balanced with warm accent touches""",
                "camera": ["Smooth gimbal dolly forward revealing scale", "Precision tracking shot with subtle parallax", 
                          "Elegant jib descent from architectural height", "Steadicam glide through premium space"],
                "atmosphere": "confident sophistication, trustworthy authority, accessible expertise, premium without pretension",
                "color_grade": """clean neutral base with subtle warm skin tone lift, 
corporate navy and slate accents, pristine whites with detail,
ARRI LogC to Rec.709 grade, subtle S-curve contrast,
skin-friendly midtones, controlled highlights""",
                "reference": "Apple 'At Work' series, Salesforce brand films, Bloomberg studio aesthetics, WeWork lifestyle content"
            },
            "friendly": {
                "visual": """sun-drenched lifestyle moments in thoughtfully designed spaces,
natural linen textures, warm wood tones, handcrafted ceramics, vintage brass accents,
authentic human interactions with genuine laughter, pets adding warmth,
cozy reading nooks, steaming coffee cups, morning light streaming through sheer curtains,
real homes with lived-in warmth, not sterile staging""",
                "lighting": """golden hour magic with warm light painting faces beautifully,
soft window light with gentle shadows, practical lamps creating pools of warmth,
candle flicker adding intimacy, fireplace glow for evening scenes,
diffused natural daylight, no harsh shadows, skin-flattering always""",
                "camera": ["Gentle observational handheld with subtle breathing", "Intimate close-up revealing emotion",
                          "Smooth follow shot maintaining connection", "Natural pan discovering moments"],
                "atmosphere": "genuine warmth that feels like home, authentic connection, relatable comfort, the feeling of being understood",
                "color_grade": """warm amber undertones, lifted shadows with orange hue,
Kodak Portra 400 film emulation, creamy highlight rolloff,
nostalgic but not dated, cozy color temperature,
natural skin warmth, soft green foliage rendering""",
                "reference": "Google 'Year in Search', Airbnb 'Belong Anywhere', Coca-Cola 'Real Magic', IKEA lifestyle films"
            },
            "luxurious": {
                "visual": """opulent materials: Calacatta marble with gold veining, brushed brass fixtures, 
hand-stitched leather, Venetian velvet, Baccarat crystal catching light,
haute couture fabrics draped perfectly, fresh peonies in crystal vases,
architectural masterpieces with double-height ceilings, museum-quality art,
Monaco yacht interiors, Parisian apartment grandeur, Swiss chalet elegance""",
                "lighting": """dramatic chiaroscuro with sparkling jewelry-style key lights,
deep cinematic shadows adding mystery and depth, rim lights creating halos,
chandelier sparkle, candelabra ambiance, moonlight through silk curtains,
spotlight reveals on hero products, volumetric rays through dust particles""",
                "camera": ["Majestic crane revealing grandeur", "Hypnotic orbit around precious subject",
                          "Slow cinematic reveal building anticipation", "Tracking dolly through opulent space"],
                "atmosphere": "timeless elegance, exclusive access to extraordinary, aspirational yet attainable sophistication, old money understated luxury",
                "color_grade": """deep rich blacks with shadow detail, golden highlight accents,
film noir influence with selective color pops, desaturated base with jewel tone accents,
skin rendered like Renaissance paintings, metallic surfaces gleaming,
high contrast with preserved detail, S-curve with lifted blacks""",
                "reference": "Chanel 'The One That I Want', Cartier 'Shape Your Time', Louis Vuitton 'L'Invitation au Voyage', Dior haute couture films"
            },
            "playful": {
                "visual": """explosion of saturated colors: electric pink, lime green, sunshine yellow,
dynamic geometric shapes in motion, Memphis design influence, pop art bold graphics,
confetti moments, balloon installations, candy-colored environments,
energetic Gen-Z aesthetics, TikTok-native visual language,
creative chaos with intentional composition, maximum visual stimulation""",
                "lighting": """bright even wash eliminating shadows, colorful gel lighting creating mood,
neon tube accents, RGB LED effects, festival-style color mixing,
ring light beauty, billboard brightness, club atmosphere with moving lights""",
                "camera": ["Snappy whip pan with motion blur", "Energetic tracking matching subject energy",
                          "Playful zoom punch for emphasis", "Quick-cut montage building rhythm"],
                "atmosphere": "infectious joy, youthful rebellion, creative expression, FOMO-inducing excitement, main character energy",
                "color_grade": """maximum saturation pushed to the edge, boosted contrast for punch,
candy-colored palette, punchy split-tone processing,
crushed blacks with neon shadows, blown highlights as aesthetic choice,
Instagram-filter boldness, dopamine-triggering colors""",
                "reference": "Spotify Wrapped, Nintendo Switch lifestyle, Fenty Beauty campaigns, McDonald's 'Famous Orders'"
            },
            "minimalist": {
                "visual": """vast negative space as primary design element, single subject commanding attention,
Scandinavian simplicity, Japanese wabi-sabi philosophy, Bauhaus geometric purity,
white-on-white layering, subtle texture variations, mono-material focus,
architectural concrete poetry, zen garden stillness, gallery-white environments""",
                "lighting": """ethereal diffused glow from massive soft sources, shadowless high-key illumination,
gentle gradients across seamless backgrounds, morning fog softness,
studio infinity cove, natural north-facing window light,
pure and clean, no dramatic shadows, meditative calm""",
                "camera": ["Contemplative static frame holding stillness", "Glacially slow push-in building tension",
                          "Clean geometric tilt reveal", "Zen-like static observation"],
                "atmosphere": "profound calm, intentional emptiness, thoughtful stillness, the luxury of less, space to breathe",
                "color_grade": """desaturated to near monochrome, pure whites with subtle warmth,
soft grays with delicate undertones, whisper-quiet pastel accents,
high-key exposure, compressed dynamic range, ethereal processing,
Fuji Acros film simulation, Nordic color science""",
                "reference": "Muji lifestyle films, Apple 'Designed by Apple', Aesop store interiors, Comme des GarÃ§ons campaigns"
            },
            "innovative": {
                "visual": """bleeding-edge technology visualization, holographic UI floating in space,
quantum computing aesthetics, neural network visualizations, data as art,
SpaceX-style clean tech, Tesla factory precision, server room cathedral lighting,
transparent OLED displays, robotic precision movements, 3D-printed structures""",
                "lighting": """cool 6500K tech-blue key lighting, cyan LED accent strips,
monitor glow illuminating faces, fiber optic star fields,
volumetric fog rays through darkness, laser precision beams,
neon edge lighting, holographic rim effects, screen reflection fills""",
                "camera": ["Drone descent through impossible space", "Matrix-style frozen moment orbit",
                          "Sci-fi tracking through tech corridor", "Reveal dolly from micro to macro"],
                "atmosphere": "bleeding-edge discovery, future-is-now excitement, technological sublime, humanity enhanced by innovation",
                "color_grade": """cool blue dominant with electric cyan accents, teal and orange tension,
digital color banding as aesthetic, cyberpunk influence,
high contrast with crushed blacks, LED color contamination,
Blade Runner 2049 color science, TRON Legacy glow""",
                "reference": "Tesla 'Cybertruck Reveal', Apple WWDC keynotes, Boston Dynamics showcases, SpaceX launch films"
            },
            "trustworthy": {
                "visual": """unscripted real-life moments captured with documentary authenticity,
genuine expressions without direction, real locations with character,
working hands showing expertise, weathered faces telling stories,
community gatherings, multi-generational families, actual customers not actors,
behind-the-scenes access, the beautiful imperfection of real life""",
                "lighting": """purely available light honoring reality, honest shadows telling time of day,
no artificial enhancement, window light as-is, street lamp authenticity,
overcast soft light, harsh noon sun when real, evening golden hour natural,
true-to-life exposure, documentary brightness levels""",
                "camera": ["Observational documentary handheld with human presence", "VÃ©ritÃ© steady wide shot",
                          "Patient follow allowing moments to unfold", "Intimate interview framing"],
                "atmosphere": "unfiltered truth, earned trust through transparency, real stories real people, authentic connection that can't be faked",
                "color_grade": """minimal intervention, true-to-life color, documentary naturalism,
news broadcast neutrality, slight desaturation for gravitas,
honest skin tones, no beauty filter, weather-accurate rendering,
16mm film texture optional, truthful processing""",
                "reference": "Nike 'Dream Crazy', Patagonia 'Don't Buy This Jacket', Dove 'Real Beauty', P&G 'Thank You Mom'"
            },
            "energetic": {
                "visual": """peak athletic performance frozen in power, explosive action with controlled blur,
dynamic Dutch angles creating tension, sports arena electricity,
sweat droplets catching light, muscle definition at maximum exertion,
finish line moments, victory celebrations, against-all-odds determination,
urban parkour flow, extreme sports at the edge""",
                "lighting": """dramatic backlighting creating heroic silhouettes, lens flare as victory symbol,
stadium lights creating atmosphere, golden hour athlete glory,
high contrast action lighting, rim lights defining form,
sweat glistening under spot lights, dust particles catching beams""",
                "camera": ["Phantom slow-motion revealing power", "Steadicam chase matching athlete pace",
                          "Explosive zoom for impact moments", "Crane revealing scale of achievement"],
                "atmosphere": "unlimited human potential, adrenaline coursing, unstoppable momentum, the glory of pushing limits, victory tastes sweet",
                "color_grade": """high contrast blockbuster processing, teal shadows with orange highlights,
punchy saturated colors, crushed blacks for drama,
Michael Bay color science, sports broadcast punch,
highlight bloom for glory, deep blacks for intensity""",
                "reference": "Nike 'Just Do It', Red Bull 'Gives You Wings', Under Armour 'Rule Yourself', Gatorade 'Is It In You'"
            },
            "faith": {
                "visual": """sacred atmosphere with divine light rays streaming through stained glass windows,
peaceful church interiors with warm wood pews and candle glow, hands in prayer, open Bible pages,
cross silhouettes against sunrise, quiet garden meditation spaces, baptism waters, communion elements,
family gathered in worship, community fellowship moments, hands reaching upward in praise,
dove in flight symbolizing Holy Spirit, mountaintop vistas symbolizing faith journey""",
                "lighting": """heavenly light rays breaking through clouds, ethereal golden hour glow,
soft divine radiance from above, warm candlelight ambiance, sunrise hope lighting,
gentle rim light creating halos, peaceful diffused natural light through windows,
dawn light symbolizing new beginnings, sunset reflecting God's glory""",
                "camera": ["Slow reverent tilt upward toward light", "Gentle dolly forward into sacred space",
                          "Peaceful wide establishing shot", "Intimate close-up on hands in prayer"],
                "atmosphere": "sacred peace, divine presence, hopeful redemption, comforting grace, eternal love, heavenly serenity, spiritual transformation",
                "color_grade": """warm golden tones of grace, soft whites symbolizing purity,
heavenly blue accents, gentle desaturation for reverence,
sunrise orange and gold, peaceful earth tones,
skin rendered with divine warmth, ethereal highlight glow,
Kodak Ektar warmth, film-like softness for timeless feel""",
                "reference": "The Chosen series cinematography, Hillsong worship films, church promotional content, Christian lifestyle brand imagery"
            },
        }
        
        # ç²å–é¢¨æ ¼é…ç½®
        style_config = CINEMATIC_STYLES.get(personality, CINEMATIC_STYLES["professional"])
        
        # æå–å ´æ™¯è¦–è¦ºç²¾è¯å’Œè² é¢æç¤ºè©
        scene_visuals = []
        scene_negatives = []
        for scene in scenes[:4]:
            visual = scene.get("visual_prompt", "")
            if visual:
                scene_visuals.append(visual)
            negative = scene.get("negative_prompt", "")
            if negative:
                scene_negatives.append(negative)
        
        primary_color = color_palette[0] if color_palette else "#6366F1"
        secondary_color = color_palette[1] if len(color_palette) > 1 else primary_color
        
        # é¸æ“‡ç›¸æ‡‰çš„æ”å½±æ©Ÿé‹å‹•
        camera_move = random.choice(style_config["camera"])
        
        # æ ¹æ“šå ´æ™¯å…§å®¹æ±ºå®šä¸»é«”
        main_subject = scene_visuals[0] if scene_visuals else description or "elegant product presentation"
        
        # éŸ³æ¨‚æ°›åœæ˜ å°„
        MUSIC_VIBES = {
            "upbeat": "energetic rhythm driving the visual pace, upbeat tempo sync",
            "calm": "peaceful ambient soundscape, gentle flow",
            "emotional": "touching cinematic score building emotion, swelling strings",
            "epic": "powerful orchestral crescendo, dramatic build",
            "minimal": "subtle electronic beats, understated pulse",
            "inspirational": "uplifting motivational music, hopeful progression",
        }
        music_vibe = MUSIC_VIBES.get(music_genre, "modern contemporary soundtrack")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ä¸‰æ˜æ²» Prompt æ¶æ§‹ (Sandwich Prompt Architecture)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # å‰ç¶´ (PREFIX): å¼·åˆ¶é›»å½±è³ªæ„Ÿè§¸ç™¼å™¨
        # ç”¨æˆ¶è¼¸å…¥ (USER INPUT): å ´æ™¯å…§å®¹æè¿°
        # å¾Œç¶´ (SUFFIX): å“è³ªå¢å¼·ä¿®é£¾è©
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # ===== å‰ç¶´: é›»å½±é¢¨æ ¼è§¸ç™¼å™¨ (Cinematic Style Triggers) =====
        CINEMATIC_PREFIX = """Cinematic shot, 35mm film grain, Kodak Portra 400 film stock, 
Anamorphic lens with natural lens flare, Wide angle establishing shot, 
Shallow depth of field, Beautiful bokeh F/1.8, 
Color graded with teal and orange tones, High contrast moody lighting,
Professional Hollywood cinematography"""

        # ===== å¾Œç¶´: å“è³ªå¢å¼·ä¿®é£¾è© (Quality Modifiers) =====
        QUALITY_SUFFIX = """Masterpiece, Best quality, Ultra-realistic, 8K resolution, 
Intricate details, Sharp focus, Hyper-detailed textures,
Professional color grading, Film-like dynamic range,
Smooth continuous motion at 24fps cinematic cadence,
Buttery smooth camera movement, No stuttering or lag,
Premium production value, Award-winning cinematography"""

        # ===== è² é¢æç¤ºè© (Negative Prompts - éš±è—è¨­å®š) =====
        NEGATIVE_MODIFIERS = """Blurry, Low quality, Distorted, Deformed, Watermark, Text overlay,
Bad anatomy, Static frozen frame, Jittery motion, Flickering, Frame drops,
Choppy animation, Stuttering, Lag, Pixelated, Grainy noise,
Compression artifacts, Amateur lighting, Overexposed, Underexposed,
AI generated look, CGI plastic feel, Uncanny valley"""

        # ===== ç”¨æˆ¶å…§å®¹ (User Content) =====
        user_content = f"""Very slow {camera_move} gracefully revealing {main_subject}. 
All movement extremely slow and fluid, like a luxury perfume commercial.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
VISUAL DIRECTION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SCENE AESTHETIC:
{style_config["visual"]}
Overall mood: {style}, premium commercial production quality
Art direction reference: {style_config["reference"]}
Visual storytelling approach: Emotion-driven, visually immersive

CINEMATOGRAPHY (SLOW & STEADY - CRITICAL FOR SMOOTH PLAYBACK):
- Camera movement: VERY SLOW {camera_move.lower()}, extremely smooth, glacially paced
- Movement speed: 50% slower than normal, gentle and deliberate
- Camera stability: Rock-solid gimbal stabilization, zero vibration or shake
- Motion style: Floating, dreamy, hypnotic slow-motion feel
- Lens choice: Premium cinema lens with beautiful rendering, minimal distortion
- Depth of field: Shallow with creamy circular bokeh, subject isolation
- Focus: Smooth gradual focus pulls, never sudden changes
- Framing: Rule of thirds, golden ratio, intentional negative space
- IMPORTANT: All motion must be continuous and fluid, no sudden movements

LIGHTING MASTERCLASS:
{style_config["lighting"]}
- Key light: Soft, flattering, three-dimensional
- Fill light: Subtle shadow detail without flatness
- Rim/hair light: Elegant subject separation
- Practical lights: Motivated, adds depth and realism
- Color temperature harmony: {style_config["color_grade"]}

ATMOSPHERE & EMOTIONAL RESONANCE:
{style_config["atmosphere"]}
Story context: {description}
Emotional journey: Build anticipation â†’ Reveal â†’ Satisfaction

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TECHNICAL EXCELLENCE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FORMAT & RESOLUTION:
- Aspect ratio: 9:16 vertical, perfectly composed for {target_platform}
- Resolution: Native 4K (2160x3840) source, pristine clarity
- Frame rate: 24fps true cinematic motion cadence
- Bit depth: 10-bit color for smooth gradients

COLOR SCIENCE:
- Primary brand color: {primary_color} (hero element)
- Secondary accent: {secondary_color} (complementary)
- Color grading: Film emulation, lifted blacks, controlled highlights
- Skin tones: Natural, healthy, flattering
- Overall palette: Cohesive, intentional, brand-aligned

MOTION QUALITY (CRITICAL - ULTRA SLOW & SMOOTH):
- Speed: ALL motion at 50% slower than normal speed, dreamy slow-motion aesthetic
- Frame consistency: Every frame must flow perfectly into the next, absolutely no stuttering
- Camera stability: Professional gimbal-smooth, rock-solid, zero micro-jitters or vibration
- Motion style: Floating, hypnotic, meditative pace - like a luxury perfume commercial
- Motion interpolation: Fluid 24fps with perfect motion cadence, no dropped frames
- Subject motion: Slow, graceful, deliberate movements only - no fast actions
- Motion blur: Cinematic 180Â° shutter angle, organic natural blur on moving elements
- Transitions: Seamless, invisible, butter-smooth dissolves
- Temporal coherence: Maintain perfect visual consistency across all frames
- AVOID: Fast movements, quick cuts, sudden changes, jerky motion
- PREFER: Slow reveals, gentle pans, floating camera, serene pace

AUDIO-VISUAL SYNC:
Rhythm synchronized with {music_vibe}
Visual beats aligned with musical accents

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
QUALITY IMPERATIVES (NON-NEGOTIABLE)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MUST ACHIEVE:
âœ“ Broadcast-ready, television commercial standard
âœ“ Award-winning cinematography aesthetics
âœ“ Magazine-cover level visual polish
âœ“ Luxury brand production value
âœ“ Photorealistic textures and materials
âœ“ Perfect exposure, no clipped highlights or crushed blacks
âœ“ Pristine image clarity, zero compression artifacts
âœ“ Professional colorist-level grading
âœ“ Seamless, natural movement throughout
âœ“ Emotionally engaging visual narrative

ABSOLUTELY AVOID (ZERO TOLERANCE):
âœ— STUTTERING, LAG, or choppy motion - this is the #1 priority to avoid
âœ— FAST MOVEMENTS - all motion must be slow and deliberate
âœ— QUICK CUTS or rapid scene changes
âœ— Frame drops, skipped frames, or inconsistent frame timing
âœ— Jerky movement, sudden jumps, or motion discontinuity
âœ— Frozen frames or static pauses in motion
âœ— Any blur, softness, or focus issues
âœ— Pixelation, aliasing, or resolution problems
âœ— Morphing, warping, or shape distortion
âœ— Uncanny valley, AI-generated artifacts
âœ— Unnatural human movement or expressions
âœ— Watermarks, logos, text, or overlays
âœ— Compression artifacts, banding, posterization
âœ— Exposure problems (over/under)
âœ— Amateur, stock footage, or generic appearance
âœ— Camera shake, jitter, or micro-vibrations
âœ— Color banding in gradients or skies
âœ— Noise or unwanted grain
âœ— Cheap, tacky, or low-budget aesthetics
âœ— Plastic skin texture or waxy appearance
âœ— Temporal flickering or inconsistent lighting between frames
âœ— Running, jumping, or any rapid physical actions
âœ— Chaotic or busy scenes with multiple moving elements"""

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # çµ„åˆä¸‰æ˜æ²»çµæ§‹ (Assemble Sandwich Structure)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # æœ€çµ‚æç¤ºè© = å‰ç¶´ + ç”¨æˆ¶å…§å®¹ + å¾Œç¶´ + (è² é¢æç¤ºè©å…§åµŒ)
        
        prompt = f"""{CINEMATIC_PREFIX}

{user_content}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
QUALITY ENHANCEMENT (SUFFIX)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{QUALITY_SUFFIX}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
STRICTLY AVOID (NEGATIVE PROMPTS EMBEDDED)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{NEGATIVE_MODIFIERS}"""

        print(f"[VideoGenerator] ğŸ“ Veo ä¸‰æ˜æ²»æ¶æ§‹æç¤ºè© (é¢¨æ ¼: {personality}):")
        print(f"  â†’ å‰ç¶´: Cinematic triggers loaded")
        print(f"  â†’ å…§å®¹: {main_subject[:50]}...")
        print(f"  â†’ å¾Œç¶´: Quality modifiers applied")
        print(f"  â†’ è² é¢: Anti-artifacts filters enabled")
        
        return prompt.strip()
    
    async def _generate_with_imagen_ffmpeg(
        self,
        script: Dict[str, Any],
        project_id: str
    ) -> VideoResult:
        """
        ä½¿ç”¨ Imagen ç”Ÿæˆåœ–ç‰‡ + FFmpeg åˆæˆå½±ç‰‡
        """
        scenes = script.get("scenes", [])
        format_str = script.get("format", "9:16")
        color_palette = script.get("color_palette", ["#6366F1", "#8B5CF6"])
        total_duration = sum(s.get("duration_seconds", 5) for s in scenes)
        
        # è¨­å®šå°ºå¯¸
        if format_str == "9:16":
            width, height = 1080, 1920
        elif format_str == "16:9":
            width, height = 1920, 1080
        else:
            width, height = 1080, 1080
        
        # 1. ç”Ÿæˆæ‰€æœ‰å ´æ™¯åœ–ç‰‡ï¼ˆå„ªå…ˆä½¿ç”¨ç”¨æˆ¶è‡ªè¨‚åœ–ç‰‡ï¼‰
        scene_images: List[str] = []
        scene_audios: List[Optional[str]] = []
        
        for i, scene in enumerate(scenes):
            # æª¢æŸ¥æ˜¯å¦æœ‰ç”¨æˆ¶è‡ªè¨‚åœ–ç‰‡
            custom_image = self._custom_images.get(i) if hasattr(self, '_custom_images') else None
            
            # å…ˆç²å–å ´æ™¯çš„é€šç”¨è³‡æ–™ï¼ˆTTS ç­‰æœƒç”¨åˆ°ï¼‰
            narration = scene.get("narration_text", "")
            
            if custom_image:
                print(f"[VideoGenerator] ğŸ–¼ï¸ å ´æ™¯ {i+1}/{len(scenes)}: ä½¿ç”¨ç”¨æˆ¶è‡ªè¨‚åœ–ç‰‡")
                # è™•ç†è‡ªè¨‚åœ–ç‰‡ï¼ˆä¸æ·»åŠ æ–‡å­—ï¼‰
                image_base64 = await self._process_custom_image(custom_image, width, height)
                if image_base64:
                    scene_images.append(image_base64)
                else:
                    # å¦‚æœè‡ªè¨‚åœ–ç‰‡è™•ç†å¤±æ•—ï¼Œå›é€€åˆ° AI ç”Ÿæˆ
                    print(f"[VideoGenerator] âš ï¸ è‡ªè¨‚åœ–ç‰‡è™•ç†å¤±æ•—ï¼Œä½¿ç”¨ AI ç”Ÿæˆ")
                    image_base64 = await self._generate_scene_image_fallback(scene, color_palette, width, height, i, len(scenes))
                    if image_base64:
                        scene_images.append(image_base64)
            else:
                print(f"[VideoGenerator] ğŸ“¸ ç”Ÿæˆå ´æ™¯ {i+1}/{len(scenes)}")
                
                visual_prompt = scene.get("visual_prompt", "")
                negative_prompt = scene.get("negative_prompt", "")
                quality_tags = scene.get("quality_tags", "")
                
                # ç”Ÿæˆåœ–ç‰‡ï¼ˆä¸æ·»åŠ æ–‡å­—è¦†è“‹ï¼Œä¿æŒç•«é¢ä¹¾æ·¨ï¼‰
                image_base64 = await self._generate_image(
                    visual_prompt,
                    color_palette,
                    width,
                    height,
                    None,  # ä¸æ·»åŠ æ–‡å­—
                    i + 1,
                    len(scenes),
                    negative_prompt,
                    quality_tags
                )
                
                if image_base64:
                    scene_images.append(image_base64)
            
            # ç”ŸæˆèªéŸ³
            audio_path = None
            if narration and EDGE_TTS_AVAILABLE:
                voice_style = scene.get("voice_emotion", "friendly")
                audio_path = await self._generate_tts(narration, project_id, i, voice_style)
            scene_audios.append(audio_path)
        
        print(f"[VideoGenerator] âœ… åœ–ç‰‡ç”Ÿæˆå®Œæˆï¼Œå…± {len(scene_images)} å¼µ")
        
        # 2. èƒŒæ™¯éŸ³æ¨‚è™•ç†
        music_url = script.get("music_url")
        music_volume = script.get("music_volume", 0.3)
        music_genre = script.get("music_genre", "upbeat")
        
        # è™•ç†é¢¨æ ¼é¸æ“‡æ¨¡å¼ (style:xxx) - å¾ Mixkit å…è²»éŸ³æ¨‚åº«ç²å–
        if music_url and music_url.startswith("style:"):
            style = music_url.replace("style:", "")
            music_genre = style
            music_url = get_music_url_for_style(style)
            print(f"[VideoGenerator] ğŸµ é¢¨æ ¼é¸æ“‡: {style} -> Mixkit éŸ³æ¨‚")
        
        music_path = None
        
        # å„ªå…ˆä½¿ç”¨ Mixkit å…è²»å•†ç”¨éŸ³æ¨‚
        if music_url and music_url.startswith("http"):
            print(f"[VideoGenerator] ğŸµ ä¸‹è¼‰ Mixkit éŸ³æ¨‚: {music_url[:60]}...")
            music_path = await self._download_external_music(music_url, project_id)
            if music_path:
                print(f"[VideoGenerator] âœ… Mixkit éŸ³æ¨‚ä¸‹è¼‰æˆåŠŸ")
            else:
                # å˜—è©¦å…¶ä»– URL
                fallback_url = get_music_url_for_style(music_genre)
                if fallback_url != music_url:
                    print(f"[VideoGenerator] ğŸ”„ å˜—è©¦å‚™ç”¨éŸ³æ¨‚...")
                    music_path = await self._download_external_music(fallback_url, project_id)
        
        # AI éŸ³æ¨‚æ¨™è¨˜ï¼ˆå‚™ç”¨ï¼‰
        if not music_path and music_url and music_url.startswith("ai:"):
            ai_style = music_url.replace("ai:", "")
            print(f"[VideoGenerator] ğŸ¹ ä½¿ç”¨ AI ç”ŸæˆèƒŒæ™¯éŸ³æ¨‚ (é¢¨æ ¼: {ai_style})")
            music_path = await self._generate_background_music(ai_style, total_duration, project_id)
        
        if not music_path:
            # æœ€å¾Œå›é€€ï¼šç›´æ¥ç²å– Mixkit éŸ³æ¨‚
            fallback_url = get_music_url_for_style(music_genre)
            print(f"[VideoGenerator] ğŸµ ä½¿ç”¨å‚™ç”¨ Mixkit éŸ³æ¨‚...")
            music_path = await self._download_external_music(fallback_url, project_id)
        
        if not music_path:
            # çœŸæ­£çš„æœ€å¾Œå›é€€ï¼šAI ç”Ÿæˆ
            print(f"[VideoGenerator] ğŸ¹ å›é€€åˆ° AI ç”ŸæˆèƒŒæ™¯éŸ³æ¨‚ (é¢¨æ ¼: {music_genre})")
            music_path = await self._generate_background_music(music_genre, total_duration, project_id)
        
        # 3. ä½¿ç”¨ FFmpeg åˆæˆå½±ç‰‡
        video_path = await self._create_video_ffmpeg(
            scene_images,
            scenes,
            scene_audios,
            music_path,
            project_id,
            width,
            height,
            music_volume
        )
        
        # 4. è™•ç†å½±ç‰‡è¼¸å‡º
        video_base64 = None
        video_url = ""
        file_size = 0
        generation_method = "imagen+ffmpeg"
        
        if video_path and os.path.exists(video_path):
            file_size = os.path.getsize(video_path)
            print(f"[VideoGenerator] ğŸ‰ å½±ç‰‡åˆæˆæˆåŠŸï¼Œå¤§å°: {file_size / 1024 / 1024:.2f} MB")
            
            # å˜—è©¦ä¸Šå‚³åˆ°é›²ç«¯å„²å­˜
            try:
                from app.services.cloud_storage import cloud_storage
                if cloud_storage.is_configured():
                    print(f"[VideoGenerator] â˜ï¸ æ­£åœ¨ä¸Šå‚³åˆ°é›²ç«¯å„²å­˜...")
                    upload_result = cloud_storage.upload_file(
                        file_path=video_path,
                        user_id=0,  # ç³»çµ±ç”Ÿæˆï¼Œä½¿ç”¨ 0 ä½œç‚º user_id
                        file_type="videos",
                        original_filename=f"video_{project_id}.mp4"
                    )
                    if upload_result.get("success"):
                        video_url = upload_result["url"]
                        print(f"[VideoGenerator] âœ… é›²ç«¯ä¸Šå‚³æˆåŠŸ: {video_url}")
                        # åˆªé™¤æœ¬åœ°æª”æ¡ˆ
                        try:
                            os.remove(video_path)
                        except:
                            pass
                    else:
                        print(f"[VideoGenerator] âš ï¸ é›²ç«¯ä¸Šå‚³å¤±æ•—: {upload_result.get('error')}")
                        # å›é€€åˆ°æœ¬åœ°å„²å­˜
                        video_url = self._save_to_local(video_path, project_id)
                else:
                    print(f"[VideoGenerator] âš ï¸ é›²ç«¯å„²å­˜æœªè¨­å®šï¼Œä½¿ç”¨æœ¬åœ°å„²å­˜")
                    video_url = self._save_to_local(video_path, project_id)
            except Exception as e:
                print(f"[VideoGenerator] âš ï¸ é›²ç«¯å„²å­˜ç•°å¸¸: {e}ï¼Œä½¿ç”¨æœ¬åœ°å„²å­˜")
                video_url = self._save_to_local(video_path, project_id)
        else:
            video_url = scene_images[0] if scene_images else ""
            generation_method = "placeholder"
        
        return VideoResult(
            video_url=video_url,
            video_base64=video_base64,
            thumbnail_url=scene_images[0] if scene_images else None,
            duration=total_duration,
            format=format_str,
            file_size=file_size,
            scene_images=scene_images,
            generation_method=generation_method
        )
    
    async def _generate_image(
        self,
        visual_prompt: str,
        color_palette: List[str],
        width: int,
        height: int,
        text_overlay: Optional[str],
        scene_num: int,
        total_scenes: int,
        negative_prompt: str = "",
        quality_tags: str = ""
    ) -> Optional[str]:
        """ç”Ÿæˆå ´æ™¯åœ–ç‰‡ - å°ˆæ¥­ç´šå“è³ª"""
        
        aspect_ratio = f"{width}:{height}"
        if width == 1080 and height == 1920:
            aspect_ratio = "9:16"
        elif width == 1920 and height == 1080:
            aspect_ratio = "16:9"
        elif width == height:
            aspect_ratio = "1:1"
        
        # é è¨­å“è³ªæ¨™ç±¤ (Premium Quality v3.0 - å¼·èª¿çœŸå¯¦æ„Ÿï¼Œå»é™¤ AI æ„Ÿ)
        default_quality = """â•â•â• AUTHENTIC QUALITY MARKERS â•â•â•
masterpiece, best quality, ultra high resolution, 8K UHD source,
shot on Hasselblad H6D-400C medium format, ARRI Alexa Mini LF footage,
natural film grain texture, Kodak Vision3 500T film emulation, analog warmth,
authentic lighting imperfections, genuine texture, organic feel,
real-world photography, captured moment, not generated,
human photographer aesthetic, handcrafted composition, intentional imperfections,
true-to-life colors, natural color science, no artificial enhancement,
professional studio but natural look, genuine atmosphere,
editorial magazine quality, luxury brand campaign authenticity,
broadcast ready, advertising standard, real commercial production"""
        
        # é è¨­è² é¢æç¤ºè© (v3.1 - é¦–è¦ç›®æ¨™ï¼šæ¶ˆé™¤ AI ç”Ÿæˆç‰¹å¾µ + ç¦æ­¢ä»»ä½•æ–‡å­—)
        default_negative = """â•â•â• ZERO TEXT ALLOWED - ABSOLUTE RULE â•â•â•
text, words, letters, alphabet, characters, typography, font,
Chinese characters, ä¸­æ–‡, æ¼¢å­—, ç¹é«”å­—, ç°¡é«”å­—, Japanese text, Korean text,
any language text, readable text, legible text, numbers, digits,
titles, captions, subtitles, labels, watermark, signature, logo,
brand name, slogan, tagline, quote, signs, banners, posters with text,
text overlay, text on image, written content,

â•â•â• AI ARTIFACTS - MUST ELIMINATE â•â•â•
AI generated, artificial intelligence created, machine generated, synthetic image,
artificial looking, computer generated, CGI appearance, 3D render look,
plastic skin, waxy texture, silicone appearance, mannequin-like,
overly smooth skin, poreless face, airbrushed look, over-retouched,
unnaturally perfect, too clean, too symmetrical, mathematical precision,
uncanny valley, dead eyes, lifeless expression, frozen face,
hyper-saturated colors, over-processed, HDR artifacts, tone-mapped look,
soulless, generic, stock photo aesthetic, template-based,
video game graphics, Unreal Engine render (negative), Unity render,
deepfake appearance, morph artifacts, face swap artifacts,
digital painting look, illustration style when photo needed,

â•â•â• TECHNICAL ISSUES â•â•â•
blurry, out of focus, motion blur, camera shake, soft focus,
pixelated, low resolution, poor quality, degraded, compression artifacts,
distorted, warped, deformed, malformed, bad anatomy,
extra limbs, mutated hands, extra fingers, missing limbs,
cropped awkwardly, cut off, bad framing,
overexposed, underexposed, flat lighting, harsh shadows,
noisy, grainy (unless intentional), jpeg artifacts, banding,
cluttered background, distracting elements,
cheap, tacky, amateur, unprofessional"""
        
        # åˆä½µå“è³ªæ¨™ç±¤
        final_quality = quality_tags if quality_tags else default_quality
        final_negative = negative_prompt if negative_prompt else default_negative
        
        # 1. å˜—è©¦ä½¿ç”¨ Imagen
        client = vertexai_client or genai_client
        if client and visual_prompt:
            # æ§‹å»ºå°ˆæ¥­ç´šå¢å¼·æç¤ºè© (Premium Quality v2.0)
            enhanced_prompt = f"""â•â•â• VISUAL SUBJECT â•â•â•
{visual_prompt}

â•â•â• AUTHENTICITY DIRECTIVE (CRITICAL) â•â•â•
This must look like a REAL photograph taken by a professional human photographer
NOT AI generated, NOT CGI, NOT 3D render, NOT digital art
Capture authentic moment with natural imperfections
Include subtle film grain, natural lighting variance, organic textures
Real-world physics, genuine materials, authentic atmosphere

â•â•â• ARTISTIC DIRECTION â•â•â•
Premium video frame designed for viral short-form content
Format: {aspect_ratio} vertical, optimized for mobile-first viewing
Aesthetic: Luxury brand commercial shot on location, editorial magazine quality
Style: Natural photography with cinematic color grading
Feel: Handcrafted, intentional, human-directed

â•â•â• CINEMATOGRAPHY â•â•â•
Camera: Shot on ARRI Alexa Mini LF with Cooke S7/i lenses
Composition: Rule of thirds, golden ratio, intentional negative space
Focus: Razor sharp on subject, creamy bokeh background separation
Depth: Shallow depth of field with beautiful anamorphic bokeh
Framing: Perfect {aspect_ratio} composition, professional framing
Movement: Subtle natural camera presence, not sterile

â•â•â• LIGHTING DESIGN â•â•â•
Setup: Professional cinematographer lighting, natural motivated sources
Key light: Soft, flattering, dimensional with natural falloff
Fill light: Subtle shadow detail, not artificially lifted
Rim light: Organic subject-background separation
Quality: Real studio lighting, not computer generated

â•â•â• COLOR SCIENCE â•â•â•
Film stock: Kodak Vision3 500T / Fujifilm Eterna look
Grading: Cinematic film emulation, natural lifted shadows
White balance: Perfect neutral or intentionally warm/cool for mood
Palette: True-to-life colors, not hyper-saturated
Skin tones: Natural, healthy, real human skin texture (not plastic)

â•â•â• TECHNICAL SPECIFICATIONS â•â•â•
Resolution: {width}x{height} pixels, native 4K clarity
Sharpness: Natural sharp, not over-sharpened AI look
Texture: Real film grain, subtle lens characteristics
Dynamic range: Natural tonal range, organic highlight rolloff
Format: Authentic photography aesthetic

â•â•â• QUALITY IMPERATIVES â•â•â•
{final_quality}

â•â•â• EXCLUSIONS (CRITICAL - ESPECIALLY AI ARTIFACTS) â•â•â•
{final_negative}"""
            
            imagen_models = [
                "models/imagen-4.0-fast-generate-001",
                "models/gemini-2.0-flash-exp-image-generation",
                "models/imagen-4.0-generate-001",
            ]
            
            for model_name in imagen_models:
                try:
                    if hasattr(client.models, 'generate_images'):
                        response = await asyncio.wait_for(
                            asyncio.to_thread(
                                client.models.generate_images,
                                model=model_name,
                                prompt=enhanced_prompt
                            ),
                            timeout=60.0
                        )
                        
                        if response.generated_images:
                            image_data = response.generated_images[0].image.image_bytes
                            
                            # èª¿æ•´å°ºå¯¸
                            img = Image.open(io.BytesIO(image_data))
                            img = self._resize_image(img, width, height)
                            
                            # ä¸æ·»åŠ æ–‡å­—è¦†è“‹ - ä¿æŒç•«é¢ä¹¾æ·¨
                            # if text_overlay:
                            #     img = self._add_text_overlay(img, text_overlay, color_palette)
                            
                            buffer = io.BytesIO()
                            img.save(buffer, format='PNG', quality=95)
                            
                            print(f"[VideoGenerator] âœ“ Imagen åœ–ç‰‡ç”ŸæˆæˆåŠŸ (å ´æ™¯ {scene_num})")
                            return f"data:image/png;base64,{base64.b64encode(buffer.getvalue()).decode()}"
                        
                except asyncio.TimeoutError:
                    continue
                except Exception as e:
                    if "not found" not in str(e).lower():
                        print(f"[VideoGenerator] Imagen éŒ¯èª¤: {str(e)[:80]}")
                    continue
        
        # 2. ç”Ÿæˆè¨­è¨ˆåœ–ï¼ˆä¸æ·»åŠ æ–‡å­—ï¼‰
        print(f"[VideoGenerator] ğŸ¨ å ´æ™¯ {scene_num}: ä½¿ç”¨è¨­è¨ˆåœ–")
        return self._generate_designed_image(color_palette, width, height, None, scene_num, total_scenes)
    
    def _resize_image(self, img: Image.Image, target_width: int, target_height: int) -> Image.Image:
        """èª¿æ•´åœ–ç‰‡å°ºå¯¸"""
        original_ratio = img.width / img.height
        target_ratio = target_width / target_height
        
        if original_ratio > target_ratio:
            new_width = int(img.height * target_ratio)
            left = (img.width - new_width) // 2
            img = img.crop((left, 0, left + new_width, img.height))
        else:
            new_height = int(img.width / target_ratio)
            top = (img.height - new_height) // 2
            img = img.crop((0, top, img.width, top + new_height))
        
        return img.resize((target_width, target_height), Image.Resampling.LANCZOS)
    
    def _add_text_overlay(self, img: Image.Image, text: str, color_palette: List[str]) -> Image.Image:
        """æ·»åŠ æ–‡å­—ç–ŠåŠ """
        draw = ImageDraw.Draw(img)
        width, height = img.size
        
        try:
            font_size = width // 18
            try:
                font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", font_size)
            except:
                font = ImageFont.load_default()
            
            text = text[:40] if len(text) > 40 else text
            bbox = draw.textbbox((0, 0), text, font=font)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]
            
            x = (width - text_width) // 2
            y = height // 2 - text_height // 2
            
            padding = 30
            draw.rounded_rectangle(
                [(x - padding, y - padding), (x + text_width + padding, y + text_height + padding)],
                radius=15,
                fill=(0, 0, 0, 180)
            )
            draw.text((x, y), text, fill=(255, 255, 255), font=font)
            
        except Exception as e:
            print(f"[VideoGenerator] æ–‡å­—ç¹ªè£½éŒ¯èª¤: {e}")
        
        return img
    
    def _desaturate_color(self, rgb: tuple, factor: float = 0.4) -> tuple:
        """
        é™ä½é¡è‰²é£½å’Œåº¦
        factor: 0 = å®Œå…¨ç°åº¦, 1 = åŸè‰²
        """
        r, g, b = rgb
        gray = int(0.299 * r + 0.587 * g + 0.114 * b)
        return (
            int(gray + (r - gray) * factor),
            int(gray + (g - gray) * factor),
            int(gray + (b - gray) * factor)
        )
    
    def _generate_designed_image(
        self,
        color_palette: List[str],
        width: int,
        height: int,
        text_overlay: Optional[str],
        scene_num: int,
        total_scenes: int
    ) -> str:
        """
        ç”Ÿæˆç´”é»‘è‰²åŸºæœ¬è¨­è¨ˆåœ–
        
        çµ±ä¸€ä½¿ç”¨ç´”é»‘èƒŒæ™¯ï¼Œç°¡æ½”å°ˆæ¥­
        """
        if not PIL_AVAILABLE:
            return ""
        
        try:
            # å‰µå»ºç´”é»‘åœ–ç‰‡
            img = Image.new('RGB', (width, height), color=(0, 0, 0))
            
            print(f"[VideoGenerator] â¬› ç”Ÿæˆç´”é»‘è¨­è¨ˆåœ– å ´æ™¯ {scene_num}/{total_scenes} ({width}x{height})")
            
            # è½‰æ›ç‚º base64
            import io
            import base64
            buffer = io.BytesIO()
            img.save(buffer, format='PNG', quality=95)
            return f"data:image/png;base64,{base64.b64encode(buffer.getvalue()).decode()}"
            
        except Exception as e:
            print(f"[VideoGenerator] è¨­è¨ˆåœ–ç”ŸæˆéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return ""
    
    async def _generate_tts(
        self,
        text: str,
        project_id: str,
        scene_idx: int,
        voice_style: str = "friendly"
    ) -> Optional[str]:
        """ç”Ÿæˆ TTS èªéŸ³"""
        if not EDGE_TTS_AVAILABLE:
            return None
        
        try:
            voice = self.TTS_VOICES.get(voice_style, self.TTS_VOICES["friendly"])
            audio_path = self.output_dir / f"tts_{project_id}_{scene_idx}.mp3"
            
            communicate = edge_tts.Communicate(text, voice)
            await communicate.save(str(audio_path))
            
            print(f"[VideoGenerator] ğŸ¤ TTS: {text[:30]}...")
            return str(audio_path)
            
        except Exception as e:
            print(f"[VideoGenerator] TTS éŒ¯èª¤: {e}")
            return None
    
    async def _add_audio_to_video(
        self,
        video_path: str,
        script: Dict[str, Any],
        project_id: str,
        duration: int
    ) -> Optional[str]:
        """
        ç‚ºå½±ç‰‡æ·»åŠ  TTS æ—ç™½å’ŒèƒŒæ™¯éŸ³æ¨‚
        
        ç”¨æ–¼ Kling/Veo ç­‰ç›´æ¥ç”Ÿæˆçš„å½±ç‰‡ï¼Œå®ƒå€‘æœ¬èº«ä¸åŒ…å«éŸ³è¨Š
        """
        try:
            scenes = script.get("scenes", [])
            voice_id = script.get("tts_voice", "zh-TW-HsiaoChenNeural")
            music_url = script.get("music_url")
            music_volume = script.get("music_volume", 0.3)
            
            print(f"[VideoGenerator] ğŸ” éŸ³è¨Šè™•ç†: å ´æ™¯æ•¸={len(scenes)}, èªéŸ³={voice_id}")
            print(f"[VideoGenerator] ğŸ” éŸ³æ¨‚è¨­å®š: URL={music_url}, éŸ³é‡={music_volume}")
            
            # 1. ç”Ÿæˆ TTS æ—ç™½
            tts_audios = []
            total_narration = ""
            for i, scene in enumerate(scenes):
                # æ”¯æ´å¤šç¨®æ¬„ä½åç¨±ï¼šnarration_text, narration, subtitle_text
                narration = (
                    scene.get("narration_text", "") or 
                    scene.get("narration", "") or 
                    scene.get("subtitle_text", "")
                )
                # éæ¿¾æ‰ä½”ä½ç¬¦æ–‡å­—
                if narration in ["ï¼ˆç„¡æ—ç™½ï¼‰", "(ç„¡æ—ç™½)", "ï¼ˆæ²’æœ‰æ—ç™½ï¼‰", "(æ²’æœ‰æ—ç™½)", ""]:
                    narration = ""
                print(f"[VideoGenerator] ğŸ” å ´æ™¯ {i+1} æ—ç™½: '{narration[:30] if narration else '(ç„¡å¯¦éš›æ—ç™½)'}...'")
                if narration:
                    total_narration += narration + " "
            
            print(f"[VideoGenerator] ğŸ” ç¸½æ—ç™½é•·åº¦: {len(total_narration)} å­—å…ƒ, EDGE_TTS={EDGE_TTS_AVAILABLE}")
            
            # ä½¿ç”¨å®Œæ•´æ—ç™½ç”Ÿæˆå–®ä¸€ TTS æª”æ¡ˆ
            tts_path = None
            if total_narration.strip() and EDGE_TTS_AVAILABLE:
                tts_path = self.output_dir / f"tts_full_{project_id}.mp3"
                try:
                    communicate = edge_tts.Communicate(total_narration.strip(), voice_id)
                    await communicate.save(str(tts_path))
                    print(f"[VideoGenerator] ğŸ¤ TTS ç”Ÿæˆå®Œæˆ: {total_narration[:50]}...")
                except Exception as e:
                    print(f"[VideoGenerator] TTS ç”Ÿæˆå¤±æ•—: {e}")
                    tts_path = None
            
            # 2. èƒŒæ™¯éŸ³æ¨‚è™•ç†
            music_path = None
            music_genre = script.get("music_genre", "upbeat")
            
            # å„ªå…ˆä½¿ç”¨ç”¨æˆ¶ä¸Šå‚³çš„è‡ªè¨‚éŸ³æ¨‚
            custom_music_base64 = script.get("custom_music_base64") or getattr(self, '_custom_music_base64', None)
            custom_music_name = script.get("custom_music_name") or getattr(self, '_custom_music_name', None)
            
            if custom_music_base64:
                print(f"[VideoGenerator] ğŸµ ä½¿ç”¨ç”¨æˆ¶è‡ªè¨‚éŸ³æ¨‚: {custom_music_name or 'æœªå‘½å'}")
                try:
                    # è§£ç¢¼ base64 ä¸¦ä¿å­˜ç‚ºè‡¨æ™‚æª”æ¡ˆ
                    import base64
                    music_data = base64.b64decode(custom_music_base64)
                    # å¾æª”åæ¨æ–·æ ¼å¼ï¼Œé è¨­ç‚º mp3
                    ext = "mp3"
                    if custom_music_name:
                        ext = custom_music_name.split('.')[-1].lower()
                        if ext not in ['mp3', 'wav', 'ogg', 'aac', 'm4a']:
                            ext = 'mp3'
                    music_path = self.output_dir / f"custom_music_{project_id}.{ext}"
                    with open(music_path, 'wb') as f:
                        f.write(music_data)
                    print(f"[VideoGenerator] âœ… è‡ªè¨‚éŸ³æ¨‚å·²ä¿å­˜: {music_path}, å¤§å°: {len(music_data)} bytes")
                except Exception as e:
                    print(f"[VideoGenerator] âš ï¸ è‡ªè¨‚éŸ³æ¨‚è™•ç†å¤±æ•—: {e}")
                    music_path = None
            
            # å¦‚æœæ²’æœ‰è‡ªè¨‚éŸ³æ¨‚ï¼Œä½¿ç”¨é è¨­éŸ³æ¨‚åº«
            if not music_path:
                # è™•ç†é¢¨æ ¼é¸æ“‡æ¨¡å¼ (style:xxx) - å¾ Mixkit å…è²»éŸ³æ¨‚åº«ç²å–
                actual_music_url = music_url
                if music_url and music_url.startswith("style:"):
                    style = music_url.replace("style:", "")
                    music_genre = style
                    actual_music_url = get_music_url_for_style(style)
                    print(f"[VideoGenerator] ğŸµ é¢¨æ ¼é¸æ“‡: {style} -> Mixkit éŸ³æ¨‚")
                
                # å„ªå…ˆä½¿ç”¨ Mixkit å…è²»å•†ç”¨éŸ³æ¨‚
                if actual_music_url and actual_music_url.startswith("http"):
                    print(f"[VideoGenerator] ğŸµ ä¸‹è¼‰ Mixkit éŸ³æ¨‚...")
                    music_path = await self._download_external_music(actual_music_url, project_id)
                    if music_path:
                        print(f"[VideoGenerator] âœ… Mixkit éŸ³æ¨‚ä¸‹è¼‰æˆåŠŸ")
                    else:
                        # å˜—è©¦å‚™ç”¨
                        fallback_url = get_music_url_for_style(music_genre)
                        if fallback_url != actual_music_url:
                            music_path = await self._download_external_music(fallback_url, project_id)
                
                # AI éŸ³æ¨‚æ¨™è¨˜ï¼ˆå‚™ç”¨ï¼‰
                if not music_path and actual_music_url and actual_music_url.startswith("ai:"):
                    ai_style = actual_music_url.replace("ai:", "")
                    print(f"[VideoGenerator] ğŸ¹ ä½¿ç”¨ AI ç”ŸæˆèƒŒæ™¯éŸ³æ¨‚ (é¢¨æ ¼: {ai_style})")
                    music_path = await self._generate_background_music(ai_style, duration, project_id)
                
                if not music_path:
                    # æœ€å¾Œå›é€€
                    fallback_url = get_music_url_for_style(music_genre)
                    music_path = await self._download_external_music(fallback_url, project_id)
                
                if not music_path:
                    print(f"[VideoGenerator] ğŸ¹ å›é€€åˆ° AI ç”ŸæˆèƒŒæ™¯éŸ³æ¨‚")
                    music_path = await self._generate_background_music(music_genre, duration, project_id)
            
            # 3. å¦‚æœæ²’æœ‰éŸ³è¨Šï¼Œç›´æ¥è¿”å›åŸå½±ç‰‡
            if not tts_path and not music_path:
                print("[VideoGenerator] âš ï¸ æ²’æœ‰å¯ç”¨çš„éŸ³è¨Šï¼Œè¿”å›åŸå§‹å½±ç‰‡")
                return video_path
            
            # 4. ä½¿ç”¨ FFmpeg åˆæˆ
            output_path = self.output_dir / f"final_{project_id}.mp4"
            static_dir = Path("/app/static/videos")
            final_path = static_dir / f"final_{project_id}.mp4"
            
            if tts_path and music_path:
                # TTS + èƒŒæ™¯éŸ³æ¨‚ï¼ˆä¿æŒåŸå§‹å½±ç‰‡é•·åº¦ï¼‰
                cmd = [
                    "ffmpeg", "-y",
                    "-i", video_path,
                    "-i", str(tts_path),
                    "-stream_loop", "-1", "-i", str(music_path),  # å¾ªç’°æ’­æ”¾éŸ³æ¨‚
                    "-filter_complex",
                    f"[1:a]apad=pad_dur={duration}[tts_padded];[2:a]volume={music_volume}[bgm];[tts_padded][bgm]amix=inputs=2:duration=first:dropout_transition=2[aout]",
                    "-map", "0:v:0",
                    "-map", "[aout]",
                    "-c:v", "copy",
                    "-c:a", "aac",
                    "-b:a", "192k",
                    "-t", str(duration),  # é™åˆ¶è¼¸å‡ºé•·åº¦ç‚ºåŸå§‹å½±ç‰‡é•·åº¦
                    str(final_path)
                ]
            elif tts_path:
                # åªæœ‰ TTSï¼ˆç”¨éœéŸ³å¡«å……è‡³å½±ç‰‡é•·åº¦ï¼‰
                cmd = [
                    "ffmpeg", "-y",
                    "-i", video_path,
                    "-i", str(tts_path),
                    "-filter_complex", f"[1:a]apad=pad_dur={duration}[aout]",
                    "-map", "0:v:0",
                    "-map", "[aout]",
                    "-c:v", "copy",
                    "-c:a", "aac",
                    "-b:a", "192k",
                    "-t", str(duration),
                    str(final_path)
                ]
            else:
                # åªæœ‰èƒŒæ™¯éŸ³æ¨‚ï¼ˆå¾ªç’°æ’­æ”¾ï¼‰
                cmd = [
                    "ffmpeg", "-y",
                    "-i", video_path,
                    "-stream_loop", "-1", "-i", str(music_path),  # å¾ªç’°æ’­æ”¾éŸ³æ¨‚
                    "-filter_complex", f"[1:a]volume={music_volume}[bgm]",
                    "-map", "0:v:0",
                    "-map", "[bgm]",
                    "-c:v", "copy",
                    "-c:a", "aac",
                    "-b:a", "192k",
                    "-t", str(duration),  # é™åˆ¶è¼¸å‡ºé•·åº¦
                    str(final_path)
                ]
            
            print(f"[VideoGenerator] ğŸ¬ FFmpeg åˆæˆéŸ³è¨Š...")
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await process.communicate()
            
            if process.returncode != 0:
                print(f"[VideoGenerator] âŒ FFmpeg éŒ¯èª¤: {stderr.decode()[:500]}")
                return video_path
            
            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            try:
                if tts_path and os.path.exists(tts_path):
                    os.remove(tts_path)
                if music_path and os.path.exists(music_path):
                    os.remove(music_path)
            except:
                pass
            
            print(f"[VideoGenerator] âœ… éŸ³è¨Šåˆæˆå®Œæˆ")
            return str(final_path)
            
        except Exception as e:
            print(f"[VideoGenerator] âŒ éŸ³è¨Šæ·»åŠ å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return video_path
    
    def _save_to_local(self, video_path: str, project_id: str) -> str:
        """ä¿å­˜å½±ç‰‡åˆ°æœ¬åœ°éœæ…‹ç›®éŒ„"""
        import shutil
        static_dir = Path("/app/static/videos")
        static_dir.mkdir(parents=True, exist_ok=True)
        
        video_filename = f"video_{project_id}.mp4"
        static_path = static_dir / video_filename
        
        shutil.move(video_path, static_path)
        print(f"[VideoGenerator] ğŸ“ å½±ç‰‡å·²ä¿å­˜: {static_path}")
        
        return f"/video/download/{video_filename}"
    
    async def _download_external_music(
        self,
        music_url: str,
        project_id: str
    ) -> Optional[str]:
        """
        ä¸‹è¼‰å¤–éƒ¨éŸ³æ¨‚æª”æ¡ˆï¼ˆPixabay ç­‰å…è²»è³‡æºï¼‰
        """
        try:
            import aiohttp
            
            music_path = self.output_dir / f"bgm_ext_{project_id}.mp3"
            
            # æ·»åŠ ç€è¦½å™¨æ¨™é ­ä»¥ç¹éé˜²ç›œéˆ
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "audio/mpeg, audio/*, */*",
                "Accept-Language": "en-US,en;q=0.9,zh-TW;q=0.8,zh;q=0.7",
                "Referer": "https://pixabay.com/",
                "Origin": "https://pixabay.com",
            }
            
            async with aiohttp.ClientSession(headers=headers) as session:
                async with session.get(music_url, timeout=aiohttp.ClientTimeout(total=60)) as response:
                    if response.status == 200:
                        content = await response.read()
                        with open(music_path, 'wb') as f:
                            f.write(content)
                        print(f"[VideoGenerator] ğŸµ å¤–éƒ¨éŸ³æ¨‚ä¸‹è¼‰å®Œæˆ: {len(content) / 1024:.1f} KB")
                        return str(music_path)
                    else:
                        print(f"[VideoGenerator] å¤–éƒ¨éŸ³æ¨‚ä¸‹è¼‰å¤±æ•—: HTTP {response.status}")
                        # å˜—è©¦å‚™ç”¨æ–¹æ³•ï¼šä½¿ç”¨ httpx
                        return await self._download_music_httpx(music_url, project_id)
                        
        except Exception as e:
            print(f"[VideoGenerator] å¤–éƒ¨éŸ³æ¨‚ä¸‹è¼‰éŒ¯èª¤: {e}")
            # å˜—è©¦å‚™ç”¨æ–¹æ³•
            return await self._download_music_httpx(music_url, project_id)
    
    async def _download_music_httpx(
        self,
        music_url: str,
        project_id: str
    ) -> Optional[str]:
        """
        å‚™ç”¨ä¸‹è¼‰æ–¹æ³•ï¼šä½¿ç”¨ httpx
        """
        try:
            import httpx
            
            music_path = self.output_dir / f"bgm_ext_{project_id}.mp3"
            
            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "*/*",
                "Referer": "https://pixabay.com/",
            }
            
            async with httpx.AsyncClient(timeout=60.0, follow_redirects=True) as client:
                response = await client.get(music_url, headers=headers)
                if response.status_code == 200:
                    with open(music_path, 'wb') as f:
                        f.write(response.content)
                    print(f"[VideoGenerator] ğŸµ å¤–éƒ¨éŸ³æ¨‚ä¸‹è¼‰å®Œæˆ (httpx): {len(response.content) / 1024:.1f} KB")
                    return str(music_path)
                else:
                    print(f"[VideoGenerator] httpx ä¸‹è¼‰å¤±æ•—: HTTP {response.status_code}")
                    return None
                    
        except Exception as e:
            print(f"[VideoGenerator] httpx ä¸‹è¼‰éŒ¯èª¤: {e}")
            return None
    
    async def _generate_background_music(
        self,
        mood: str,
        duration: float,
        project_id: str
    ) -> Optional[str]:
        """
        ç”Ÿæˆå°ˆæ¥­ç´šèƒŒæ™¯éŸ³æ¨‚
        
        åŒ…å«ï¼š
        - å’Œå¼¦é€²è¡Œ
        - ç¯€å¥å‹æ…‹
        - ä½éŸ³ç·š
        - ç’°å¢ƒéŸ³æ•ˆ
        - å‹•æ…‹è®ŠåŒ–
        """
        try:
            music_path = self.output_dir / f"bgm_{project_id}.wav"
            
            sample_rate = 44100
            total_samples = int(duration * sample_rate)
            
            # å°ˆæ¥­å’Œå¼¦é…ç½® - ä¸åŒæƒ…ç·’çš„å’Œå¼¦é€²è¡Œ
            MOOD_CONFIGS = {
                "upbeat": {
                    "chords": [
                        [261.63, 329.63, 392.00],  # C major
                        [293.66, 369.99, 440.00],  # D major
                        [329.63, 415.30, 493.88],  # E minor
                        [349.23, 440.00, 523.25],  # F major
                    ],
                    "bpm": 120,
                    "bass_octave": 0.5,
                    "brightness": 1.2,
                    "rhythm_intensity": 0.8,
                },
                "calm": {
                    "chords": [
                        [220.00, 277.18, 329.63],  # A minor
                        [246.94, 311.13, 369.99],  # B diminished
                        [261.63, 329.63, 392.00],  # C major
                        [293.66, 349.23, 440.00],  # D minor
                    ],
                    "bpm": 70,
                    "bass_octave": 0.25,
                    "brightness": 0.7,
                    "rhythm_intensity": 0.3,
                },
                "energetic": {
                    "chords": [
                        [329.63, 415.30, 493.88],  # E major
                        [369.99, 466.16, 554.37],  # F# minor
                        [392.00, 493.88, 587.33],  # G major
                        [440.00, 554.37, 659.25],  # A major
                    ],
                    "bpm": 140,
                    "bass_octave": 0.5,
                    "brightness": 1.4,
                    "rhythm_intensity": 1.0,
                },
                "emotional": {
                    "chords": [
                        [261.63, 311.13, 392.00],  # C sus2
                        [293.66, 349.23, 440.00],  # D minor
                        [220.00, 277.18, 329.63],  # A minor
                        [246.94, 293.66, 369.99],  # B minor 7
                    ],
                    "bpm": 80,
                    "bass_octave": 0.25,
                    "brightness": 0.9,
                    "rhythm_intensity": 0.4,
                },
                "epic": {
                    "chords": [
                        [261.63, 329.63, 392.00],  # C major
                        [220.00, 277.18, 329.63],  # A minor
                        [349.23, 440.00, 523.25],  # F major
                        [392.00, 493.88, 587.33],  # G major
                    ],
                    "bpm": 100,
                    "bass_octave": 0.5,
                    "brightness": 1.3,
                    "rhythm_intensity": 0.9,
                },
                "minimal": {
                    "chords": [
                        [261.63, 392.00],  # C5
                        [293.66, 440.00],  # D5
                        [329.63, 493.88],  # E5
                        [261.63, 392.00],  # C5
                    ],
                    "bpm": 90,
                    "bass_octave": 0.25,
                    "brightness": 0.6,
                    "rhythm_intensity": 0.2,
                },
                "inspirational": {
                    "chords": [
                        [261.63, 329.63, 392.00],  # C major
                        [329.63, 392.00, 493.88],  # E minor
                        [349.23, 440.00, 523.25],  # F major
                        [392.00, 493.88, 587.33],  # G major
                    ],
                    "bpm": 95,
                    "bass_octave": 0.5,
                    "brightness": 1.1,
                    "rhythm_intensity": 0.6,
                },
            }
            
            config = MOOD_CONFIGS.get(mood, MOOD_CONFIGS["upbeat"])
            chords = config["chords"]
            bpm = config["bpm"]
            bass_octave = config["bass_octave"]
            brightness = config["brightness"]
            rhythm_intensity = config["rhythm_intensity"]
            
            # è¨ˆç®—ç¯€æ‹
            beat_duration = 60.0 / bpm
            samples_per_beat = int(beat_duration * sample_rate)
            chord_duration = beat_duration * 4  # æ¯å€‹å’Œå¼¦æŒçºŒ 4 æ‹
            samples_per_chord = int(chord_duration * sample_rate)
            
            audio_data = []
            
            for i in range(total_samples):
                t = i / sample_rate
                
                # ç•¶å‰å’Œå¼¦
                chord_idx = int(t / chord_duration) % len(chords)
                freqs = chords[chord_idx]
                
                # å’Œå¼¦å¢ŠéŸ³ï¼ˆæŸ”å’Œçš„èƒŒæ™¯ï¼‰
                pad = 0.0
                for f in freqs:
                    # ä½¿ç”¨æ­£å¼¦æ³¢ + è¼•å¾®è«§æ³¢
                    pad += math.sin(2 * math.pi * f * t) * 0.08
                    pad += math.sin(2 * math.pi * f * 2 * t) * 0.02 * brightness  # å…«åº¦è«§æ³¢
                    pad += math.sin(2 * math.pi * f * 0.5 * t) * 0.04  # ä½å…«åº¦
                
                # ä½éŸ³ç·š
                bass_freq = freqs[0] * bass_octave
                bass = math.sin(2 * math.pi * bass_freq * t) * 0.12
                # ä½éŸ³åŒ…çµ¡ - åœ¨æ¯æ‹é–‹å§‹æ™‚å¼·èª¿
                beat_position = (t % beat_duration) / beat_duration
                bass_envelope = math.exp(-beat_position * 3) * 0.8 + 0.2
                bass *= bass_envelope
                
                # ç¯€å¥å…ƒç´  - è¼•å¾®çš„è„ˆè¡
                rhythm = 0.0
                if rhythm_intensity > 0.3:
                    pulse_freq = bpm / 60  # æ¯ç§’æ‹æ•¸
                    rhythm = math.sin(2 * math.pi * pulse_freq * t) * 0.05 * rhythm_intensity
                    # æ·»åŠ é«˜å¸½æ„Ÿè¦ºçš„é«˜é »
                    hihat_t = t % (beat_duration / 2)
                    if hihat_t < 0.01:
                        rhythm += 0.03 * rhythm_intensity
                
                # ç’°å¢ƒå±¤ - éå¸¸è¼•å¾®çš„å™ªéŸ³æ„Ÿ
                import random
                ambient = (random.random() - 0.5) * 0.01
                
                # å‹•æ…‹è®ŠåŒ– - æ ¹æ“šæ™‚é–“ä½ç½®èª¿æ•´éŸ³é‡
                progress = i / total_samples
                
                # é–‹å ´æ¼¸å…¥ï¼ˆå‰ 10%ï¼‰
                if progress < 0.1:
                    dynamics = progress / 0.1
                # çµå°¾æ¼¸å‡ºï¼ˆå¾Œ 15%ï¼‰
                elif progress > 0.85:
                    dynamics = (1.0 - progress) / 0.15
                # ä¸­é–“é«˜æ½®é»
                elif 0.4 < progress < 0.6:
                    dynamics = 1.0 + (0.5 - abs(progress - 0.5)) * 0.3
                else:
                    dynamics = 1.0
                
                # æ··åˆæ‰€æœ‰å…ƒç´ 
                sample = (pad + bass + rhythm + ambient) * dynamics * 0.7
                
                # è»Ÿé™å¹…
                if sample > 0.95:
                    sample = 0.95
                elif sample < -0.95:
                    sample = -0.95
                
                audio_data.append(int(sample * 32767))
            
            # å¯«å…¥ WAV æ–‡ä»¶ï¼ˆç«‹é«”è²ï¼‰
            with open(music_path, 'wb') as f:
                num_channels = 1
                bits_per_sample = 16
                byte_rate = sample_rate * num_channels * bits_per_sample // 8
                block_align = num_channels * bits_per_sample // 8
                data_size = len(audio_data) * bits_per_sample // 8
                
                f.write(b'RIFF')
                f.write(struct.pack('<I', 36 + data_size))
                f.write(b'WAVE')
                f.write(b'fmt ')
                f.write(struct.pack('<I', 16))  # PCM
                f.write(struct.pack('<H', 1))   # Audio format (PCM)
                f.write(struct.pack('<H', num_channels))
                f.write(struct.pack('<I', sample_rate))
                f.write(struct.pack('<I', byte_rate))
                f.write(struct.pack('<H', block_align))
                f.write(struct.pack('<H', bits_per_sample))
                f.write(b'data')
                f.write(struct.pack('<I', data_size))
                for s in audio_data:
                    f.write(struct.pack('<h', s))
            
            print(f"[VideoGenerator] ğŸµ å°ˆæ¥­èƒŒæ™¯éŸ³æ¨‚ ({mood}, {bpm}BPM, {duration:.1f}ç§’)")
            return str(music_path)
            
        except Exception as e:
            print(f"[VideoGenerator] èƒŒæ™¯éŸ³æ¨‚éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    async def _create_video_ffmpeg(
        self,
        scene_images: List[str],
        scenes: List[Dict],
        scene_audios: List[Optional[str]],
        music_path: Optional[str],
        project_id: str,
        width: int,
        height: int,
        music_volume: float = 0.3
    ) -> Optional[str]:
        """
        ä½¿ç”¨ FFmpeg åˆæˆå°ˆæ¥­ç´šå½±ç‰‡
        
        ç‰¹æ•ˆåŒ…å«ï¼š
        - Ken Burns æ•ˆæœï¼ˆç·©æ…¢ç¸®æ”¾/å¹³ç§»å‹•æ…‹ï¼‰
        - å ´æ™¯è½‰å ´ï¼ˆäº¤å‰æ·¡åŒ–ï¼‰
        - TTS èªéŸ³æ··åˆ
        - é«˜å“è³ªç·¨ç¢¼
        """
        
        # æª¢æŸ¥ FFmpeg
        try:
            result = await asyncio.create_subprocess_exec(
                "ffmpeg", "-version",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            await result.communicate()
            if result.returncode != 0:
                return None
        except:
            print("[VideoGenerator] FFmpeg æœªå®‰è£")
            return None
        
        try:
            import random
            
            # Ken Burns æ•ˆæœé…ç½® - è®“éœæ…‹åœ–ç‰‡ç”¢ç”Ÿå‹•æ…‹æ„Ÿ
            KEN_BURNS_EFFECTS = [
                # (èµ·å§‹ç¸®æ”¾, çµæŸç¸®æ”¾, Xåç§»æ–¹å‘, Yåç§»æ–¹å‘)
                (1.0, 1.15, 0.5, 0.5),    # ç·©æ…¢æ”¾å¤§ï¼Œä¸­å¿ƒ
                (1.15, 1.0, 0.5, 0.5),    # ç·©æ…¢ç¸®å°ï¼Œä¸­å¿ƒ
                (1.0, 1.12, 0.0, 0.0),    # æ”¾å¤§ï¼Œå·¦ä¸Šè§’
                (1.0, 1.12, 1.0, 1.0),    # æ”¾å¤§ï¼Œå³ä¸‹è§’
                (1.12, 1.0, 0.0, 1.0),    # ç¸®å°ï¼Œå·¦ä¸‹è§’
                (1.12, 1.0, 1.0, 0.0),    # ç¸®å°ï¼Œå³ä¸Šè§’
                (1.0, 1.08, 0.5, 0.0),    # å¾®æ”¾å¤§ï¼Œä¸Šä¸­
                (1.0, 1.08, 0.5, 1.0),    # å¾®æ”¾å¤§ï¼Œä¸‹ä¸­
            ]
            
            # è½‰å ´æ™‚é•·ï¼ˆç§’ï¼‰
            TRANSITION_DURATION = 0.5
            
            # ä¿å­˜åœ–ç‰‡ä¸¦ç”Ÿæˆå¸¶æ•ˆæœçš„ç‰‡æ®µ
            image_paths = []
            for i, img_base64 in enumerate(scene_images):
                img_data = img_base64.split(",")[1] if "," in img_base64 else img_base64
                img_bytes = base64.b64decode(img_data)
                
                img_path = self.output_dir / f"scene_{project_id}_{i}.png"
                with open(img_path, "wb") as f:
                    f.write(img_bytes)
                
                duration = scenes[i].get("duration_seconds", 5) if i < len(scenes) else 5
                camera_movement = scenes[i].get("camera_movement", "static") if i < len(scenes) else "static"
                image_paths.append((str(img_path), duration, camera_movement))
            
            # ç”Ÿæˆæ¯å€‹å ´æ™¯çš„è¦–é »ç‰‡æ®µï¼ˆå¸¶ Ken Burns æ•ˆæœï¼‰
            segment_files = []
            for i, (img_path, duration, camera_move) in enumerate(image_paths):
                segment_path = self.output_dir / f"segment_{project_id}_{i}.mp4"
                
                # æ ¹æ“šå ´æ™¯ç·¨è™Ÿé¸æ“‡ä¸åŒçš„ Ken Burns æ•ˆæœ
                effect = KEN_BURNS_EFFECTS[i % len(KEN_BURNS_EFFECTS)]
                start_scale, end_scale, x_dir, y_dir = effect
                
                # æ ¹æ“š camera_movement èª¿æ•´æ•ˆæœ
                if camera_move == "dolly_in":
                    start_scale, end_scale = 1.0, 1.2
                elif camera_move == "dolly_out":
                    start_scale, end_scale = 1.2, 1.0
                elif camera_move == "tracking":
                    x_dir = 0.0 if random.random() > 0.5 else 1.0
                elif camera_move == "crane_up":
                    y_dir = 1.0
                    start_scale, end_scale = 1.0, 1.1
                elif camera_move == "crane_down":
                    y_dir = 0.0
                    start_scale, end_scale = 1.1, 1.0
                elif camera_move == "orbit":
                    x_dir = 0.0
                    start_scale, end_scale = 1.05, 1.05
                
                # è¨ˆç®— FFmpeg zoompan æ¿¾é¡åƒæ•¸
                # zoompan åƒæ•¸ï¼šz=ç¸®æ”¾, x=Xä½ç½®, y=Yä½ç½®, d=ç¸½å¹€æ•¸, s=è¼¸å‡ºå°ºå¯¸, fps=å¹€ç‡
                fps = 30
                total_frames = int(duration * fps)
                
                # è¨ˆç®—ç¸®æ”¾å‹•ç•«
                # z å¾ start_scale åˆ° end_scale
                # ä½¿ç”¨ easing è®“å‹•ç•«æ›´æµæš¢
                zoom_expr = f"if(lte(on,1),{start_scale},{start_scale}+(on/{total_frames})*({end_scale}-{start_scale}))"
                
                # è¨ˆç®—ä½ç½®ï¼ˆè®“åœ–ç‰‡åœ¨æ”¾å¤§æ™‚é©ç•¶åç§»ï¼‰
                # ç•¶æ”¾å¤§æ™‚ï¼Œä½ç½®å¾ä¸­å¿ƒå‘æŒ‡å®šæ–¹å‘åç§»
                x_offset = f"(iw-iw/zoom)/2 + (iw/zoom-iw)*{x_dir}*(on/{total_frames})"
                y_offset = f"(ih-ih/zoom)/2 + (ih/zoom-ih)*{y_dir}*(on/{total_frames})"
                
                # æ§‹å»º zoompan æ¿¾é¡
                zoompan_filter = f"zoompan=z='{zoom_expr}':x='{x_offset}':y='{y_offset}':d={total_frames}:s={width}x{height}:fps={fps}"
                
                # æ·»åŠ æ·¡å…¥æ•ˆæœï¼ˆç¬¬ä¸€å€‹å ´æ™¯ï¼‰å’Œæ·¡å‡ºæ•ˆæœï¼ˆæœ€å¾Œä¸€å€‹å ´æ™¯ï¼‰
                fade_filter = ""
                if i == 0:
                    fade_filter = f",fade=t=in:st=0:d=0.5"
                if i == len(image_paths) - 1:
                    fade_filter += f",fade=t=out:st={duration - 0.5}:d=0.5"
                
                # å®Œæ•´çš„è¦–è¦ºæ¿¾é¡éˆ
                video_filter = f"{zoompan_filter}{fade_filter},format=yuv420p"
                
                cmd = [
                    "ffmpeg", "-y",
                    "-loop", "1",
                    "-i", img_path,
                    "-t", str(duration),
                    "-vf", video_filter,
                    "-c:v", "libx264",
                    "-preset", "slow",       # æ›´é«˜å“è³ª
                    "-crf", "18",            # é«˜å“è³ª
                    "-profile:v", "high",    # H.264 High Profile
                    "-level", "4.1",         # æ”¯æ´ 1080p@30fps
                    "-r", str(fps),
                    "-pix_fmt", "yuv420p",
                    "-movflags", "+faststart",  # æ”¯æ´ç¶²é ä¸²æµ
                    str(segment_path)
                ]
                
                print(f"[VideoGenerator] ğŸï¸ å ´æ™¯ {i+1}: Ken Burns æ•ˆæœ ({start_scale:.2f}â†’{end_scale:.2f})")
                
                process = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                stdout, stderr = await process.communicate()
                
                if process.returncode != 0:
                    print(f"[VideoGenerator] å ´æ™¯ {i+1} FFmpeg éŒ¯èª¤: {stderr.decode()[:200]}")
                    # é™ç´šåˆ°ç°¡å–®æ¨¡å¼
                    simple_cmd = [
                        "ffmpeg", "-y",
                        "-loop", "1",
                        "-i", img_path,
                        "-t", str(duration),
                        "-vf", f"scale={width}:{height}:force_original_aspect_ratio=decrease,pad={width}:{height}:(ow-iw)/2:(oh-ih)/2:black,format=yuv420p",
                        "-c:v", "libx264",
                        "-preset", "medium",
                        "-crf", "20",
                        "-r", "30",
                        "-pix_fmt", "yuv420p",
                        str(segment_path)
                    ]
                    process = await asyncio.create_subprocess_exec(
                        *simple_cmd,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )
                    await process.communicate()
                
                if os.path.exists(segment_path):
                    segment_files.append(str(segment_path))
            
            if not segment_files:
                print("[VideoGenerator] âŒ æ²’æœ‰æˆåŠŸç”Ÿæˆçš„ç‰‡æ®µ")
                return None
            
            print(f"[VideoGenerator] âœ… {len(segment_files)} å€‹å ´æ™¯ç‰‡æ®µç”Ÿæˆå®Œæˆ")
            
            # ä½¿ç”¨ xfade è½‰å ´åˆä½µç‰‡æ®µï¼ˆäº¤å‰æ·¡åŒ–æ•ˆæœï¼‰
            merged_video = self.output_dir / f"merged_{project_id}.mp4"
            
            if len(segment_files) == 1:
                # åªæœ‰ä¸€å€‹ç‰‡æ®µï¼Œç›´æ¥è¤‡è£½
                import shutil
                shutil.copy(segment_files[0], str(merged_video))
            else:
                # æ§‹å»º xfade æ¿¾é¡éˆé€²è¡Œäº¤å‰æ·¡åŒ–è½‰å ´
                # è¨ˆç®—æ¯å€‹ç‰‡æ®µçš„æ™‚é•·ï¼ˆç”¨æ–¼è¨­ç½® offsetï¼‰
                offsets = []
                cumulative = 0
                for i, (_, dur, _) in enumerate(image_paths[:-1]):
                    cumulative += dur - TRANSITION_DURATION
                    offsets.append(cumulative)
                
                # æ§‹å»ºè¤‡é›œæ¿¾é¡
                inputs = " ".join([f"-i {seg}" for seg in segment_files])
                
                # ç”Ÿæˆ xfade æ¿¾é¡éˆ
                filter_complex = []
                prev_label = "[0:v]"
                
                # è±å¯Œçš„è½‰å ´æ•ˆæœåº« - 20+ ç¨®å°ˆæ¥­éå ´
                TRANSITION_EFFECTS = [
                    # åŸºç¤æ·¡åŒ–
                    "fade",           # äº¤å‰æ·¡åŒ–
                    "fadeblack",      # æ·¡å…¥é»‘è‰²
                    "fadewhite",      # æ·¡å…¥ç™½è‰²
                    "fadegrays",      # æ·¡å…¥ç°éš
                    # æ»‘å‹•æ•ˆæœ
                    "slideleft",      # å·¦æ»‘
                    "slideright",     # å³æ»‘
                    "slideup",        # ä¸Šæ»‘
                    "slidedown",      # ä¸‹æ»‘
                    # æ“¦é™¤æ•ˆæœ
                    "wipeleft",       # å·¦æ“¦é™¤
                    "wiperight",      # å³æ“¦é™¤
                    "wipeup",         # ä¸Šæ“¦é™¤
                    "wipedown",       # ä¸‹æ“¦é™¤
                    # å¹¾ä½•æ•ˆæœ
                    "circlecrop",     # åœ“å½¢è£åˆ‡
                    "circleopen",     # åœ“å½¢å±•é–‹
                    "circleclose",    # åœ“å½¢æ”¶ç¸®
                    "rectcrop",       # çŸ©å½¢è£åˆ‡
                    "diagtl",         # å°è§’ç·šï¼ˆå·¦ä¸Šï¼‰
                    "diagtr",         # å°è§’ç·šï¼ˆå³ä¸Šï¼‰
                    "diagbl",         # å°è§’ç·šï¼ˆå·¦ä¸‹ï¼‰
                    "diagbr",         # å°è§’ç·šï¼ˆå³ä¸‹ï¼‰
                    # ç‰¹æ®Šæ•ˆæœ
                    "dissolve",       # æº¶è§£
                    "pixelize",       # åƒç´ åŒ–
                    "radial",         # å¾‘å‘
                    "horzopen",       # æ°´å¹³å±•é–‹
                    "horzclose",      # æ°´å¹³æ”¶ç¸®
                    "vertopen",       # å‚ç›´å±•é–‹
                    "vertclose",      # å‚ç›´æ”¶ç¸®
                    "smoothleft",     # å¹³æ»‘å·¦æ»‘
                    "smoothright",    # å¹³æ»‘å³æ»‘
                    "smoothup",       # å¹³æ»‘ä¸Šæ»‘
                    "smoothdown",     # å¹³æ»‘ä¸‹æ»‘
                ]
                
                import random
                # éš¨æ©Ÿæ‰“äº‚è½‰å ´æ•ˆæœé †åºï¼Œå¢åŠ è®ŠåŒ–æ€§
                shuffled_transitions = TRANSITION_EFFECTS.copy()
                random.shuffle(shuffled_transitions)
                
                for i in range(len(segment_files) - 1):
                    next_label = f"[{i+1}:v]"
                    output_label = f"[v{i}]" if i < len(segment_files) - 2 else "[vout]"
                    offset = offsets[i]
                    
                    # ä½¿ç”¨éš¨æ©Ÿé¸æ“‡çš„è½‰å ´æ•ˆæœ
                    transition_type = shuffled_transitions[i % len(shuffled_transitions)]
                    
                    filter_complex.append(
                        f"{prev_label}{next_label}xfade=transition={transition_type}:duration={TRANSITION_DURATION}:offset={offset}{output_label}"
                    )
                    prev_label = output_label
                    
                    print(f"[VideoGenerator] ğŸ¬ å ´æ™¯ {i+1}â†’{i+2} è½‰å ´: {transition_type}")
                
                filter_str = ";".join(filter_complex)
                
                # åŸ·è¡Œå¸¶è½‰å ´çš„åˆä½µ
                cmd_str = f'ffmpeg -y {inputs} -filter_complex "{filter_str}" -map "[vout]" -c:v libx264 -preset slow -crf 18 -pix_fmt yuv420p {str(merged_video)}'
                
                process = await asyncio.create_subprocess_shell(
                    cmd_str,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                stdout, stderr = await process.communicate()
                
                if process.returncode != 0:
                    print(f"[VideoGenerator] xfade è½‰å ´å¤±æ•—ï¼Œä½¿ç”¨ç°¡å–®åˆä½µ: {stderr.decode()[:200]}")
                    # é™ç´šåˆ°ç°¡å–® concat
                    concat_file = self.output_dir / f"concat_{project_id}.txt"
                    with open(concat_file, "w") as f:
                        for seg in segment_files:
                            f.write(f"file '{seg}'\n")
                    
                    cmd = [
                        "ffmpeg", "-y",
                        "-f", "concat",
                        "-safe", "0",
                        "-i", str(concat_file),
                        "-c:v", "libx264",
                        "-preset", "slow",
                        "-crf", "18",
                        str(merged_video)
                    ]
                    
                    process = await asyncio.create_subprocess_exec(
                        *cmd,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )
                    await process.communicate()
                    
                    if concat_file.exists():
                        os.remove(concat_file)
            
            if not os.path.exists(merged_video):
                print("[VideoGenerator] âŒ å½±ç‰‡åˆä½µå¤±æ•—")
                return None
            
            print("[VideoGenerator] âœ… å½±ç‰‡è½‰å ´åˆä½µå®Œæˆ")
            
            # æ··åˆéŸ³è¨Šï¼ˆTTS + èƒŒæ™¯éŸ³æ¨‚ï¼‰
            output_path = self.output_dir / f"video_{project_id}.mp4"
            
            # åˆä½µæ‰€æœ‰ TTS éŸ³è¨Š
            tts_combined = None
            valid_audios = [(i, a) for i, a in enumerate(scene_audios) if a and os.path.exists(a)]
            
            if valid_audios:
                tts_combined = self.output_dir / f"tts_combined_{project_id}.mp3"
                
                # å‰µå»ºéœéŸ³ç‰‡æ®µå¡«å……
                audio_segments = []
                current_time = 0
                
                for i, (scene_idx, audio_path) in enumerate(valid_audios):
                    scene_start = sum(s.get("duration_seconds", 5) for s in scenes[:scene_idx])
                    
                    # å¦‚æœéœ€è¦åœ¨ TTS å‰æ·»åŠ éœéŸ³
                    if scene_start > current_time:
                        silence_duration = scene_start - current_time
                        silence_path = self.output_dir / f"silence_{project_id}_{i}.mp3"
                        cmd = [
                            "ffmpeg", "-y",
                            "-f", "lavfi",
                            "-i", f"anullsrc=r=44100:cl=mono",
                            "-t", str(silence_duration),
                            "-c:a", "libmp3lame",
                            str(silence_path)
                        ]
                        process = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)
                        await process.communicate()
                        if os.path.exists(silence_path):
                            audio_segments.append(str(silence_path))
                    
                    audio_segments.append(audio_path)
                    
                    # ç²å– TTS éŸ³è¨Šæ™‚é•·
                    probe_cmd = ["ffprobe", "-v", "error", "-show_entries", "format=duration", "-of", "csv=p=0", audio_path]
                    probe = await asyncio.create_subprocess_exec(*probe_cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)
                    probe_out, _ = await probe.communicate()
                    try:
                        tts_duration = float(probe_out.decode().strip())
                    except:
                        tts_duration = 3
                    current_time = scene_start + tts_duration
                
                # åˆä½µæ‰€æœ‰éŸ³è¨Šæ®µ
                if audio_segments:
                    audio_concat = self.output_dir / f"audio_concat_{project_id}.txt"
                    with open(audio_concat, "w") as f:
                        for seg in audio_segments:
                            f.write(f"file '{seg}'\n")
                    
                    cmd = [
                        "ffmpeg", "-y",
                        "-f", "concat",
                        "-safe", "0",
                        "-i", str(audio_concat),
                        "-c:a", "libmp3lame",
                        "-b:a", "192k",
                        str(tts_combined)
                    ]
                    process = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)
                    await process.communicate()
                    
                    # æ¸…ç†
                    if audio_concat.exists():
                        os.remove(audio_concat)
                    for seg in audio_segments:
                        if seg not in [a for _, a in valid_audios] and os.path.exists(seg):
                            os.remove(seg)
            
            # æœ€çµ‚æ··éŸ³
            if music_path and os.path.exists(music_path) and os.path.exists(merged_video):
                print(f"[VideoGenerator] ğŸµ æ··éŸ³: èƒŒæ™¯éŸ³æ¨‚éŸ³é‡={music_volume}")
                if tts_combined and os.path.exists(tts_combined):
                    # æ··åˆ TTS + èƒŒæ™¯éŸ³æ¨‚
                    cmd = [
                        "ffmpeg", "-y",
                        "-i", str(merged_video),
                        "-i", str(tts_combined),
                        "-i", music_path,
                        "-filter_complex",
                        f"[1:a]volume=1.2[tts];[2:a]volume={music_volume}[bgm];[tts][bgm]amix=inputs=2:duration=longest[aout]",
                        "-map", "0:v:0",
                        "-map", "[aout]",
                        "-c:v", "copy",
                        "-c:a", "aac",
                        "-b:a", "192k",
                        "-shortest",
                        str(output_path)
                    ]
                else:
                    # åªæœ‰èƒŒæ™¯éŸ³æ¨‚
                    cmd = [
                        "ffmpeg", "-y",
                        "-i", str(merged_video),
                        "-i", music_path,
                        "-filter_complex", f"[1:a]volume={music_volume}[bgm]",
                        "-map", "0:v:0",
                        "-map", "[bgm]",
                        "-c:v", "copy",
                        "-c:a", "aac",
                        "-b:a", "192k",
                        "-shortest",
                        str(output_path)
                    ]
                
                process = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                stdout, stderr = await process.communicate()
                
                if process.returncode != 0:
                    print(f"[VideoGenerator] éŸ³è¨Šæ··åˆå¤±æ•—: {stderr.decode()[:200]}")
                    output_path = merged_video
            elif tts_combined and os.path.exists(tts_combined):
                # åªæœ‰ TTS
                cmd = [
                    "ffmpeg", "-y",
                    "-i", str(merged_video),
                    "-i", str(tts_combined),
                    "-c:v", "copy",
                    "-c:a", "aac",
                    "-b:a", "192k",
                    "-map", "0:v:0",
                    "-map", "1:a:0",
                    "-shortest",
                    str(output_path)
                ]
                process = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE)
                await process.communicate()
            else:
                output_path = merged_video
            
            # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
            try:
                for seg in segment_files:
                    if os.path.exists(seg):
                        os.remove(seg)
                for i in range(len(scene_images)):
                    img_path = self.output_dir / f"scene_{project_id}_{i}.png"
                    if img_path.exists():
                        os.remove(img_path)
                if music_path and os.path.exists(music_path):
                    os.remove(music_path)
                if tts_combined and os.path.exists(tts_combined):
                    os.remove(tts_combined)
                if merged_video.exists() and str(merged_video) != str(output_path):
                    os.remove(merged_video)
                # æ¸…ç† TTS éŸ³è¨Š
                for audio in scene_audios:
                    if audio and os.path.exists(audio):
                        os.remove(audio)
            except Exception as cleanup_err:
                print(f"[VideoGenerator] æ¸…ç†è­¦å‘Š: {cleanup_err}")
            
            if os.path.exists(output_path):
                print(f"[VideoGenerator] ğŸ¬ å°ˆæ¥­ç´šå½±ç‰‡åˆæˆæˆåŠŸï¼")
                return str(output_path)
            
            return None
            
        except Exception as e:
            print(f"[VideoGenerator] FFmpeg éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    async def add_subtitles_to_video(
        self,
        video_path: str,
        subtitles: List[Dict[str, Any]],
        output_path: Optional[str] = None,
        font_size: int = 48,
        font_color: str = "white",
        outline_color: str = "black",
        outline_width: int = 3,
        position: str = "bottom",  # bottom, center, top
        margin_bottom: int = 80
    ) -> Optional[str]:
        """
        åœ¨å½±ç‰‡ä¸Šç–ŠåŠ å‹•æ…‹å­—å¹•ï¼ˆç¡¬å­—å¹•/ç‡’éŒ„å­—å¹•ï¼‰
        
        Args:
            video_path: è¼¸å…¥å½±ç‰‡è·¯å¾‘
            subtitles: å­—å¹•è³‡æ–™åˆ—è¡¨ [{"text": "...", "start": 0.0, "end": 3.0}, ...]
            output_path: è¼¸å‡ºè·¯å¾‘ï¼ˆä¸æŒ‡å®šå‰‡è¦†è“‹åŸæª”ï¼‰
            font_size: å­—å‹å¤§å°
            font_color: å­—å‹é¡è‰²
            outline_color: æé‚Šé¡è‰²
            outline_width: æé‚Šå¯¬åº¦
            position: å­—å¹•ä½ç½®
            margin_bottom: åº•éƒ¨é‚Šè·
        
        Returns:
            è¼¸å‡ºå½±ç‰‡è·¯å¾‘
        """
        if not subtitles:
            return video_path
        
        try:
            # ç”Ÿæˆ SRT æª”æ¡ˆ
            srt_path = self.output_dir / f"subtitles_{uuid.uuid4()}.srt"
            self._generate_srt_file(subtitles, str(srt_path))
            
            # è¨­å®šè¼¸å‡ºè·¯å¾‘
            if not output_path:
                output_path = str(self.output_dir / f"subtitled_{uuid.uuid4()}.mp4")
            
            # è¨ˆç®—å­—å¹•ä½ç½®
            if position == "bottom":
                y_position = f"h-{margin_bottom}-text_h"
            elif position == "center":
                y_position = "(h-text_h)/2"
            else:  # top
                y_position = f"{margin_bottom}"
            
            # æ§‹å»º FFmpeg å­—å¹•æ¿¾é¡
            # ä½¿ç”¨ subtitles æ¿¾é¡ï¼ˆæ”¯æ´ SRT æ ¼å¼ï¼‰
            # å­—å‹è¨­å®šï¼šä½¿ç”¨æ€æºé»‘é«”æˆ–ç³»çµ±ä¸­æ–‡å­—å‹
            srt_path_escaped = str(srt_path).replace(":", "\\:")
            subtitle_filter = (
                f"subtitles={srt_path_escaped}:"
                f"force_style='FontSize={font_size},"
                f"FontName=Noto Sans CJK TC,"
                f"PrimaryColour=&H00FFFFFF,"  # AABBGGRR æ ¼å¼ï¼Œç™½è‰²
                f"OutlineColour=&H00000000,"  # é»‘è‰²æé‚Š
                f"BorderStyle=3,"
                f"Outline={outline_width},"
                f"Shadow=1,"
                f"MarginV={margin_bottom},"
                f"Alignment=2'"  # 2=åº•éƒ¨å±…ä¸­
            )
            
            cmd = [
                "ffmpeg", "-y",
                "-i", video_path,
                "-vf", subtitle_filter,
                "-c:v", "libx264",
                "-preset", "fast",
                "-crf", "20",
                "-c:a", "copy",
                output_path
            ]
            
            print(f"[VideoGenerator] ğŸ“ æ­£åœ¨ç–ŠåŠ å­—å¹•...")
            
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await process.communicate()
            
            # æ¸…ç† SRT æª”æ¡ˆ
            if srt_path.exists():
                os.remove(srt_path)
            
            if process.returncode != 0:
                print(f"[VideoGenerator] å­—å¹•ç–ŠåŠ å¤±æ•—: {stderr.decode()[:300]}")
                # å˜—è©¦ä½¿ç”¨ drawtext ä½œç‚ºå‚™é¸æ–¹æ¡ˆ
                return await self._add_subtitles_drawtext(
                    video_path, subtitles, output_path, 
                    font_size, font_color, outline_color, outline_width, margin_bottom
                )
            
            if os.path.exists(output_path):
                print(f"[VideoGenerator] âœ… å­—å¹•ç–ŠåŠ æˆåŠŸ")
                return output_path
            
            return video_path
            
        except Exception as e:
            print(f"[VideoGenerator] å­—å¹•ç–ŠåŠ éŒ¯èª¤: {e}")
            return video_path
    
    async def _add_subtitles_drawtext(
        self,
        video_path: str,
        subtitles: List[Dict[str, Any]],
        output_path: str,
        font_size: int,
        font_color: str,
        outline_color: str,
        outline_width: int,
        margin_bottom: int
    ) -> Optional[str]:
        """
        ä½¿ç”¨ drawtext æ¿¾é¡ç–ŠåŠ å­—å¹•ï¼ˆå‚™é¸æ–¹æ¡ˆï¼‰
        """
        try:
            # æ§‹å»º drawtext æ¿¾é¡éˆ
            filter_parts = []
            
            for sub in subtitles:
                text = sub.get("text", "").replace("'", r"\'").replace(":", r"\:")
                start = sub.get("start", 0)
                end = sub.get("end", start + 3)
                
                # drawtext æ¿¾é¡åƒæ•¸
                filter_parts.append(
                    f"drawtext=text='{text}':"
                    f"fontsize={font_size}:"
                    f"fontcolor={font_color}:"
                    f"borderw={outline_width}:"
                    f"bordercolor={outline_color}:"
                    f"x=(w-text_w)/2:"
                    f"y=h-{margin_bottom}-text_h:"
                    f"enable='between(t,{start},{end})'"
                )
            
            filter_str = ",".join(filter_parts)
            
            cmd = [
                "ffmpeg", "-y",
                "-i", video_path,
                "-vf", filter_str,
                "-c:v", "libx264",
                "-preset", "fast",
                "-crf", "20",
                "-c:a", "copy",
                output_path
            ]
            
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            await process.communicate()
            
            if os.path.exists(output_path):
                return output_path
            
            return video_path
            
        except Exception as e:
            print(f"[VideoGenerator] drawtext å­—å¹•å¤±æ•—: {e}")
            return video_path
    
    def _generate_srt_file(
        self,
        subtitles: List[Dict[str, Any]],
        output_path: str
    ):
        """
        ç”Ÿæˆ SRT å­—å¹•æª”æ¡ˆ
        """
        srt_content = []
        
        for i, sub in enumerate(subtitles):
            text = sub.get("text", "")
            start = sub.get("start", 0)
            end = sub.get("end", start + 3)
            
            # æ ¼å¼åŒ–æ™‚é–“æˆ³
            start_time = self._format_srt_timestamp(start)
            end_time = self._format_srt_timestamp(end)
            
            srt_content.append(f"{i + 1}")
            srt_content.append(f"{start_time} --> {end_time}")
            srt_content.append(text)
            srt_content.append("")
        
        with open(output_path, "w", encoding="utf-8") as f:
            f.write("\n".join(srt_content))
    
    def _format_srt_timestamp(self, seconds: float) -> str:
        """æ ¼å¼åŒ– SRT æ™‚é–“æˆ³"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millis = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"
    
    def _hex_to_rgb(self, hex_color: str) -> tuple:
        """HEX è½‰ RGB"""
        hex_color = hex_color.lstrip('#')
        return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))
    
    async def _process_custom_image(
        self,
        image_input: str,
        width: int,
        height: int
    ) -> Optional[str]:
        """
        è™•ç†ç”¨æˆ¶è‡ªè¨‚åœ–ç‰‡ï¼ˆä¸æ·»åŠ ä»»ä½•æ–‡å­—è¦†è“‹ï¼‰
        
        image_input: Base64 åœ–ç‰‡è³‡æ–™æˆ– URL
        è¿”å›è™•ç†å¾Œçš„ Base64 åœ–ç‰‡
        """
        try:
            if not PIL_AVAILABLE:
                print("[VideoGenerator] PIL ä¸å¯ç”¨ï¼Œç„¡æ³•è™•ç†è‡ªè¨‚åœ–ç‰‡")
                return None
            
            # è§£æåœ–ç‰‡è³‡æ–™
            if image_input.startswith("data:image"):
                # Base64 æ ¼å¼
                header, data = image_input.split(",", 1)
                image_bytes = base64.b64decode(data)
            elif image_input.startswith("/upload/"):
                # æœ¬åœ°ä¸Šå‚³çš„ URL
                file_path = f"/app/static{image_input.replace('/upload/', '/uploads/')}"
                if not os.path.exists(file_path):
                    # å˜—è©¦å ´æ™¯åœ–ç‰‡è·¯å¾‘
                    file_path = f"/app/static/uploads/scenes/{image_input.split('/')[-1]}"
                
                if os.path.exists(file_path):
                    with open(file_path, "rb") as f:
                        image_bytes = f.read()
                else:
                    print(f"[VideoGenerator] æ‰¾ä¸åˆ°åœ–ç‰‡: {image_input}")
                    return None
            elif image_input.startswith("http"):
                # é ç«¯ URLï¼ˆæœªä¾†å¯æ”¯æ´ï¼‰
                print("[VideoGenerator] æš«ä¸æ”¯æ´é ç«¯ URL")
                return None
            else:
                # å‡è¨­æ˜¯ç´” Base64
                try:
                    image_bytes = base64.b64decode(image_input)
                except:
                    print("[VideoGenerator] ç„¡æ³•è§£æåœ–ç‰‡è³‡æ–™")
                    return None
            
            # é–‹å•Ÿä¸¦è™•ç†åœ–ç‰‡
            img = Image.open(io.BytesIO(image_bytes))
            
            # è½‰æ›ç‚º RGBï¼ˆè™•ç† RGBA æˆ–å…¶ä»–æ¨¡å¼ï¼‰
            if img.mode != "RGB":
                img = img.convert("RGB")
            
            # èª¿æ•´å°ºå¯¸ä»¥ç¬¦åˆç›®æ¨™æ ¼å¼
            img = self._resize_image(img, width, height)
            
            # ä¸æ·»åŠ ä»»ä½•æ–‡å­—è¦†è“‹ - ä¿æŒç•«é¢ä¹¾æ·¨
            
            # è½‰æ›ç‚º Base64
            buffer = io.BytesIO()
            img.save(buffer, format="PNG", quality=95)
            base64_data = base64.b64encode(buffer.getvalue()).decode()
            
            print(f"[VideoGenerator] âœ“ è‡ªè¨‚åœ–ç‰‡è™•ç†æˆåŠŸ ({width}x{height})")
            return f"data:image/png;base64,{base64_data}"
            
        except Exception as e:
            print(f"[VideoGenerator] è‡ªè¨‚åœ–ç‰‡è™•ç†éŒ¯èª¤: {e}")
            return None
    
    async def _generate_scene_image_fallback(
        self,
        scene: Dict[str, Any],
        color_palette: List[str],
        width: int,
        height: int,
        scene_index: int,
        total_scenes: int
    ) -> Optional[str]:
        """å ´æ™¯åœ–ç‰‡ç”Ÿæˆçš„å‚™ç”¨æ–¹æ³•ï¼ˆä¸æ·»åŠ æ–‡å­—ï¼‰"""
        visual_prompt = scene.get("visual_prompt", "")
        negative_prompt = scene.get("negative_prompt", "")
        quality_tags = scene.get("quality_tags", "")
        
        return await self._generate_image(
            visual_prompt,
            color_palette,
            width,
            height,
            None,  # ä¸æ·»åŠ æ–‡å­—è¦†è“‹
            scene_index + 1,
            total_scenes,
            negative_prompt,
            quality_tags
        )


# å–®ä¾‹å¯¦ä¾‹
video_generator = VideoGeneratorService()
